{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae231f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Aggregation Notebook: Convert per-run JSON results into CSV summaries\n",
    "\n",
    "import os, glob, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"./outputs\"   # change if needed\n",
    "CSV_DIR = os.path.join(OUT_DIR, \"csv\")\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "json_files = sorted(glob.glob(os.path.join(OUT_DIR, \"*_results.json\")))\n",
    "print(f\"Found {len(json_files)} results files.\")\n",
    "if len(json_files) == 0:\n",
    "    raise FileNotFoundError(\"No *_results.json files found in OUT_DIR. Run training notebook first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26feecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all JSON files\n",
    "runs = []\n",
    "for fp in json_files:\n",
    "    with open(fp, \"r\") as f:\n",
    "        runs.append(json.load(f))\n",
    "print(f\"Loaded {len(runs)} runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7333d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Baseline Summary (one row per dataset–model–seed)\n",
    "baseline_rows = []\n",
    "for r in runs:\n",
    "    c = r[\"config\"]\n",
    "    b = r[\"baseline\"]\n",
    "    baseline_rows.append({\n",
    "        \"dataset\": c[\"dataset\"],\n",
    "        \"model\": c[\"model_name\"],\n",
    "        \"seed\": c.get(\"seed\", None),\n",
    "        \"epochs\": c[\"epochs\"],\n",
    "        \"batch_size\": c[\"batch_size\"],\n",
    "        \"lr\": c.get(\"lr\", None),\n",
    "        \"weight_decay\": c.get(\"weight_decay\", None),\n",
    "        \"momentum\": c.get(\"momentum\", None),\n",
    "        \"test_acc\": b[\"test_acc\"],\n",
    "        \"test_loss\": b[\"test_loss\"],\n",
    "        \"checkpoint_path\": r.get(\"checkpoint_path\", None),\n",
    "    })\n",
    "\n",
    "df_baseline = pd.DataFrame(baseline_rows).sort_values([\"dataset\",\"model\",\"seed\"])\n",
    "df_baseline.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8acc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Uncertainty Metrics Summary (one row per dataset–model–seed–method)\n",
    "metric_rows = []\n",
    "for r in runs:\n",
    "    c = r[\"config\"]\n",
    "    um = r[\"uncertainty_metrics\"]\n",
    "    for method, vals in um.items():\n",
    "        metric_rows.append({\n",
    "            \"dataset\": c[\"dataset\"],\n",
    "            \"model\": c[\"model_name\"],\n",
    "            \"seed\": c.get(\"seed\", None),\n",
    "            \"method\": method,\n",
    "            \"AUROC_error\": vals.get(\"AUROC_error\", np.nan),\n",
    "            \"ECE\": vals.get(\"ECE\", np.nan),\n",
    "            \"ARC_area\": vals.get(\"ARC_area\", np.nan),\n",
    "            \"AvUC\": vals.get(\"AvUC\", np.nan),\n",
    "        })\n",
    "\n",
    "df_uncert = pd.DataFrame(metric_rows).sort_values([\"dataset\",\"model\",\"seed\",\"method\"])\n",
    "df_uncert.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Percentile Rejection Table (one row per dataset–model–seed–method–reject_percent)\n",
    "rej_rows = []\n",
    "for r in runs:\n",
    "    c = r[\"config\"]\n",
    "    for row in r[\"percentile_rejection\"]:\n",
    "        rej_rows.append({\n",
    "            \"dataset\": c[\"dataset\"],\n",
    "            \"model\": c[\"model_name\"],\n",
    "            \"seed\": c.get(\"seed\", None),\n",
    "            \"method\": row[\"method\"],\n",
    "            \"reject_percent\": row[\"reject_percent\"],\n",
    "            \"rejection_rate\": row[\"rejection_rate\"],\n",
    "            \"accuracy_before_rejection\": row[\"accuracy_before_rejection\"],\n",
    "            \"accuracy_after_rejection\": row[\"accuracy_after_rejection\"],\n",
    "            \"kept_count\": row.get(\"kept_count\", None),\n",
    "        })\n",
    "\n",
    "df_reject = pd.DataFrame(rej_rows).sort_values([\"dataset\",\"model\",\"seed\",\"method\",\"reject_percent\"])\n",
    "df_reject.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3fc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot uncertainty metrics (no seed column)\n",
    "df_pivot = (\n",
    "    df_uncert\n",
    "    .groupby([\"dataset\",\"model\",\"method\"], as_index=False)\n",
    "    .first()\n",
    "    .pivot(index=[\"dataset\",\"model\"],\n",
    "           columns=\"method\",\n",
    "           values=[\"AUROC_error\",\"ECE\",\"ARC_area\",\"AvUC\"]\n",
    "        )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten columns\n",
    "df_pivot.columns = [\n",
    "    col[0] if col[1] == \"\" else f\"{col[0]}_{col[1]}\"\n",
    "    if isinstance(col, tuple) else col\n",
    "    for col in df_pivot.columns\n",
    "]\n",
    "\n",
    "print(\"Pivot columns after flattening:\")\n",
    "print(df_pivot.columns)\n",
    "\n",
    "# Merge (without seed)\n",
    "df_wide = df_baseline.merge(\n",
    "    df_pivot,\n",
    "    on=[\"dataset\",\"model\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Final wide shape:\", df_wide.shape)\n",
    "df_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd78482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV files\n",
    "baseline_csv = os.path.join(CSV_DIR, \"baseline_summary.csv\")\n",
    "uncert_csv = os.path.join(CSV_DIR, \"uncertainty_metrics.csv\")\n",
    "reject_csv = os.path.join(CSV_DIR, \"percentile_rejection.csv\")\n",
    "wide_csv = os.path.join(CSV_DIR, \"thesis_wide_summary.csv\")\n",
    "\n",
    "df_baseline.to_csv(baseline_csv, index=False)\n",
    "df_uncert.to_csv(uncert_csv, index=False)\n",
    "df_reject.to_csv(reject_csv, index=False)\n",
    "df_wide.to_csv(wide_csv, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", baseline_csv)\n",
    "print(\" -\", uncert_csv)\n",
    "print(\" -\", reject_csv)\n",
    "print(\" -\", wide_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f47ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Where to save figures\n",
    "FIG_DIR = \"./outputs/figures\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "def save_fig(fig, filename_base):\n",
    "    png = os.path.join(FIG_DIR, f\"{filename_base}.png\")\n",
    "    pdf = os.path.join(FIG_DIR, f\"{filename_base}.pdf\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(png, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.savefig(pdf, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return png, pdf\n",
    "\n",
    "def _clean_method_labels(s):\n",
    "    # Optional: make method names prettier\n",
    "    return str(s).replace(\"entropy\", \"Entropy\").replace(\"gradient\", \"Gradient\").replace(\"hybrid\", \"Hybrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = pd.read_csv(\"./outputs/csv/baseline_summary.csv\")\n",
    "df_uncert   = pd.read_csv(\"./outputs/csv/uncertainty_metrics.csv\")\n",
    "df_reject   = pd.read_csv(\"./outputs/csv/percentile_rejection.csv\")\n",
    "\n",
    "# Normalize naming\n",
    "df_uncert[\"method_display\"] = df_uncert[\"method\"].map(_clean_method_labels)\n",
    "df_reject[\"method_display\"] = df_reject[\"method\"].map(_clean_method_labels)\n",
    "\n",
    "df_baseline.head(), df_uncert.head(), df_reject.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avuc_bars(df_uncert):\n",
    "    # Skip if AvUC column doesn't exist\n",
    "    if \"AvUC\" not in df_uncert.columns:\n",
    "        print(\"AvUC column not found in data. Skipping AvUC plots.\")\n",
    "        return\n",
    "        \n",
    "    for (dataset, model), g in df_uncert.groupby([\"dataset\", \"model\"], sort=False):\n",
    "        summary = g.groupby(\"method\", as_index=False)[\"AvUC\"].mean()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = plt.gca()\n",
    "        ax.bar(summary[\"method\"], summary[\"AvUC\"])\n",
    "        ax.set_ylabel(\"AvUC (Lower is Better)\")\n",
    "        ax.set_title(f\"AvUC by Method — {dataset} / {model}\")\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        save_fig(fig, f\"AvUC_{dataset}_{model}\")\n",
    "\n",
    "plot_avuc_bars(df_uncert)\n",
    "print(f\"Saved AvUC plots to: {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc26111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_multipanel_by_model(\n",
    "    df_uncert: pd.DataFrame,\n",
    "    metric_col: str,\n",
    "    fig_name: str,\n",
    "    title_metric: str,\n",
    "    datasets_base=(\"cifar10\", \"cifar100\", \"tinyimagenet\"),\n",
    "    models=(\"shufflenetv2_0_5\", \"mobilenetv3_small\", \"efficientnetv2_s\"),\n",
    "    methods=(\"entropy\", \"gradnorm\", \"hybrid\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-panel bar chart: one subplot per model, x-axis = dataset (base),\n",
    "    bars = methods (entropy/gradnorm/hybrid).\n",
    "    Produces two figures: clean and corrupted (-c).\n",
    "\n",
    "    Expected df_uncert columns:\n",
    "      - dataset, model, method, metric_col (e.g., AUROC_error or ARC_area)\n",
    "      - seed optional\n",
    "    \"\"\"\n",
    "\n",
    "    required = {\"dataset\", \"model\", \"method\", metric_col}\n",
    "    missing = required - set(df_uncert.columns)\n",
    "    if missing:\n",
    "        print(f\"[SKIP] Missing columns for {fig_name}: {missing}\")\n",
    "        return\n",
    "\n",
    "    # Normalize dataset to (base_dataset, regime)\n",
    "    df = df_uncert.copy()\n",
    "    df[\"regime\"] = np.where(df[\"dataset\"].astype(str).str.endswith(\"-c\"), \"corrupted\", \"clean\")\n",
    "    df[\"dataset_base\"] = df[\"dataset\"].astype(str).str.replace(\"-c\", \"\", regex=False)\n",
    "\n",
    "    # Only keep target datasets/models/methods\n",
    "    df = df[df[\"dataset_base\"].isin(datasets_base)]\n",
    "    df = df[df[\"model\"].isin(models)]\n",
    "    df = df[df[\"method\"].isin(methods)]\n",
    "\n",
    "    # Human-friendly labels\n",
    "    dataset_label = {\n",
    "        \"cifar10\": \"CIFAR-10\",\n",
    "        \"cifar100\": \"CIFAR-100\",\n",
    "        \"tinyimagenet\": \"Tiny ImageNet\",\n",
    "    }\n",
    "    model_label = {\n",
    "        \"shufflenetv2_0_5\": \"ShuffleNetV2 (0.5×)\",\n",
    "        \"mobilenetv3_small\": \"MobileNetV3-Small\",\n",
    "        \"efficientnetv2_s\": \"EfficientNetV2-S\",\n",
    "    }\n",
    "    method_label = {\n",
    "        \"entropy\": \"Entropy\",\n",
    "        \"gradnorm\": \"Gradient\",\n",
    "        \"hybrid\": \"Hybrid\",\n",
    "    }\n",
    "\n",
    "    def _plot_for_regime(regime: str):\n",
    "        df_r = df[df[\"regime\"] == regime]\n",
    "        if df_r.empty:\n",
    "            print(f\"[SKIP] No data for {fig_name} ({regime}).\")\n",
    "            return\n",
    "\n",
    "        # Aggregate over seeds if present\n",
    "        # Using mean is safe; you can switch to median later if needed\n",
    "        summary = (\n",
    "            df_r.groupby([\"model\", \"dataset_base\", \"method\"], as_index=False)[metric_col]\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(models), figsize=(16, 4), sharey=True)\n",
    "        if len(models) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        x = np.arange(len(datasets_base))\n",
    "        width = 0.25  # 3 methods\n",
    "\n",
    "        for ax, m in zip(axes, models):\n",
    "            g = summary[summary[\"model\"] == m]\n",
    "            if g.empty:\n",
    "                ax.set_title(model_label.get(m, m))\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            # Ensure consistent method order\n",
    "            for i, method in enumerate(methods):\n",
    "                vals = []\n",
    "                for d in datasets_base:\n",
    "                    row = g[(g[\"dataset_base\"] == d) & (g[\"method\"] == method)]\n",
    "                    vals.append(row[metric_col].iloc[0] if len(row) else np.nan)\n",
    "\n",
    "                ax.bar(x + (i - 1) * width, vals, width, label=method_label.get(method, method))\n",
    "\n",
    "            ax.set_title(model_label.get(m, m))\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([dataset_label.get(d, d) for d in datasets_base], rotation=0)\n",
    "            ax.set_ylim(0.0, 1.0)\n",
    "            ax.grid(axis=\"y\", alpha=0.25)\n",
    "\n",
    "        fig.suptitle(f\"{title_metric} by Dataset ({regime.capitalize()})\", y=1.05)\n",
    "        axes[0].set_ylabel(title_metric)\n",
    "        axes[-1].legend(loc=\"lower right\")\n",
    "\n",
    "        save_fig(fig, f\"{fig_name}_{regime}.png\")\n",
    "\n",
    "    _plot_for_regime(\"clean\")\n",
    "    _plot_for_regime(\"corrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ece_bars(df_uncert):\n",
    "    for (dataset, model), g in df_uncert.groupby([\"dataset\", \"model\"], sort=False):\n",
    "        summary = g.groupby(\"method\", as_index=False)[\"ECE\"].mean()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = plt.gca()\n",
    "\n",
    "        ax.bar(summary[\"method\"], summary[\"ECE\"])\n",
    "        ax.set_ylabel(\"ECE (Lower is Better)\")\n",
    "        ax.set_title(f\"ECE by Method — {dataset} / {model}\")\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        save_fig(fig, f\"ECE_{dataset}_{model}\")\n",
    "\n",
    "plot_ece_bars(df_uncert)\n",
    "print(f\"Saved ECE plots to: {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13176e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auroc_multipanel(df_uncert: pd.DataFrame):\n",
    "    plot_metric_multipanel_by_model(\n",
    "        df_uncert=df_uncert,\n",
    "        metric_col=\"AUROC_error\",\n",
    "        fig_name=\"auroc_multipanel\",\n",
    "        title_metric=\"AUROC (Error Detection)\",\n",
    "    )\n",
    "\n",
    "def plot_arc_multipanel(df_uncert: pd.DataFrame):\n",
    "    plot_metric_multipanel_by_model(\n",
    "        df_uncert=df_uncert,\n",
    "        metric_col=\"ARC_area\",\n",
    "        fig_name=\"arc_multipanel\",\n",
    "        title_metric=\"ARC Area (Selective Prediction)\",\n",
    "    )\n",
    "\n",
    "plot_auroc_multipanel(df_uncert)\n",
    "plot_arc_multipanel(df_uncert)\n",
    "\n",
    "print(\"Done. Figures saved to:\", FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rejection_curves(df_reject):\n",
    "    # Expect columns: dataset, model, seed, method, reject_percent, accuracy_after_rejection, accuracy_before_rejection\n",
    "    for (dataset, model), g in df_reject.groupby([\"dataset\", \"model\"], sort=False):\n",
    "        fig = plt.figure()\n",
    "        ax = plt.gca()\n",
    "\n",
    "        # Plot one line per method\n",
    "        for method, gm in g.groupby(\"method\", sort=False):\n",
    "            gm = gm.sort_values(\"reject_percent\")\n",
    "            ax.plot(gm[\"reject_percent\"], gm[\"accuracy_after_rejection\"], marker=\"o\", label=method)\n",
    "\n",
    "        # Baseline line (accuracy before rejection)\n",
    "        base_acc = float(g[\"accuracy_before_rejection\"].iloc[0])\n",
    "        ax.axhline(base_acc, linestyle=\"--\", linewidth=1.0, label=\"Baseline (No Rejection)\")\n",
    "\n",
    "        ax.set_xlabel(\"Rejected Most-Uncertain Samples (%)\")\n",
    "        ax.set_ylabel(\"Accuracy on Kept Samples\")\n",
    "        ax.set_title(f\"Accuracy vs Rejection — {dataset} / {model}\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend()\n",
    "\n",
    "        save_fig(fig, f\"RejectionCurve_{dataset}_{model}\")\n",
    "\n",
    "plot_rejection_curves(df_reject)\n",
    "print(f\"Saved rejection curves to: {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_diagrams(runs):\n",
    "    for r in runs:\n",
    "        rel = r.get(\"reliability\", None)\n",
    "        if rel is None:\n",
    "            continue\n",
    "\n",
    "        cfg = r[\"config\"]\n",
    "        dataset = cfg[\"dataset\"]\n",
    "        model = cfg[\"model_name\"]\n",
    "        seed = cfg.get(\"seed\", None)\n",
    "\n",
    "        edges = np.array(rel[\"edges\"], dtype=float)\n",
    "        bin_conf = np.array(rel[\"bin_conf\"], dtype=float)\n",
    "        bin_acc = np.array(rel[\"bin_acc\"], dtype=float)\n",
    "\n",
    "        mask = ~np.isnan(bin_conf) & ~np.isnan(bin_acc)\n",
    "        bin_conf = bin_conf[mask]\n",
    "        bin_acc = bin_acc[mask]\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot([0, 1], [0, 1])\n",
    "        plt.plot(bin_conf, bin_acc, marker=\"o\")\n",
    "        plt.xlabel(\"Predicted confidence\")\n",
    "        plt.ylabel(\"Empirical accuracy\")\n",
    "        plt.title(f\"Reliability: {dataset} | {model} | seed={seed}\")\n",
    "\n",
    "        out_path = os.path.join(FIG_DIR, f\"reliability_{dataset}_{model}_seed{seed}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "plot_reliability_diagrams(runs)\n",
    "print(f\"Saved reliability diagrams to: {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distributions(runs, method=\"entropy\"):\n",
    "    for r in runs:\n",
    "        sd_all = r.get(\"score_distributions\", None)\n",
    "        if sd_all is None or method not in sd_all:\n",
    "            continue\n",
    "\n",
    "        sd = sd_all[method]\n",
    "        cfg = r[\"config\"]\n",
    "        dataset = cfg[\"dataset\"]\n",
    "        model = cfg[\"model_name\"]\n",
    "        seed = cfg.get(\"seed\", None)\n",
    "\n",
    "        edges = np.array(sd[\"edges\"], dtype=float)\n",
    "        correct_counts = np.array(sd[\"correct_counts\"], dtype=float)\n",
    "        incorrect_counts = np.array(sd[\"incorrect_counts\"], dtype=float)\n",
    "\n",
    "        centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "        width = (edges[1] - edges[0]) * 0.9\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.bar(centers, correct_counts, width=width, alpha=0.6, label=\"Correct\")\n",
    "        plt.bar(centers, incorrect_counts, width=width, alpha=0.6, label=\"Incorrect\")\n",
    "\n",
    "        plt.xlabel(f\"{method} score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(f\"{method.capitalize()} Distribution: {dataset} | {model} | seed={seed}\")\n",
    "        plt.legend()\n",
    "\n",
    "        out_path = os.path.join(FIG_DIR, f\"hist_{method}_{dataset}_{model}_seed{seed}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "# Generate all three\n",
    "plot_score_distributions(runs, \"entropy\")\n",
    "plot_score_distributions(runs, \"gradient\")\n",
    "plot_score_distributions(runs, \"hybrid\")\n",
    "print(f\"Saved score distribution histograms to: {FIG_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
