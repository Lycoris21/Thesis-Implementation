{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215badcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. install prerequisites\n",
    "!pip -q install scikit-learn medmnist tqdm ipywidgets jupyter robustbench torch-uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a57a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. imports + global config\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. reproducibility utilities\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n",
    "\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. settings\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # Datasets: \"cifar10\", \"cifar100\", \"tinyimagenet\", \"tissuemnist\",\n",
    "    #           \"cifar10-c\", \"cifar100-c\", \"tinyimagenet-c\"\n",
    "    dataset: str = \"cifar100\"\n",
    "\n",
    "    # Models: \"shufflenetv2_0_5\", \"mobilenetv3_small\", \"efficientnetv2_s\"\n",
    "    model_name: str = \"shufflenetv2_0_5\"\n",
    "\n",
    "    # Data paths\n",
    "    data_root: str = \"./data\"\n",
    "    tinyimagenet_root: str = \"./tiny-imagenet-200\"  # required only for tinyimagenet\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 64  # Reduced from 128 for smaller GPU\n",
    "    num_workers: int = 0\n",
    "    lr: float = 0.05\n",
    "    weight_decay: float = 1e-4\n",
    "    momentum: float = 0.9\n",
    "    seed: int = 42\n",
    "\n",
    "    # Percentile rejection\n",
    "    reject_percentiles: Tuple[int, ...] = (10, 20, 30, 40, 50)\n",
    "\n",
    "    # Hybrid definition\n",
    "    hybrid_weight_entropy: float = 0.5\n",
    "    hybrid_weight_grad: float = 0.5\n",
    "\n",
    "    # Where to save outputs\n",
    "    out_dir: str = \"./outputs\"\n",
    "\n",
    "cfg = TrainConfig()\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "asdict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. data transforms (Shahriar-aligned normalization)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transforms(train: bool, dataset_name: str = \"\"):\n",
    "    if train:\n",
    "        if dataset_name == \"tissuemnist\":\n",
    "            return transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "            ])\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036cc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tinyimagenet_c_loaders(cfg: TrainConfig):\n",
    "    \"\"\"\n",
    "    Load Tiny ImageNet-C from local directory structure.\n",
    "    Structure: Tiny-ImageNet-C/{corruption}/{severity}/{class_id}/{image}.JPEG\n",
    "    Train: on clean Tiny ImageNet\n",
    "    Test: on corrupted Tiny ImageNet-C (all corruptions, all severities)\n",
    "    \"\"\"\n",
    "    train_tf = build_transforms(train=True, dataset_name=\"tinyimagenet\")\n",
    "    test_tf  = build_transforms(train=False, dataset_name=\"tinyimagenet\")\n",
    "    \n",
    "    # Load regular training data (standard Tiny ImageNet)\n",
    "    tiny_root = download_tinyimagenet(cfg.data_root)\n",
    "    train_dir = os.path.join(tiny_root, \"train\")\n",
    "    \n",
    "    def is_valid_file(x):\n",
    "        return x.lower().endswith(('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'))\n",
    "    \n",
    "    train_ds = datasets.ImageFolder(root=train_dir, transform=train_tf, is_valid_file=is_valid_file)\n",
    "    \n",
    "    # Load corrupted Tiny ImageNet-C from local directory\n",
    "    tinyimagenet_c_root = os.path.join(cfg.data_root, \"Tiny-ImageNet-C\")\n",
    "    if not os.path.exists(tinyimagenet_c_root):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Tiny-ImageNet-C not found at {tinyimagenet_c_root}\\n\"\n",
    "            f\"Expected structure: Tiny-ImageNet-C/{{corruption}}/{{severity}}/{{class}}/{{image}}.JPEG\"\n",
    "        )\n",
    "    \n",
    "    test_ds = LocalTinyImageNetCDataset(root=tinyimagenet_c_root, transform=test_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "    print(f\"✓ Loaded Tiny ImageNet-C from local directory - {len(test_ds)} samples\")\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a. TinyImageNet downloader\n",
    "def download_tinyimagenet(root: str):\n",
    "    \"\"\"\n",
    "    Downloads and extracts TinyImageNet-200 to root directory.\n",
    "    Download URL: http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "    \n",
    "    If automatic download fails, manually download the zip file and place it in the root directory.\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    \n",
    "    tiny_root = os.path.join(root, \"tiny-imagenet-200\")\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    if os.path.exists(os.path.join(tiny_root, \"train\")):\n",
    "        print(f\"✓ TinyImageNet already exists at {tiny_root}\")\n",
    "        return tiny_root\n",
    "    \n",
    "    # Download\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    zip_path = os.path.join(root, \"tiny-imagenet-200.zip\")\n",
    "    \n",
    "    print(f\"Downloading TinyImageNet from {url}...\")\n",
    "    print(\"(This is ~270MB and will take a few minutes)\")\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(\"✓ Download complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Auto-download failed: {e}\")\n",
    "        print(f\"\\nTo use TinyImageNet, manually download from:\")\n",
    "        print(f\"  {url}\")\n",
    "        print(f\"Then extract to: {root}\")\n",
    "        raise\n",
    "    \n",
    "    # Extract\n",
    "    print(f\"Extracting to {root}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(root)\n",
    "    print(\"✓ Extraction complete\")\n",
    "    \n",
    "    # Cleanup zip\n",
    "    os.remove(zip_path)\n",
    "    \n",
    "    return tiny_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be430860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b. Diagnostic: Check TinyImageNet structure\n",
    "def check_tinyimagenet_structure(data_root: str):\n",
    "    \"\"\"Debug: inspect the TinyImageNet directory structure\"\"\"\n",
    "    tiny_root = os.path.join(data_root, \"tiny-imagenet-200\")\n",
    "    \n",
    "    if not os.path.exists(tiny_root):\n",
    "        print(f\"❌ TinyImageNet not found at {tiny_root}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✓ TinyImageNet found at {tiny_root}\")\n",
    "    \n",
    "    # Check train folder\n",
    "    train_dir = os.path.join(tiny_root, \"train\")\n",
    "    if os.path.exists(train_dir):\n",
    "        train_classes = os.listdir(train_dir)\n",
    "        print(f\"  ✓ Train: {len(train_classes)} classes\")\n",
    "    else:\n",
    "        print(f\"  ❌ Train folder not found\")\n",
    "    \n",
    "    # Check val folder\n",
    "    val_dir = os.path.join(tiny_root, \"val\")\n",
    "    if os.path.exists(val_dir):\n",
    "        val_contents = os.listdir(val_dir)\n",
    "        print(f\"  ✓ Val: {len(val_contents)} items: {val_contents[:5]}\")\n",
    "        \n",
    "        # Check if images are organized into class folders\n",
    "        class_folders = [d for d in val_contents if os.path.isdir(os.path.join(val_dir, d)) and d != \"images\"]\n",
    "        if class_folders:\n",
    "            sample_class = class_folders[0]\n",
    "            sample_images = os.listdir(os.path.join(val_dir, sample_class))\n",
    "            print(f\"    - Class folder '{sample_class}' has {len(sample_images)} images\")\n",
    "        else:\n",
    "            print(f\"    - No class folders found (images still in flat structure)\")\n",
    "            images_dir = os.path.join(val_dir, \"images\")\n",
    "            if os.path.exists(images_dir):\n",
    "                img_count = len(os.listdir(images_dir))\n",
    "                sample_imgs = os.listdir(images_dir)[:3]\n",
    "                print(f\"    - Found {img_count} images in 'images' folder\")\n",
    "                print(f\"    - Sample: {sample_imgs}\")\n",
    "    else:\n",
    "        print(f\"  ❌ Val folder not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01553d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c. Helper functions: get_num_classes and build_loaders\n",
    "def get_num_classes(dataset_name: str) -> int:\n",
    "    \"\"\"Return number of classes for each dataset.\"\"\"\n",
    "    mapping = {\n",
    "        \"cifar10\": 10,\n",
    "        \"cifar10-c\": 10,\n",
    "        \"cifar100\": 100,\n",
    "        \"cifar100-c\": 100,\n",
    "        \"tinyimagenet\": 200,\n",
    "        \"tinyimagenet-c\": 200,\n",
    "        \"tissuemnist\": 8,\n",
    "    }\n",
    "    if dataset_name not in mapping:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}. Available: {list(mapping.keys())}\")\n",
    "    return mapping[dataset_name]\n",
    "\n",
    "def build_loaders(cfg: TrainConfig) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Build train and test loaders for all supported datasets.\n",
    "    \n",
    "    Protocol for corrupted datasets:\n",
    "    - Train: on clean dataset\n",
    "    - Test: on corrupted dataset\n",
    "    \n",
    "    Note: Test loader uses num_workers=0 for efficiency during evaluation with gradients\n",
    "    \"\"\"\n",
    "    train_tf = build_transforms(train=True, dataset_name=cfg.dataset.replace(\"-c\", \"\"))\n",
    "    test_tf  = build_transforms(train=False, dataset_name=cfg.dataset.replace(\"-c\", \"\"))\n",
    "    \n",
    "    # Test loaders use 0 workers for efficiency (gradient computation is GPU-bound, not I/O bound)\n",
    "    test_num_workers = 0\n",
    "    \n",
    "    # === CIFAR-10 ===\n",
    "    if cfg.dataset == \"cifar10\":\n",
    "        train_ds = datasets.CIFAR10(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        test_ds = datasets.CIFAR10(root=cfg.data_root, train=False, transform=test_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === CIFAR-10-C (train clean, test corrupted) ===\n",
    "    elif cfg.dataset == \"cifar10-c\":\n",
    "        # Train on clean CIFAR-10\n",
    "        train_ds = datasets.CIFAR10(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        \n",
    "        # Test on corrupted CIFAR-10 (from local directory: ./data/cifar-10-c/)\n",
    "        cifar10c_root = os.path.join(cfg.data_root, \"cifar-10-c\")\n",
    "        if not os.path.exists(cifar10c_root):\n",
    "            raise FileNotFoundError(\n",
    "                f\"CIFAR-10-C not found at {cifar10c_root}\\n\"\n",
    "                f\"Download from: https://zenodo.org/record/2535967\\n\"\n",
    "                f\"Extract to: {cfg.data_root}/cifar-10-c/\"\n",
    "            )\n",
    "        \n",
    "        # Load all corruptions at max severity (severity 5)\n",
    "        test_ds = CIFAR10CDataset(root=cifar10c_root, transform=test_tf, severity=5)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === CIFAR-100 ===\n",
    "    elif cfg.dataset == \"cifar100\":\n",
    "        train_ds = datasets.CIFAR100(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        test_ds = datasets.CIFAR100(root=cfg.data_root, train=False, transform=test_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === CIFAR-100-C (train clean, test corrupted) ===\n",
    "    elif cfg.dataset == \"cifar100-c\":\n",
    "        # Train on clean CIFAR-100\n",
    "        train_ds = datasets.CIFAR100(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        \n",
    "        # Test on corrupted CIFAR-100 (from local directory: ./data/cifar-100-c/)\n",
    "        cifar100c_root = os.path.join(cfg.data_root, \"cifar-100-c\")\n",
    "        if not os.path.exists(cifar100c_root):\n",
    "            raise FileNotFoundError(\n",
    "                f\"CIFAR-100-C not found at {cifar100c_root}\\n\"\n",
    "                f\"Download from: https://zenodo.org/record/3555552\\n\"\n",
    "                f\"Extract to: {cfg.data_root}/cifar-100-c/\"\n",
    "            )\n",
    "        \n",
    "        # Load all corruptions at max severity (severity 5)\n",
    "        test_ds = CIFAR100CDataset(root=cifar100c_root, transform=test_tf, severity=5)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === Tiny ImageNet ===\n",
    "    elif cfg.dataset == \"tinyimagenet\":\n",
    "        tiny_root = download_tinyimagenet(cfg.data_root)\n",
    "        train_dir = os.path.join(tiny_root, \"train\")\n",
    "        val_dir = os.path.join(tiny_root, \"val\")\n",
    "        \n",
    "        def is_valid_file(x):\n",
    "            return x.lower().endswith(('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'))\n",
    "        \n",
    "        train_ds = datasets.ImageFolder(root=train_dir, transform=train_tf, is_valid_file=is_valid_file)\n",
    "        test_ds = datasets.ImageFolder(root=val_dir, transform=test_tf, is_valid_file=is_valid_file)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === Tiny ImageNet-C (train clean, test corrupted) ===\n",
    "    elif cfg.dataset == \"tinyimagenet-c\":\n",
    "        return build_tinyimagenet_c_loaders(cfg)\n",
    "    \n",
    "    # === TissueMNIST ===\n",
    "    elif cfg.dataset == \"tissuemnist\":\n",
    "        from medmnist import TissueMNIST\n",
    "        train_ds = TissueMNIST(root=cfg.data_root, split=\"train\", transform=train_tf, download=True)\n",
    "        test_ds = TissueMNIST(root=cfg.data_root, split=\"test\", transform=test_tf, download=True)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {cfg.dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for local Tiny-ImageNet-C loading\n",
    "class LocalTinyImageNetCDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Load Tiny ImageNet-C from local directory structure.\n",
    "    \n",
    "    Structure: root/{corruption}/{severity}/{class_id}/{image}.JPEG\n",
    "    Loads all corruptions and all severity levels.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Build class name to label mapping from class directories\n",
    "        class_to_label = {}\n",
    "        class_dirs = sorted([d for d in os.listdir(root) \n",
    "                            if os.path.isdir(os.path.join(root, d)) and d[0] == 'n'])\n",
    "        \n",
    "        # Get label mapping from any corruption/severity directory\n",
    "        for corruption_dir in os.listdir(root):\n",
    "            corruption_path = os.path.join(root, corruption_dir)\n",
    "            if not os.path.isdir(corruption_path):\n",
    "                continue\n",
    "            \n",
    "            # Find a severity level directory\n",
    "            for severity_dir in os.listdir(corruption_path):\n",
    "                severity_path = os.path.join(corruption_path, severity_dir)\n",
    "                if not os.path.isdir(severity_path):\n",
    "                    continue\n",
    "                \n",
    "                # Build class mapping\n",
    "                for class_id in sorted(os.listdir(severity_path)):\n",
    "                    class_path = os.path.join(severity_path, class_id)\n",
    "                    if os.path.isdir(class_path):\n",
    "                        if class_id not in class_to_label:\n",
    "                            class_to_label[class_id] = len(class_to_label)\n",
    "                \n",
    "                # Only need one severity to build the mapping\n",
    "                break\n",
    "            break\n",
    "        \n",
    "        if not class_to_label:\n",
    "            raise FileNotFoundError(f\"No class directories found in {root}\")\n",
    "        \n",
    "        # Now load all images from all corruptions and severities\n",
    "        for corruption_dir in sorted(os.listdir(root)):\n",
    "            corruption_path = os.path.join(root, corruption_dir)\n",
    "            if not os.path.isdir(corruption_path):\n",
    "                continue\n",
    "            \n",
    "            for severity_dir in sorted(os.listdir(corruption_path)):\n",
    "                severity_path = os.path.join(corruption_path, severity_dir)\n",
    "                if not os.path.isdir(severity_path):\n",
    "                    continue\n",
    "                \n",
    "                for class_id in sorted(os.listdir(severity_path)):\n",
    "                    class_path = os.path.join(severity_path, class_id)\n",
    "                    if not os.path.isdir(class_path):\n",
    "                        continue\n",
    "                    \n",
    "                    label = class_to_label[class_id]\n",
    "                    \n",
    "                    # Load all images in this class\n",
    "                    for img_file in sorted(os.listdir(class_path)):\n",
    "                        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                            img_path = os.path.join(class_path, img_file)\n",
    "                            self.images.append(img_path)\n",
    "                            self.labels.append(label)\n",
    "        \n",
    "        if not self.images:\n",
    "            raise FileNotFoundError(f\"No images found in {root}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            from PIL import Image\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load image {img_path}: {e}\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5649986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper classes for CIFAR-10-C and CIFAR-100-C loading\n",
    "class CIFAR10CDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Load CIFAR-10-C from local .npy files.\n",
    "    \n",
    "    Structure: root/{corruption}.npy (e.g., gaussian_noise.npy)\n",
    "               root/labels.npy\n",
    "    Each .npy file contains 50000 images (10000 per severity, severities 1-5).\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None, severity: int = 5):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.severity = severity\n",
    "        \n",
    "        # Load labels\n",
    "        labels_path = os.path.join(root, \"labels.npy\")\n",
    "        if not os.path.exists(labels_path):\n",
    "            raise FileNotFoundError(f\"Labels not found at {labels_path}\")\n",
    "        \n",
    "        all_labels = np.load(labels_path)  # shape: (50000,)\n",
    "        \n",
    "        # Get corruption types (all .npy files except labels.npy)\n",
    "        corruption_files = [f for f in os.listdir(root) \n",
    "                           if f.endswith('.npy') and f != 'labels.npy']\n",
    "        \n",
    "        if not corruption_files:\n",
    "            raise FileNotFoundError(f\"No corruption files found in {root}\")\n",
    "        \n",
    "        # Load all corruptions for the specified severity\n",
    "        all_images = []\n",
    "        all_labels_list = []\n",
    "        \n",
    "        # Each corruption file has 50000 images: 10000 per severity (1-5)\n",
    "        # Severity 1: indices 0-9999\n",
    "        # Severity 5: indices 40000-49999\n",
    "        start_idx = (severity - 1) * 10000\n",
    "        end_idx = start_idx + 10000\n",
    "        \n",
    "        for corruption_file in sorted(corruption_files):\n",
    "            corruption_path = os.path.join(root, corruption_file)\n",
    "            images = np.load(corruption_path)  # shape: (50000, 32, 32, 3)\n",
    "            \n",
    "            # Extract images for the specified severity\n",
    "            severity_images = images[start_idx:end_idx]\n",
    "            all_images.append(severity_images)\n",
    "            all_labels_list.append(all_labels[start_idx:end_idx])\n",
    "        \n",
    "        # Concatenate all corruptions\n",
    "        self.images = np.concatenate(all_images, axis=0)  # (N, 32, 32, 3)\n",
    "        self.labels = np.concatenate(all_labels_list, axis=0)  # (N,)\n",
    "        \n",
    "        print(f\"✓ Loaded CIFAR-10-C: {len(corruption_files)} corruptions, \"\n",
    "              f\"severity {severity}, {len(self.images)} total images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]  # (32, 32, 3) uint8\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        from PIL import Image\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        # Resize to target size (224 for ImageNet models)\n",
    "        img = img.resize((224, 224), Image.BILINEAR)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "class CIFAR100CDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Load CIFAR-100-C from local .npy files.\n",
    "    \n",
    "    Structure: root/{corruption}.npy (e.g., gaussian_noise.npy)\n",
    "               root/labels.npy\n",
    "    Each .npy file contains 50000 images (10000 per severity, severities 1-5).\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None, severity: int = 5):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.severity = severity\n",
    "        \n",
    "        # Load labels\n",
    "        labels_path = os.path.join(root, \"labels.npy\")\n",
    "        if not os.path.exists(labels_path):\n",
    "            raise FileNotFoundError(f\"Labels not found at {labels_path}\")\n",
    "        \n",
    "        all_labels = np.load(labels_path)  # shape: (50000,)\n",
    "        \n",
    "        # Get corruption types (all .npy files except labels.npy)\n",
    "        corruption_files = [f for f in os.listdir(root) \n",
    "                           if f.endswith('.npy') and f != 'labels.npy']\n",
    "        \n",
    "        if not corruption_files:\n",
    "            raise FileNotFoundError(f\"No corruption files found in {root}\")\n",
    "        \n",
    "        # Load all corruptions for the specified severity\n",
    "        all_images = []\n",
    "        all_labels_list = []\n",
    "        \n",
    "        # Each corruption file has 50000 images: 10000 per severity (1-5)\n",
    "        # Severity 1: indices 0-9999\n",
    "        # Severity 5: indices 40000-49999\n",
    "        start_idx = (severity - 1) * 10000\n",
    "        end_idx = start_idx + 10000\n",
    "        \n",
    "        for corruption_file in sorted(corruption_files):\n",
    "            corruption_path = os.path.join(root, corruption_file)\n",
    "            images = np.load(corruption_path)  # shape: (50000, 32, 32, 3)\n",
    "            \n",
    "            # Extract images for the specified severity\n",
    "            severity_images = images[start_idx:end_idx]\n",
    "            all_images.append(severity_images)\n",
    "            all_labels_list.append(all_labels[start_idx:end_idx])\n",
    "        \n",
    "        # Concatenate all corruptions\n",
    "        self.images = np.concatenate(all_images, axis=0)  # (N, 32, 32, 3)\n",
    "        self.labels = np.concatenate(all_labels_list, axis=0)  # (N,)\n",
    "        \n",
    "        print(f\"✓ Loaded CIFAR-100-C: {len(corruption_files)} corruptions, \"\n",
    "              f\"severity {severity}, {len(self.images)} total images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]  # (32, 32, 3) uint8\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        from PIL import Image\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        # Resize to target size (224 for ImageNet models)\n",
    "        img = img.resize((224, 224), Image.BILINEAR)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b175a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. model builder\n",
    "def build_model(model_name: str, num_classes: int) -> nn.Module:\n",
    "    if model_name == \"shufflenetv2_0_5\":\n",
    "        m = models.shufflenet_v2_x0_5(weights=None)\n",
    "        in_features = m.fc.in_features\n",
    "        m.fc = nn.Linear(in_features, num_classes)\n",
    "        return m\n",
    "\n",
    "    if model_name == \"mobilenetv3_small\":\n",
    "        m = models.mobilenet_v3_small(weights=None)\n",
    "        in_features = m.classifier[-1].in_features\n",
    "        m.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "        return m\n",
    "\n",
    "    if model_name == \"efficientnetv2_s\":\n",
    "        m = models.efficientnet_v2_s(weights=None)\n",
    "        in_features = m.classifier[-1].in_features\n",
    "        m.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "        return m\n",
    "\n",
    "    raise ValueError(f\"Unknown model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99946ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. training loop\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True).long().view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total += y.numel()\n",
    "            total_loss += loss.item() * y.numel()\n",
    "\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "def train_model(cfg: TrainConfig, model, train_loader):\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=cfg.lr,\n",
    "        momentum=cfg.momentum,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        nesterov=True\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": tr_loss, \"train_acc\": tr_acc, \"lr\": scheduler.get_last_lr()[0]})\n",
    "        print(f\"Epoch {epoch:03d}/{cfg.epochs} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | lr={history[-1]['lr']:.6f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. metrics: ece, risk-coverage + arc area, avuc loss\n",
    "def expected_calibration_error(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15) -> float:\n",
    "    \"\"\"\n",
    "    probs: (N, K) softmax probabilities\n",
    "    y_true: (N,)\n",
    "    \"\"\"\n",
    "    confidences = probs.max(axis=1)\n",
    "    predictions = probs.argmax(axis=1)\n",
    "    accuracies = (predictions == y_true).astype(np.float32)\n",
    "\n",
    "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bin_edges[i], bin_edges[i + 1]\n",
    "        mask = (confidences > lo) & (confidences <= hi) if i > 0 else (confidences >= lo) & (confidences <= hi)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        bin_acc = accuracies[mask].mean()\n",
    "        bin_conf = confidences[mask].mean()\n",
    "        ece += (mask.sum() / len(y_true)) * abs(bin_acc - bin_conf)\n",
    "    return float(ece)\n",
    "\n",
    "def risk_coverage_curve(uncertainty: np.ndarray, correct: np.ndarray):\n",
    "    \"\"\"\n",
    "    uncertainty: higher means more uncertain\n",
    "    correct: 1 if correct prediction else 0\n",
    "\n",
    "    Returns arrays coverage (ascending) and risk (= 1 - accuracy among kept).\n",
    "    \"\"\"\n",
    "    order = np.argsort(uncertainty)  # keep least uncertain first\n",
    "    correct_sorted = correct[order].astype(np.float32)\n",
    "\n",
    "    n = len(correct_sorted)\n",
    "    cum_correct = np.cumsum(correct_sorted)\n",
    "    kept = np.arange(1, n + 1)\n",
    "    acc_kept = cum_correct / kept\n",
    "    risk = 1.0 - acc_kept\n",
    "    coverage = kept / n\n",
    "    return coverage, risk\n",
    "\n",
    "def arc_area(uncertainty: np.ndarray, correct: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Area under accuracy-coverage curve (kept least uncertain first).\n",
    "    \"\"\"\n",
    "    coverage, risk = risk_coverage_curve(uncertainty, correct)\n",
    "    acc = 1.0 - risk\n",
    "    return float(np.trapezoid(acc, coverage))\n",
    "\n",
    "def avuc_loss(danger: np.ndarray, correct: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    AvUC as defined in your thesis:\n",
    "      AvUC = mean( | 1{correct} - (1 - D(x)) | )\n",
    "    where D(x) is the danger score in [0,1] (higher = more dangerous / uncertain).\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    D = danger.astype(np.float64)\n",
    "    c = correct.astype(np.float64)  # 1 if correct else 0\n",
    "    conf = 1.0 - D\n",
    "    return float(np.mean(np.abs(c - conf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. core: unified test evaluation (one pass) + uncertainty signals\n",
    "@torch.inference_mode()\n",
    "def _forward_only(model, x):\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    return logits, probs\n",
    "\n",
    "def evaluate_with_uncertainty(cfg: TrainConfig, model: nn.Module, test_loader: DataLoader):\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Freeze parameters explicitly so backward does not build grads for weights\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_probs = []\n",
    "    all_entropy = []\n",
    "    all_gradnorm = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    batch_count = 0\n",
    "    total_batches = len(test_loader)\n",
    "\n",
    "    # diagnostics\n",
    "    max_abs_logit = 0.0\n",
    "    saw_nan = False\n",
    "    saw_inf = False\n",
    "    batch_diagnostics = []  # optional per-batch summary records\n",
    "\n",
    "    for x, y in tqdm(test_loader, desc=\"Test+Uncertainty\", leave=False):\n",
    "        batch_count += 1\n",
    "        \n",
    "        # labels\n",
    "        y = y.to(DEVICE, non_blocking=True).long().view(-1)\n",
    "\n",
    "        # Inputs: need grad for gradient sensitivity\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        x = x.detach()\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        # Forward (with grad enabled, since we need input gradients)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # Baseline loss and accuracy\n",
    "        loss = criterion(logits, y)\n",
    "        preds = probs.argmax(dim=1)\n",
    "        correct = (preds == y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += correct.sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "        # Entropy per sample\n",
    "        entropy = -(probs * torch.log(probs.clamp_min(1e-12))).sum(dim=1)\n",
    "\n",
    "        # Gradient sensitivity: grad of predicted-class logit w.r.t input\n",
    "        # Pick the logit corresponding to predicted class for each sample\n",
    "        idx = torch.arange(logits.size(0), device=DEVICE)\n",
    "        selected = logits[idx, preds]  # shape: (B,)\n",
    "\n",
    "        # Backward: we want d(selected)/dx for each sample\n",
    "        # Use torch.autograd.grad to avoid storing param grads\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=selected.sum(),\n",
    "            inputs=x,\n",
    "            create_graph=False,\n",
    "            retain_graph=False,\n",
    "            only_inputs=True\n",
    "        )[0]  # shape: (B, C, H, W)\n",
    "\n",
    "        grad_norm = grads.view(grads.size(0), -1).norm(p=2, dim=1)\n",
    "\n",
    "        # --- diagnostics checks (new) ---\n",
    "        with torch.no_grad():\n",
    "            logits_abs_max_batch = float(logits.abs().max().cpu().item())\n",
    "            logits_mean_batch = float(logits.mean().cpu().item())\n",
    "            loss_val = float(loss.item())\n",
    "            batch_avg_loss = loss_val / float(y.numel())\n",
    "\n",
    "            nan_in_logits = bool(torch.isnan(logits).any().item())\n",
    "            inf_in_logits = bool(torch.isinf(logits).any().item())\n",
    "            nan_in_probs = bool(torch.isnan(probs).any().item())\n",
    "            inf_in_probs = bool(torch.isinf(probs).any().item())\n",
    "\n",
    "            saw_nan = saw_nan or nan_in_logits or nan_in_probs\n",
    "            saw_inf = saw_inf or inf_in_logits or inf_in_probs\n",
    "            if logits_abs_max_batch > max_abs_logit:\n",
    "                max_abs_logit = logits_abs_max_batch\n",
    "\n",
    "            # record a compact batch summary (useful for debugging or saving)\n",
    "            batch_diagnostics.append({\n",
    "                \"batch\": batch_count,\n",
    "                \"loss_sum\": loss_val,\n",
    "                \"batch_avg_loss\": batch_avg_loss,\n",
    "                \"logits_max_abs\": logits_abs_max_batch,\n",
    "                \"logits_mean\": logits_mean_batch,\n",
    "                \"nan\": nan_in_logits or nan_in_probs,\n",
    "                \"inf\": inf_in_logits or inf_in_probs\n",
    "            })\n",
    "\n",
    "            # Print occasional progress diagnostics to help locate problem batches\n",
    "            if batch_count % 50 == 0 or nan_in_logits or inf_in_logits:\n",
    "                print(f\"[BATCH {batch_count}/{total_batches}] loss_sum={loss_val:.4e} \"\n",
    "                      f\"batch_avg_loss={batch_avg_loss:.4e} logits_max_abs={logits_abs_max_batch:.4e} \"\n",
    "                      f\"logits_mean={logits_mean_batch:.4e} nan={nan_in_logits or nan_in_probs} inf={inf_in_logits or inf_in_probs}\")\n",
    "\n",
    "        # Store batch results\n",
    "        all_y_true.append(y.detach().cpu().numpy())\n",
    "        all_y_pred.append(preds.detach().cpu().numpy())\n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_entropy.append(entropy.detach().cpu().numpy())\n",
    "        all_gradnorm.append(grad_norm.detach().cpu().numpy())\n",
    "\n",
    "    # concat results\n",
    "    y_true = np.concatenate(all_y_true)\n",
    "    y_pred = np.concatenate(all_y_pred)\n",
    "    probs  = np.concatenate(all_probs)\n",
    "    entropy = np.concatenate(all_entropy)\n",
    "    gradnorm = np.concatenate(all_gradnorm)\n",
    "\n",
    "    test_loss = total_loss / total\n",
    "    test_acc = total_correct / total\n",
    "\n",
    "    # \"Accuracy Before Rejection\" must match test_acc\n",
    "    correct_vec = (y_pred == y_true).astype(np.int32)\n",
    "\n",
    "    # final diagnostics print\n",
    "    print(\"=== EVAL DIAGNOSTICS ===\")\n",
    "    print(f\"total_samples={total} total_loss_sum={total_loss:.4e} test_loss_avg={test_loss:.4e} test_acc={test_acc:.4f}\")\n",
    "    print(f\"max_abs_logit={max_abs_logit:.4e} saw_nan={saw_nan} saw_inf={saw_inf}\")\n",
    "\n",
    "    diagnostics = {\n",
    "        \"max_abs_logit\": float(max_abs_logit),\n",
    "        \"saw_nan\": bool(saw_nan),\n",
    "        \"saw_inf\": bool(saw_inf),\n",
    "        \"batch_diagnostics_sample\": batch_diagnostics[:10]  # keep small preview\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": float(test_loss),\n",
    "        \"test_acc\": float(test_acc),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"probs\": probs,\n",
    "        \"entropy\": entropy,\n",
    "        \"gradnorm\": gradnorm,\n",
    "        \"correct\": correct_vec,\n",
    "        \"diagnostics\": diagnostics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. hybrid score + uncertainty evaluation + percentile rejection + reliability bins + histograms\n",
    "def minmax_norm(x: np.ndarray) -> np.ndarray:\n",
    "    lo, hi = float(np.min(x)), float(np.max(x))\n",
    "    if hi - lo < 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return ((x - lo) / (hi - lo)).astype(np.float32)\n",
    "\n",
    "def compute_uncertainty_metrics(method_scores: Dict[str, np.ndarray], eval_pack: Dict, cfg: TrainConfig):\n",
    "    y_true = eval_pack[\"y_true\"]\n",
    "    y_pred = eval_pack[\"y_pred\"]\n",
    "    probs  = eval_pack[\"probs\"]\n",
    "    correct = eval_pack[\"correct\"]\n",
    "\n",
    "    errors = 1 - correct  # 1 if incorrect, 0 if correct\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # ECE uses probabilities (common baseline confidence calibration)\n",
    "    ece = expected_calibration_error(probs, y_true, n_bins=15)\n",
    "\n",
    "    for name, score in method_scores.items():\n",
    "        # AUROC for detecting errors: higher uncertainty should indicate error\n",
    "        # If all errors are same class, roc_auc_score can fail; handle safely\n",
    "        try:\n",
    "            auroc = roc_auc_score(errors, score)\n",
    "        except ValueError:\n",
    "            auroc = float(\"nan\")\n",
    "\n",
    "        # ARC area from risk-coverage curve (keep least uncertain first)\n",
    "        arc = arc_area(score, correct)\n",
    "        avuc = avuc_loss(score, correct)\n",
    "\n",
    "        results[name] = {\n",
    "            \"AUROC_error\": float(auroc),\n",
    "            \"ECE\": float(ece),\n",
    "            \"ARC_area\": float(arc),\n",
    "            \"AvUC\": float(avuc),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def percentile_rejection_table(method_scores: Dict[str, np.ndarray], eval_pack: Dict, cfg: TrainConfig):\n",
    "    y_true = eval_pack[\"y_true\"]\n",
    "    y_pred = eval_pack[\"y_pred\"]\n",
    "    correct = (y_pred == y_true).astype(np.int32)\n",
    "    base_acc = float(correct.mean())\n",
    "\n",
    "    table = []\n",
    "    N = len(correct)\n",
    "\n",
    "    for name, score in method_scores.items():\n",
    "        for p in cfg.reject_percentiles:\n",
    "            reject_n = int(round(N * (p / 100.0)))\n",
    "            # Reject most uncertain => sort descending\n",
    "            order = np.argsort(score)[::-1]\n",
    "            reject_idx = order[:reject_n]\n",
    "            keep_mask = np.ones(N, dtype=bool)\n",
    "            keep_mask[reject_idx] = False\n",
    "\n",
    "            kept = keep_mask.sum()\n",
    "            acc_after = float(correct[keep_mask].mean()) if kept > 0 else float(\"nan\")\n",
    "\n",
    "            table.append({\n",
    "                \"method\": name,\n",
    "                \"reject_percent\": p,\n",
    "                \"rejection_rate\": float(reject_n / N),\n",
    "                \"accuracy_before_rejection\": base_acc,\n",
    "                \"accuracy_after_rejection\": acc_after,\n",
    "                \"kept_count\": int(kept),\n",
    "            })\n",
    "\n",
    "    return table\n",
    "\n",
    "def reliability_bins_from_probs(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15):\n",
    "    conf = probs.max(axis=1)\n",
    "    pred = probs.argmax(axis=1)\n",
    "    correct = (pred == y_true).astype(np.int32)\n",
    "\n",
    "    edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(conf, edges, right=True) - 1\n",
    "    bin_ids = np.clip(bin_ids, 0, n_bins - 1)\n",
    "\n",
    "    bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_count = np.zeros(n_bins, dtype=np.int32)\n",
    "\n",
    "    for b in range(n_bins):\n",
    "        m = (bin_ids == b)\n",
    "        bin_count[b] = int(m.sum())\n",
    "        if bin_count[b] > 0:\n",
    "            bin_conf[b] = float(conf[m].mean())\n",
    "            bin_acc[b] = float(correct[m].mean())\n",
    "        else:\n",
    "            bin_conf[b] = np.nan\n",
    "            bin_acc[b] = np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_bins\": int(n_bins),\n",
    "        \"edges\": edges.tolist(),\n",
    "        \"bin_conf\": bin_conf.tolist(),\n",
    "        \"bin_acc\": bin_acc.tolist(),\n",
    "        \"bin_count\": bin_count.tolist(),\n",
    "    }\n",
    "\n",
    "def score_histogram_by_correct(score: np.ndarray, correct: np.ndarray, n_bins: int = 30, lo: float = 0.0, hi: float = 1.0):\n",
    "    score = np.clip(score, lo, hi)\n",
    "    edges = np.linspace(lo, hi, n_bins + 1)\n",
    "\n",
    "    correct_mask = correct.astype(bool)\n",
    "    incorrect_mask = ~correct_mask\n",
    "\n",
    "    c_counts, _ = np.histogram(score[correct_mask], bins=edges)\n",
    "    i_counts, _ = np.histogram(score[incorrect_mask], bins=edges)\n",
    "\n",
    "    return {\n",
    "        \"n_bins\": int(n_bins),\n",
    "        \"edges\": edges.tolist(),\n",
    "        \"correct_counts\": c_counts.astype(int).tolist(),\n",
    "        \"incorrect_counts\": i_counts.astype(int).tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1831ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. run one full experiment (train → evaluate → save results)\n",
    "def run_experiment(cfg: TrainConfig):\n",
    "    # Clear GPU cache at start\n",
    "    torch.cuda.empty_cache()\n",
    "    set_seed(cfg.seed)\n",
    "    \n",
    "    num_classes = get_num_classes(cfg.dataset)\n",
    "    train_loader, test_loader = build_loaders(cfg)\n",
    "\n",
    "    model = build_model(cfg.model_name, num_classes)\n",
    "\n",
    "    print(\"Training configuration:\")\n",
    "    print(json.dumps(asdict(cfg), indent=2))\n",
    "\n",
    "    # For corrupted datasets, try to load pre-trained clean model\n",
    "    tag = f\"{cfg.dataset}_{cfg.model_name}_e{cfg.epochs}_bs{cfg.batch_size}\"\n",
    "    ckpt_path = os.path.join(cfg.out_dir, f\"{tag}.pth\")\n",
    "    \n",
    "    # Map corrupted dataset names to their clean counterparts\n",
    "    clean_dataset_map = {\n",
    "        \"cifar10-c\": \"cifar10\",\n",
    "        \"cifar100-c\": \"cifar100\",\n",
    "        \"tinyimagenet-c\": \"tinyimagenet\"\n",
    "    }\n",
    "    \n",
    "    model_loaded = False\n",
    "    if cfg.dataset in clean_dataset_map:\n",
    "        # This is a corrupted dataset - try to load clean model\n",
    "        clean_dataset = clean_dataset_map[cfg.dataset]\n",
    "        clean_tag = f\"{clean_dataset}_{cfg.model_name}_e{cfg.epochs}_bs{cfg.batch_size}\"\n",
    "        clean_ckpt_path = os.path.join(cfg.out_dir, f\"{clean_tag}.pth\")\n",
    "        \n",
    "        if os.path.exists(clean_ckpt_path):\n",
    "            print(f\"\\n✓ Loading pre-trained model from {clean_ckpt_path}\")\n",
    "            model.load_state_dict(torch.load(clean_ckpt_path, map_location=DEVICE))\n",
    "            model_loaded = True\n",
    "            train_hist = []  # No training history since we skipped training\n",
    "        else:\n",
    "            print(f\"\\n⚠ Pre-trained model not found at {clean_ckpt_path}\")\n",
    "            print(f\"   Training from scratch on clean {clean_dataset} data...\")\n",
    "    \n",
    "    # Train if model wasn't loaded\n",
    "    if not model_loaded:\n",
    "        if os.path.exists(ckpt_path):\n",
    "            print(f\"\\n✓ Loading existing model from {ckpt_path}\")\n",
    "            model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "            train_hist = []  # No training history since we loaded existing model\n",
    "        else:\n",
    "            print(\"\\nTraining model from scratch...\")\n",
    "            model, train_hist = train_model(cfg, model, train_loader)\n",
    "            # Save final model\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "    else:\n",
    "        # For corrupted datasets, still save under the corrupted name for reference\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    # Unified evaluation with uncertainty\n",
    "    eval_pack = evaluate_with_uncertainty(cfg, model, test_loader)\n",
    "\n",
    "    # Build uncertainty scores\n",
    "    entropy = eval_pack[\"entropy\"]\n",
    "    grad = eval_pack[\"gradnorm\"]\n",
    "\n",
    "    # Entropy: normalize by ln(num_classes) to get [0,1] scale\n",
    "    entropy_n = np.clip(entropy / np.log(num_classes), 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "    # Gradient norm: min-max normalize to [0,1]\n",
    "    grad_n = minmax_norm(grad).astype(np.float32)\n",
    "\n",
    "    hybrid = (cfg.hybrid_weight_entropy * entropy_n) + (cfg.hybrid_weight_grad * grad_n)\n",
    "\n",
    "    method_scores = {\n",
    "        \"entropy\": entropy_n,\n",
    "        \"gradient\": grad_n,\n",
    "        \"hybrid\": hybrid.astype(np.float32)\n",
    "    }\n",
    "\n",
    "    # Metrics\n",
    "    metrics = compute_uncertainty_metrics(method_scores, eval_pack, cfg)\n",
    "    rej_table = percentile_rejection_table(method_scores, eval_pack, cfg)\n",
    "\n",
    "    # reliability + score distributions\n",
    "    reliability_summary = reliability_bins_from_probs(\n",
    "        probs=eval_pack[\"probs\"],\n",
    "        y_true=eval_pack[\"y_true\"],\n",
    "        n_bins=15\n",
    "    )\n",
    "\n",
    "    score_distributions = {\n",
    "        name: score_histogram_by_correct(score, eval_pack[\"correct\"], n_bins=30, lo=0.0, hi=1.0)\n",
    "        for name, score in method_scores.items()\n",
    "    }\n",
    "\n",
    "    # Consistency check\n",
    "    base_acc = eval_pack[\"test_acc\"]\n",
    "    acc_before = float((eval_pack[\"y_pred\"] == eval_pack[\"y_true\"]).mean())\n",
    "    print(f\"\\nConsistency check: test_acc={base_acc:.6f} vs accuracy_before_rejection={acc_before:.6f}\")\n",
    "\n",
    "    # Save artifacts\n",
    "    out = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"train_history\": train_hist,\n",
    "        \"baseline\": {\"test_acc\": eval_pack[\"test_acc\"], \"test_loss\": eval_pack[\"test_loss\"]},\n",
    "        \"uncertainty_metrics\": metrics,\n",
    "        \"percentile_rejection\": rej_table,\n",
    "        \"reliability\": reliability_summary,              \n",
    "        \"score_distributions\": score_distributions,     \n",
    "        \"checkpoint_path\": ckpt_path\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(cfg.out_dir, f\"{tag}_results.json\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"Saved results JSON: {out_path}\")\n",
    "\n",
    "    # Clear GPU cache at end\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5108da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Define dataset-model pairs to run\n",
    "dataset_model_pairs = [\n",
    "    # Clean datasets\n",
    "    (\"cifar10\", \"shufflenetv2_0_5\"),\n",
    "    (\"cifar10\", \"mobilenetv3_small\"),\n",
    "    (\"cifar10\", \"efficientnetv2_s\"),\n",
    "    (\"cifar100\", \"shufflenetv2_0_5\"),\n",
    "    (\"cifar100\", \"mobilenetv3_small\"),\n",
    "    (\"cifar100\", \"efficientnetv2_s\"),\n",
    "    (\"tinyimagenet\", \"shufflenetv2_0_5\"),\n",
    "    (\"tinyimagenet\", \"mobilenetv3_small\"),\n",
    "    (\"tinyimagenet\", \"efficientnetv2_s\"),\n",
    "    (\"tissuemnist\", \"shufflenetv2_0_5\"),\n",
    "    (\"tissuemnist\", \"mobilenetv3_small\"),\n",
    "    (\"tissuemnist\", \"efficientnetv2_s\"),\n",
    "    \n",
    "    # Corrupted datasets\n",
    "    (\"cifar10-c\", \"shufflenetv2_0_5\"),\n",
    "    (\"cifar10-c\", \"mobilenetv3_small\"),\n",
    "    (\"cifar10-c\", \"efficientnetv2_s\"),\n",
    "    (\"cifar100-c\", \"shufflenetv2_0_5\"),\n",
    "    (\"cifar100-c\", \"mobilenetv3_small\"),\n",
    "    (\"cifar100-c\", \"efficientnetv2_s\"),\n",
    "    (\"tinyimagenet-c\", \"shufflenetv2_0_5\"),\n",
    "    (\"tinyimagenet-c\", \"mobilenetv3_small\"),\n",
    "    (\"tinyimagenet-c\", \"efficientnetv2_s\"),\n",
    "]\n",
    "\n",
    "print(f\"Running {len(dataset_model_pairs)} experiments:\")\n",
    "for i, (ds, model) in enumerate(dataset_model_pairs, 1):\n",
    "    print(f\"  {i}. {ds:15} + {model:20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e318018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Run all experiments in sequence\n",
    "all_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, (dataset, model_name) in enumerate(dataset_model_pairs, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Experiment {idx}/{len(dataset_model_pairs)}: {dataset} + {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create a fresh config for this pair\n",
    "    experiment_cfg = TrainConfig(\n",
    "        dataset=dataset,\n",
    "        model_name=model_name,\n",
    "        data_root=cfg.data_root,\n",
    "        tinyimagenet_root=cfg.tinyimagenet_root,\n",
    "        epochs=cfg.epochs,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.num_workers,\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        momentum=cfg.momentum,\n",
    "        seed=cfg.seed,\n",
    "        reject_percentiles=cfg.reject_percentiles,\n",
    "        hybrid_weight_entropy=cfg.hybrid_weight_entropy,\n",
    "        hybrid_weight_grad=cfg.hybrid_weight_grad,\n",
    "        out_dir=cfg.out_dir,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = run_experiment(experiment_cfg)\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model_name,\n",
    "            \"status\": \"success\",\n",
    "            \"result\": result\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error in experiment {idx}: {e}\")\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model_name,\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"All experiments completed in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in all_results if r[\"status\"] == \"success\")\n",
    "failed = sum(1 for r in all_results if r[\"status\"] == \"failed\")\n",
    "print(f\"Summary: {successful} successful, {failed} failed\")\n",
    "for r in all_results:\n",
    "    status_icon = \"✓\" if r[\"status\"] == \"success\" else \"✗\"\n",
    "    print(f\"  {status_icon} {r['dataset']:15} + {r['model']:20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9521161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13a. DIAGNOSTIC: Quick test - where is it hanging?\n",
    "\n",
    "print(\"DIAGNOSTIC TEST - Finding the bottleneck\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Test CIFAR-10-C dataset loading (this is likely the culprit)\n",
    "print(\"\\n[STEP 1] Testing CIFAR-10-C dataset loading...\")\n",
    "t0 = time.time()\n",
    "\n",
    "cifar10c_root = \"./data/Tiny-ImageNet-C\"\n",
    "print(f\"Looking for files in: {cifar10c_root}\")\n",
    "\n",
    "if os.path.exists(cifar10c_root):\n",
    "    files = os.listdir(cifar10c_root)\n",
    "    print(f\"Found files: {files}\")\n",
    "    \n",
    "    # Check one file\n",
    "    data_file = os.path.join(cifar10c_root, \"gaussian_noise.npy\")\n",
    "    if os.path.exists(data_file):\n",
    "        print(f\"\\nLoading {data_file}...\")\n",
    "        t1 = time.time()\n",
    "        images = np.load(data_file)\n",
    "        elapsed = time.time() - t1\n",
    "        print(f\"✓ Loaded in {elapsed:.1f}s, shape: {images.shape}\")\n",
    "        \n",
    "        # Try to slice it\n",
    "        print(f\"\\nSlicing to severity 5 (rows 40000:50000)...\")\n",
    "        t1 = time.time()\n",
    "        subset = images[40000:50000]\n",
    "        elapsed = time.time() - t1\n",
    "        print(f\"✓ Sliced in {elapsed:.3f}s, shape: {subset.shape}\")\n",
    "    else:\n",
    "        print(f\"✗ File not found: {data_file}\")\n",
    "else:\n",
    "    print(f\"✗ Directory not found: {cifar10c_root}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOTAL DIAGNOSTIC TIME:\", time.time() - t0, \"seconds\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
