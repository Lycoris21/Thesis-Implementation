{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "215badcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\Lycoris\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\Lycoris\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\Lycoris\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\Lycoris\\AppData\\Roaming\\Python\\Python313\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# 0. install prerequisites\n",
    "!pip -q install scikit-learn medmnist tqdm ipywidgets jupyter robustbench torch-uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9a57a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. imports + global config\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c509ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "# 2. reproducibility utilities\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n",
    "\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c34b4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'cifar100',\n",
       " 'model_name': 'shufflenetv2_0_5',\n",
       " 'data_root': './data',\n",
       " 'tinyimagenet_root': './tiny-imagenet-200',\n",
       " 'epochs': 50,\n",
       " 'batch_size': 64,\n",
       " 'num_workers': 0,\n",
       " 'lr': 0.05,\n",
       " 'weight_decay': 0.0001,\n",
       " 'momentum': 0.9,\n",
       " 'seed': 42,\n",
       " 'reject_percentiles': (10, 20, 30, 40, 50),\n",
       " 'hybrid_weight_entropy': 0.5,\n",
       " 'hybrid_weight_grad': 0.5,\n",
       " 'out_dir': './outputs'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. settings\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # Datasets: \"cifar10\", \"cifar100\", \"tinyimagenet\", \"tissuemnist\",\n",
    "    #           \"cifar10-c\", \"cifar100-c\", \"tinyimagenet-c\"\n",
    "    dataset: str = \"cifar100\"\n",
    "\n",
    "    # Models: \"shufflenetv2_0_5\", \"mobilenetv3_small\", \"efficientnetv2_s\"\n",
    "    model_name: str = \"shufflenetv2_0_5\"\n",
    "\n",
    "    # Data paths\n",
    "    data_root: str = \"./data\"\n",
    "    tinyimagenet_root: str = \"./tiny-imagenet-200\"  # required only for tinyimagenet\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 64  # Reduced from 128 for smaller GPU\n",
    "    num_workers: int = 0\n",
    "    lr: float = 0.05\n",
    "    weight_decay: float = 1e-4\n",
    "    momentum: float = 0.9\n",
    "    seed: int = 42\n",
    "\n",
    "    # --- stability knobs ---\n",
    "    label_smoothing: float = 0.1      # 0.0 to disable\n",
    "    max_grad_norm: float = 1.0        # 0.0 to disable\n",
    "    lr_efficientnet: float = 0.01     # override for efficientnetv2_s (was 0.05)\n",
    "\n",
    "    # Percentile rejection\n",
    "    reject_percentiles: Tuple[int, ...] = (10, 20, 30, 40, 50)\n",
    "\n",
    "    # Hybrid definition\n",
    "    hybrid_weight_entropy: float = 0.5\n",
    "    hybrid_weight_grad: float = 0.5\n",
    "\n",
    "    # Where to save outputs\n",
    "    out_dir: str = \"./outputs\"\n",
    "\n",
    "cfg = TrainConfig()\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "asdict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fef76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. data transforms (Shahriar-aligned normalization)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transforms(train: bool, dataset_name: str = \"\"):\n",
    "    if train:\n",
    "        if dataset_name == \"tissuemnist\":\n",
    "            return transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=3),  # Convert 1-channel to 3-channel\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "            ])\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ])\n",
    "    else:\n",
    "        if dataset_name == \"tissuemnist\":\n",
    "            return transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=3),  # Convert 1-channel to 3-channel\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "            ])\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "036cc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. dataset helpers needed by loaders\n",
    "# 5a. TinyImageNet downloader\n",
    "def download_tinyimagenet(root: str):\n",
    "    \"\"\"\n",
    "    Downloads and extracts TinyImageNet-200 to root directory.\n",
    "    Download URL: http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "\n",
    "    If automatic download fails, manually download the zip file and place it in the root directory.\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "\n",
    "    tiny_root = os.path.join(root, \"tiny-imagenet-200\")\n",
    "\n",
    "    # Check if already downloaded\n",
    "    if os.path.exists(os.path.join(tiny_root, \"train\")):\n",
    "        print(f\"✓ TinyImageNet already exists at {tiny_root}\")\n",
    "        return tiny_root\n",
    "\n",
    "    # Download\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    zip_path = os.path.join(root, \"tiny-imagenet-200.zip\")\n",
    "\n",
    "    print(f\"Downloading TinyImageNet from {url}...\")\n",
    "    print(\"(This is ~270MB and will take a few minutes)\")\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(\"✓ Download complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Auto-download failed: {e}\")\n",
    "        print(f\"\\nTo use TinyImageNet, manually download from:\")\n",
    "        print(f\"  {url}\")\n",
    "        print(f\"Then extract to: {root}\")\n",
    "        raise\n",
    "\n",
    "    # Extract\n",
    "    print(f\"Extracting to {root}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(root)\n",
    "    print(\"✓ Extraction complete\")\n",
    "\n",
    "    # Cleanup zip\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    return tiny_root\n",
    "\n",
    "\n",
    "# Helper class for local Tiny-ImageNet-C loading\n",
    "class LocalTinyImageNetCDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Load Tiny ImageNet-C from local directory structure.\n",
    "\n",
    "    Structure: root/{corruption}/{severity}/{class_id}/{image}.JPEG\n",
    "    Loads all corruptions and all severity levels.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Build class name to label mapping from class directories\n",
    "        class_to_label = {}\n",
    "        class_dirs = sorted([d for d in os.listdir(root) \n",
    "                            if os.path.isdir(os.path.join(root, d)) and d[0] == 'n'])\n",
    "\n",
    "        # Get label mapping from any corruption/severity directory\n",
    "        for corruption_dir in os.listdir(root):\n",
    "            corruption_path = os.path.join(root, corruption_dir)\n",
    "            if not os.path.isdir(corruption_path):\n",
    "                continue\n",
    "\n",
    "            # Find a severity level directory\n",
    "            for severity_dir in os.listdir(corruption_path):\n",
    "                severity_path = os.path.join(corruption_path, severity_dir)\n",
    "                if not os.path.isdir(severity_path):\n",
    "                    continue\n",
    "\n",
    "                # Build class mapping\n",
    "                for class_id in sorted(os.listdir(severity_path)):\n",
    "                    class_path = os.path.join(severity_path, class_id)\n",
    "                    if os.path.isdir(class_path):\n",
    "                        if class_id not in class_to_label:\n",
    "                            class_to_label[class_id] = len(class_to_label)\n",
    "\n",
    "                # Only need one severity to build the mapping\n",
    "                break\n",
    "            break\n",
    "\n",
    "        if not class_to_label:\n",
    "            raise FileNotFoundError(f\"No class directories found in {root}\")\n",
    "\n",
    "        # Now load all images from all corruptions and severities\n",
    "        for corruption_dir in sorted(os.listdir(root)):\n",
    "            corruption_path = os.path.join(root, corruption_dir)\n",
    "            if not os.path.isdir(corruption_path):\n",
    "                continue\n",
    "\n",
    "            for severity_dir in sorted(os.listdir(corruption_path)):\n",
    "                severity_path = os.path.join(corruption_path, severity_dir)\n",
    "                if not os.path.isdir(severity_path):\n",
    "                    continue\n",
    "\n",
    "                for class_id in sorted(os.listdir(severity_path)):\n",
    "                    class_path = os.path.join(severity_path, class_id)\n",
    "                    if not os.path.isdir(class_path):\n",
    "                        continue\n",
    "\n",
    "                    label = class_to_label[class_id]\n",
    "\n",
    "                    # Load all images in this class\n",
    "                    for img_file in sorted(os.listdir(class_path)):\n",
    "                        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                            img_path = os.path.join(class_path, img_file)\n",
    "                            self.images.append(img_path)\n",
    "                            self.labels.append(label)\n",
    "\n",
    "        if not self.images:\n",
    "            raise FileNotFoundError(f\"No images found in {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            from PIL import Image\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load image {img_path}: {e}\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Helper classes for CIFAR-10-C and CIFAR-100-C loading\n",
    "class CIFAR10CDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Load CIFAR-10-C from local .npy files.\n",
    "\n",
    "    Structure: root/{corruption}.npy (e.g., gaussian_noise.npy)\n",
    "               root/labels.npy\n",
    "    Each .npy file contains 50000 images (10000 per severity, severities 1-5).\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None, severity: int = 5):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.severity = severity\n",
    "\n",
    "        # Load labels\n",
    "        labels_path = os.path.join(root, \"labels.npy\")\n",
    "        if not os.path.exists(labels_path):\n",
    "            raise FileNotFoundError(f\"Labels not found at {labels_path}\")\n",
    "\n",
    "        all_labels = np.load(labels_path)  # shape: (50000,)\n",
    "\n",
    "        # Get corruption types (all .npy files except labels.npy)\n",
    "        corruption_files = [f for f in os.listdir(root) \n",
    "                           if f.endswith('.npy') and f != 'labels.npy']\n",
    "\n",
    "        if not corruption_files:\n",
    "            raise FileNotFoundError(f\"No corruption files found in {root}\")\n",
    "\n",
    "        # Load all corruptions for the specified severity\n",
    "        all_images = []\n",
    "        all_labels_list = []\n",
    "\n",
    "        # Each corruption file has 50000 images: 10000 per severity (1-5)\n",
    "        # Severity 1: indices 0-9999\n",
    "        # Severity 5: indices 40000-49999\n",
    "        start_idx = (severity - 1) * 10000\n",
    "        end_idx = start_idx + 10000\n",
    "\n",
    "        for corruption_file in sorted(corruption_files):\n",
    "            corruption_path = os.path.join(root, corruption_file)\n",
    "            images = np.load(corruption_path)  # shape: (50000, 32, 32, 3)\n",
    "\n",
    "            # Extract images for the specified severity\n",
    "            severity_images = images[start_idx:end_idx]\n",
    "            all_images.append(severity_images)\n",
    "            all_labels_list.append(all_labels[start_idx:end_idx])\n",
    "\n",
    "        # Concatenate all corruptions\n",
    "        self.images = np.concatenate(all_images, axis=0)  # (N, 32, 32, 3)\n",
    "        self.labels = np.concatenate(all_labels_list, axis=0)  # (N,)\n",
    "\n",
    "        print(f\"✓ Loaded CIFAR-10-C: {len(corruption_files)} corruptions, \"\n",
    "              f\"severity {severity}, {len(self.images)} total images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]  # (32, 32, 3) uint8\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        from PIL import Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class CIFAR100CDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Load CIFAR-100-C from local .npy files.\n",
    "\n",
    "    Structure: root/{corruption}.npy (e.g., gaussian_noise.npy)\n",
    "               root/labels.npy\n",
    "    Each .npy file contains 50000 images (10000 per severity, severities 1-5).\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None, severity: int = 5):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.severity = severity\n",
    "\n",
    "        # Load labels\n",
    "        labels_path = os.path.join(root, \"labels.npy\")\n",
    "        if not os.path.exists(labels_path):\n",
    "            raise FileNotFoundError(f\"Labels not found at {labels_path}\")\n",
    "\n",
    "        all_labels = np.load(labels_path)  # shape: (50000,)\n",
    "\n",
    "        # Get corruption types (all .npy files except labels.npy)\n",
    "        corruption_files = [f for f in os.listdir(root) \n",
    "                           if f.endswith('.npy') and f != 'labels.npy']\n",
    "\n",
    "        if not corruption_files:\n",
    "            raise FileNotFoundError(f\"No corruption files found in {root}\")\n",
    "\n",
    "        # Load all corruptions for the specified severity\n",
    "        all_images = []\n",
    "        all_labels_list = []\n",
    "\n",
    "        # Each corruption file has 50000 images: 10000 per severity (1-5)\n",
    "        # Severity 1: indices 0-9999\n",
    "        # Severity 5: indices 40000-49999\n",
    "        start_idx = (severity - 1) * 10000\n",
    "        end_idx = start_idx + 10000\n",
    "\n",
    "        for corruption_file in sorted(corruption_files):\n",
    "            corruption_path = os.path.join(root, corruption_file)\n",
    "            images = np.load(corruption_path)  # shape: (50000, 32, 32, 3)\n",
    "\n",
    "            # Extract images for the specified severity\n",
    "            severity_images = images[start_idx:end_idx]\n",
    "            all_images.append(severity_images)\n",
    "            all_labels_list.append(all_labels[start_idx:end_idx])\n",
    "\n",
    "        # Concatenate all corruptions\n",
    "        self.images = np.concatenate(all_images, axis=0)  # (N, 32, 32, 3)\n",
    "        self.labels = np.concatenate(all_labels_list, axis=0)  # (N,)\n",
    "\n",
    "        print(f\"✓ Loaded CIFAR-100-C: {len(corruption_files)} corruptions, \"\n",
    "              f\"severity {severity}, {len(self.images)} total images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]  # (32, 32, 3) uint8\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        from PIL import Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def build_tinyimagenet_c_loaders(cfg: TrainConfig):\n",
    "    \"\"\"\n",
    "    Load Tiny ImageNet-C from local directory structure.\n",
    "    Structure: Tiny-ImageNet-C/{corruption}/{severity}/{class_id}/{image}.JPEG\n",
    "    Train: on clean Tiny ImageNet\n",
    "    Test: on corrupted Tiny ImageNet-C (all corruptions, all severities)\n",
    "    \"\"\"\n",
    "    train_tf = build_transforms(train=True, dataset_name=\"tinyimagenet\")\n",
    "    test_tf  = build_transforms(train=False, dataset_name=\"tinyimagenet\")\n",
    "\n",
    "    # Load regular training data (standard Tiny ImageNet)\n",
    "    tiny_root = download_tinyimagenet(cfg.data_root)\n",
    "    train_dir = os.path.join(tiny_root, \"train\")\n",
    "\n",
    "    def is_valid_file(x):\n",
    "        return x.lower().endswith(('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'))\n",
    "\n",
    "    train_ds = datasets.ImageFolder(root=train_dir, transform=train_tf, is_valid_file=is_valid_file)\n",
    "\n",
    "    # Load corrupted Tiny ImageNet-C from local directory\n",
    "    tinyimagenet_c_root = os.path.join(cfg.data_root, \"Tiny-ImageNet-C\")\n",
    "    if not os.path.exists(tinyimagenet_c_root):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Tiny-ImageNet-C not found at {tinyimagenet_c_root}\\n\"\n",
    "            f\"Expected structure: Tiny-ImageNet-C/{{corruption}}/{{severity}}/{{class}}/{{image}}.JPEG\"\n",
    "        )\n",
    "\n",
    "    test_ds = LocalTinyImageNetCDataset(root=tinyimagenet_c_root, transform=test_tf)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "    print(f\"✓ Loaded Tiny ImageNet-C from local directory - {len(test_ds)} samples\")\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be430860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b. Diagnostic: Check TinyImageNet structure\n",
    "def check_tinyimagenet_structure(data_root: str):\n",
    "    \"\"\"Debug: inspect the TinyImageNet directory structure\"\"\"\n",
    "    tiny_root = os.path.join(data_root, \"tiny-imagenet-200\")\n",
    "    \n",
    "    if not os.path.exists(tiny_root):\n",
    "        print(f\"❌ TinyImageNet not found at {tiny_root}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✓ TinyImageNet found at {tiny_root}\")\n",
    "    \n",
    "    # Check train folder\n",
    "    train_dir = os.path.join(tiny_root, \"train\")\n",
    "    if os.path.exists(train_dir):\n",
    "        train_classes = os.listdir(train_dir)\n",
    "        print(f\"  ✓ Train: {len(train_classes)} classes\")\n",
    "    else:\n",
    "        print(f\"  ❌ Train folder not found\")\n",
    "    \n",
    "    # Check val folder\n",
    "    val_dir = os.path.join(tiny_root, \"val\")\n",
    "    if os.path.exists(val_dir):\n",
    "        val_contents = os.listdir(val_dir)\n",
    "        print(f\"  ✓ Val: {len(val_contents)} items: {val_contents[:5]}\")\n",
    "        \n",
    "        # Check if images are organized into class folders\n",
    "        class_folders = [d for d in val_contents if os.path.isdir(os.path.join(val_dir, d)) and d != \"images\"]\n",
    "        if class_folders:\n",
    "            sample_class = class_folders[0]\n",
    "            sample_images = os.listdir(os.path.join(val_dir, sample_class))\n",
    "            print(f\"    - Class folder '{sample_class}' has {len(sample_images)} images\")\n",
    "        else:\n",
    "            print(f\"    - No class folders found (images still in flat structure)\")\n",
    "            images_dir = os.path.join(val_dir, \"images\")\n",
    "            if os.path.exists(images_dir):\n",
    "                img_count = len(os.listdir(images_dir))\n",
    "                sample_imgs = os.listdir(images_dir)[:3]\n",
    "                print(f\"    - Found {img_count} images in 'images' folder\")\n",
    "                print(f\"    - Sample: {sample_imgs}\")\n",
    "    else:\n",
    "        print(f\"  ❌ Val folder not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01553d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c. Helper functions: get_num_classes and build_loaders\n",
    "def get_num_classes(dataset_name: str) -> int:\n",
    "    \"\"\"Return number of classes for each dataset.\"\"\"\n",
    "    mapping = {\n",
    "        \"cifar10\": 10,\n",
    "        \"cifar10-c\": 10,\n",
    "        \"cifar100\": 100,\n",
    "        \"cifar100-c\": 100,\n",
    "        \"tinyimagenet\": 200,\n",
    "        \"tinyimagenet-c\": 200,\n",
    "        \"tissuemnist\": 8,\n",
    "    }\n",
    "    if dataset_name not in mapping:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}. Available: {list(mapping.keys())}\")\n",
    "    return mapping[dataset_name]\n",
    "\n",
    "def build_loaders(cfg: TrainConfig) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Build train and test loaders for all supported datasets.\n",
    "    \n",
    "    Protocol for corrupted datasets:\n",
    "    - Train: on clean dataset\n",
    "    - Test: on corrupted dataset\n",
    "    \n",
    "    Note: Test loader uses num_workers=0 for efficiency during evaluation with gradients\n",
    "    \"\"\"\n",
    "    train_tf = build_transforms(train=True, dataset_name=cfg.dataset.replace(\"-c\", \"\"))\n",
    "    test_tf  = build_transforms(train=False, dataset_name=cfg.dataset.replace(\"-c\", \"\"))\n",
    "    \n",
    "    # Test loaders use 0 workers for efficiency (gradient computation is GPU-bound, not I/O bound)\n",
    "    test_num_workers = 0\n",
    "    \n",
    "    # === CIFAR-10 ===\n",
    "    if cfg.dataset == \"cifar10\":\n",
    "        train_ds = datasets.CIFAR10(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        test_ds = datasets.CIFAR10(root=cfg.data_root, train=False, transform=test_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === CIFAR-10-C (train clean, test corrupted) ===\n",
    "    elif cfg.dataset == \"cifar10-c\":\n",
    "        # Train on clean CIFAR-10\n",
    "        train_ds = datasets.CIFAR10(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        \n",
    "        # Test on corrupted CIFAR-10 (from local directory: ./data/cifar-10-c/)\n",
    "        cifar10c_root = os.path.join(cfg.data_root, \"cifar-10-c\")\n",
    "        if not os.path.exists(cifar10c_root):\n",
    "            raise FileNotFoundError(\n",
    "                f\"CIFAR-10-C not found at {cifar10c_root}\\n\"\n",
    "                f\"Download from: https://zenodo.org/record/2535967\\n\"\n",
    "                f\"Extract to: {cfg.data_root}/cifar-10-c/\"\n",
    "            )\n",
    "        \n",
    "        # Load all corruptions at max severity (severity 5)\n",
    "        test_ds = CIFAR10CDataset(root=cifar10c_root, transform=test_tf, severity=3)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === CIFAR-100 ===\n",
    "    elif cfg.dataset == \"cifar100\":\n",
    "        train_ds = datasets.CIFAR100(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        test_ds = datasets.CIFAR100(root=cfg.data_root, train=False, transform=test_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === CIFAR-100-C (train clean, test corrupted) ===\n",
    "    elif cfg.dataset == \"cifar100-c\":\n",
    "        # Train on clean CIFAR-100\n",
    "        train_ds = datasets.CIFAR100(root=cfg.data_root, train=True, transform=train_tf, download=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        \n",
    "        # Test on corrupted CIFAR-100 (from local directory: ./data/cifar-100-c/)\n",
    "        cifar100c_root = os.path.join(cfg.data_root, \"cifar-100-c\")\n",
    "        if not os.path.exists(cifar100c_root):\n",
    "            raise FileNotFoundError(\n",
    "                f\"CIFAR-100-C not found at {cifar100c_root}\\n\"\n",
    "                f\"Download from: https://zenodo.org/record/3555552\\n\"\n",
    "                f\"Extract to: {cfg.data_root}/cifar-100-c/\"\n",
    "            )\n",
    "        \n",
    "        # Load all corruptions at max severity (severity 5)\n",
    "        test_ds = CIFAR100CDataset(root=cifar100c_root, transform=test_tf, severity=3)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === Tiny ImageNet ===\n",
    "    elif cfg.dataset == \"tinyimagenet\":\n",
    "        tiny_root = download_tinyimagenet(cfg.data_root)\n",
    "        train_dir = os.path.join(tiny_root, \"train\")\n",
    "        val_dir = os.path.join(tiny_root, \"val\")\n",
    "        \n",
    "        def is_valid_file(x):\n",
    "            return x.lower().endswith(('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'))\n",
    "        \n",
    "        train_ds = datasets.ImageFolder(root=train_dir, transform=train_tf, is_valid_file=is_valid_file)\n",
    "        test_ds = datasets.ImageFolder(root=val_dir, transform=test_tf, is_valid_file=is_valid_file)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # === Tiny ImageNet-C (train clean, test corrupted) ===\n",
    "    elif cfg.dataset == \"tinyimagenet-c\":\n",
    "        return build_tinyimagenet_c_loaders(cfg)\n",
    "    \n",
    "    # === TissueMNIST ===\n",
    "    elif cfg.dataset == \"tissuemnist\":\n",
    "        from medmnist import TissueMNIST\n",
    "        train_ds = TissueMNIST(root=cfg.data_root, split=\"train\", transform=train_tf, download=True)\n",
    "        test_ds = TissueMNIST(root=cfg.data_root, split=\"test\", transform=test_tf, download=True)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                 num_workers=test_num_workers, pin_memory=True, drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {cfg.dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b175a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. model builder\n",
    "def build_model(model_name: str, num_classes: int) -> nn.Module:\n",
    "    if model_name == \"shufflenetv2_0_5\":\n",
    "        m = models.shufflenet_v2_x0_5(weights=None)\n",
    "        in_features = m.fc.in_features\n",
    "        m.fc = nn.Linear(in_features, num_classes)\n",
    "        return m\n",
    "\n",
    "    if model_name == \"mobilenetv3_small\":\n",
    "        m = models.mobilenet_v3_small(weights=None)\n",
    "        in_features = m.classifier[-1].in_features\n",
    "        m.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "        return m\n",
    "\n",
    "    if model_name == \"efficientnetv2_s\":\n",
    "        m = models.efficientnet_v2_s(weights=None)\n",
    "        in_features = m.classifier[-1].in_features\n",
    "        m.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "        return m\n",
    "\n",
    "    raise ValueError(f\"Unknown model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d99946ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. training loop\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True).long().view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        if cfg.max_grad_norm and cfg.max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total += y.numel()\n",
    "            total_loss += loss.item() * y.numel()\n",
    "\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "def train_model(cfg: TrainConfig, model, train_loader):\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
    "\n",
    "    # --- model-specific LR override (EfficientNet only) ---\n",
    "    effective_lr = cfg.lr\n",
    "    if \"efficientnet\" in cfg.model_name.lower():\n",
    "        effective_lr = cfg.lr_efficientnet\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=effective_lr,\n",
    "        momentum=cfg.momentum,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        nesterov=True\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": tr_loss, \"train_acc\": tr_acc, \"lr\": scheduler.get_last_lr()[0]})\n",
    "        print(f\"Epoch {epoch:03d}/{cfg.epochs} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | lr={history[-1]['lr']:.6f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f514a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. metrics: ece, risk-coverage + arc area, avuc loss\n",
    "def expected_calibration_error(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15) -> float:\n",
    "    \"\"\"\n",
    "    probs: (N, K) softmax probabilities\n",
    "    y_true: (N,)\n",
    "    \"\"\"\n",
    "    confidences = probs.max(axis=1)\n",
    "    predictions = probs.argmax(axis=1)\n",
    "    accuracies = (predictions == y_true).astype(np.float32)\n",
    "\n",
    "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bin_edges[i], bin_edges[i + 1]\n",
    "        mask = (confidences > lo) & (confidences <= hi) if i > 0 else (confidences >= lo) & (confidences <= hi)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        bin_acc = accuracies[mask].mean()\n",
    "        bin_conf = confidences[mask].mean()\n",
    "        ece += (mask.sum() / len(y_true)) * abs(bin_acc - bin_conf)\n",
    "    return float(ece)\n",
    "\n",
    "def risk_coverage_curve(uncertainty: np.ndarray, correct: np.ndarray):\n",
    "    \"\"\"\n",
    "    uncertainty: higher means more uncertain\n",
    "    correct: 1 if correct prediction else 0\n",
    "\n",
    "    Returns arrays coverage (ascending) and risk (= 1 - accuracy among kept).\n",
    "    \"\"\"\n",
    "    order = np.argsort(uncertainty)  # keep least uncertain first\n",
    "    correct_sorted = correct[order].astype(np.float32)\n",
    "\n",
    "    n = len(correct_sorted)\n",
    "    cum_correct = np.cumsum(correct_sorted)\n",
    "    kept = np.arange(1, n + 1)\n",
    "    acc_kept = cum_correct / kept\n",
    "    risk = 1.0 - acc_kept\n",
    "    coverage = kept / n\n",
    "    return coverage, risk\n",
    "\n",
    "def arc_area(uncertainty: np.ndarray, correct: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Area under accuracy-coverage curve (kept least uncertain first).\n",
    "    \"\"\"\n",
    "    coverage, risk = risk_coverage_curve(uncertainty, correct)\n",
    "    acc = 1.0 - risk\n",
    "    return float(np.trapezoid(acc, coverage))\n",
    "\n",
    "def avuc_loss(danger: np.ndarray, correct: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    AvUC as defined in your thesis:\n",
    "      AvUC = mean( | 1{correct} - (1 - D(x)) | )\n",
    "    where D(x) is the danger score in [0,1] (higher = more dangerous / uncertain).\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    D = danger.astype(np.float64)\n",
    "    c = correct.astype(np.float64)  # 1 if correct else 0\n",
    "    conf = 1.0 - D\n",
    "    return float(np.mean(np.abs(c - conf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dbae9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. core: unified test evaluation (one pass) + uncertainty signals\n",
    "@torch.inference_mode()\n",
    "def _forward_only(model, x):\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    return logits, probs\n",
    "\n",
    "def evaluate_with_uncertainty(cfg: TrainConfig, model: nn.Module, test_loader: DataLoader):\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Freeze parameters explicitly so backward does not build grads for weights\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_probs = []\n",
    "    all_entropy = []\n",
    "    all_gradnorm = []\n",
    "\n",
    "    all_per_sample_loss = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    batch_count = 0\n",
    "    total_batches = len(test_loader)\n",
    "\n",
    "    # diagnostics\n",
    "    max_abs_logit = 0.0\n",
    "    saw_nan = False\n",
    "    saw_inf = False\n",
    "    batch_diagnostics = []  # optional per-batch summary records\n",
    "\n",
    "    for x, y in tqdm(test_loader, desc=\"Test+Uncertainty\", leave=False):\n",
    "        batch_count += 1\n",
    "        \n",
    "        # labels\n",
    "        y = y.to(DEVICE, non_blocking=True).long().view(-1)\n",
    "\n",
    "        # Inputs: need grad for gradient sensitivity\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        x = x.detach()\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        # Forward (with grad enabled, since we need input gradients)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # Baseline loss and accuracy\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # Per-sample cross-entropy (for tail diagnostics)\n",
    "        with torch.no_grad():\n",
    "            per_sample_loss = F.cross_entropy(logits, y, reduction=\"none\")  # shape (B,)\n",
    "            all_per_sample_loss.append(per_sample_loss.detach().cpu().numpy())\n",
    "\n",
    "        preds = probs.argmax(dim=1)\n",
    "        correct = (preds == y)\n",
    "\n",
    "        total_loss += loss.item() * y.numel() \n",
    "        total_correct += correct.sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "        # Entropy per sample\n",
    "        entropy = -(probs * torch.log(probs.clamp_min(1e-12))).sum(dim=1)\n",
    "\n",
    "        # Gradient sensitivity: grad of predicted-class logit w.r.t input\n",
    "        # Pick the logit corresponding to predicted class for each sample\n",
    "        idx = torch.arange(logits.size(0), device=DEVICE)\n",
    "        selected = logits[idx, preds]  # shape: (B,)\n",
    "\n",
    "        # Backward: we want d(selected)/dx for each sample\n",
    "        # Use torch.autograd.grad to avoid storing param grads\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=selected.sum(),\n",
    "            inputs=x,\n",
    "            create_graph=False,\n",
    "            retain_graph=False,\n",
    "            only_inputs=True\n",
    "        )[0]  # shape: (B, C, H, W)\n",
    "\n",
    "        grad_norm = grads.view(grads.size(0), -1).norm(p=2, dim=1)\n",
    "\n",
    "        # --- diagnostics checks (new) ---\n",
    "        with torch.no_grad():\n",
    "            logits_abs_max_batch = float(logits.abs().max().cpu().item())\n",
    "            logits_mean_batch = float(logits.mean().cpu().item())\n",
    "            loss_val = float(loss.item())\n",
    "            batch_avg_loss = loss_val\n",
    "\n",
    "            nan_in_logits = bool(torch.isnan(logits).any().item())\n",
    "            inf_in_logits = bool(torch.isinf(logits).any().item())\n",
    "            nan_in_probs = bool(torch.isnan(probs).any().item())\n",
    "            inf_in_probs = bool(torch.isinf(probs).any().item())\n",
    "\n",
    "            saw_nan = saw_nan or nan_in_logits or nan_in_probs\n",
    "            saw_inf = saw_inf or inf_in_logits or inf_in_probs\n",
    "            if logits_abs_max_batch > max_abs_logit:\n",
    "                max_abs_logit = logits_abs_max_batch\n",
    "\n",
    "            # record a compact batch summary (useful for debugging or saving)\n",
    "            batch_diagnostics.append({\n",
    "                \"batch\": batch_count,\n",
    "                \"loss_mean\": loss_val,\n",
    "                \"batch_avg_loss\": batch_avg_loss,\n",
    "                \"logits_max_abs\": logits_abs_max_batch,\n",
    "                \"logits_mean\": logits_mean_batch,\n",
    "                \"nan\": nan_in_logits or nan_in_probs,\n",
    "                \"inf\": inf_in_logits or inf_in_probs\n",
    "            })\n",
    "\n",
    "            # Print occasional progress diagnostics to help locate problem batches\n",
    "            if batch_count % 50 == 0 or nan_in_logits or inf_in_logits:\n",
    "                print(f\"[BATCH {batch_count}/{total_batches}] loss_sum={loss_val:.4e} \"\n",
    "                      f\"batch_avg_loss={batch_avg_loss:.4e} logits_max_abs={logits_abs_max_batch:.4e} \"\n",
    "                      f\"logits_mean={logits_mean_batch:.4e} nan={nan_in_logits or nan_in_probs} inf={inf_in_logits or inf_in_probs}\")\n",
    "\n",
    "        # Store batch results\n",
    "        all_y_true.append(y.detach().cpu().numpy())\n",
    "        all_y_pred.append(preds.detach().cpu().numpy())\n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_entropy.append(entropy.detach().cpu().numpy())\n",
    "        all_gradnorm.append(grad_norm.detach().cpu().numpy())\n",
    "\n",
    "    # concat results\n",
    "    y_true = np.concatenate(all_y_true)\n",
    "    y_pred = np.concatenate(all_y_pred)\n",
    "    probs  = np.concatenate(all_probs)\n",
    "    entropy = np.concatenate(all_entropy)\n",
    "    gradnorm = np.concatenate(all_gradnorm)\n",
    "\n",
    "    per_sample_loss_all = np.concatenate(all_per_sample_loss)\n",
    "\n",
    "    print(\"=== LOSS TAIL DIAGNOSTICS ===\")\n",
    "    print(f\"loss_median={np.median(per_sample_loss_all):.4f}\")\n",
    "    print(f\"loss_p95={np.percentile(per_sample_loss_all, 95):.4f}\")\n",
    "    print(f\"loss_p99={np.percentile(per_sample_loss_all, 99):.4f}\")\n",
    "    print(f\"loss_max={np.max(per_sample_loss_all):.4f}\")\n",
    "\n",
    "    test_loss = total_loss / total\n",
    "    test_acc = total_correct / total\n",
    "\n",
    "    # \"Accuracy Before Rejection\" must match test_acc\n",
    "    correct_vec = (y_pred == y_true).astype(np.int32)\n",
    "\n",
    "    # final diagnostics print\n",
    "    print(\"=== EVAL DIAGNOSTICS ===\")\n",
    "    print(f\"total_samples={total} total_loss_sum={total_loss:.4e} test_loss_avg={test_loss:.4e} test_acc={test_acc:.4f}\")\n",
    "    print(f\"max_abs_logit={max_abs_logit:.4e} saw_nan={saw_nan} saw_inf={saw_inf}\")\n",
    "\n",
    "    diagnostics = {\n",
    "        \"max_abs_logit\": float(max_abs_logit),\n",
    "        \"saw_nan\": bool(saw_nan),\n",
    "        \"saw_inf\": bool(saw_inf),\n",
    "        \"batch_diagnostics_sample\": batch_diagnostics[:10]  # keep small preview\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": float(test_loss),\n",
    "        \"test_acc\": float(test_acc),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"probs\": probs,\n",
    "        \"entropy\": entropy,\n",
    "        \"gradnorm\": gradnorm,\n",
    "        \"correct\": correct_vec,\n",
    "        \"diagnostics\": diagnostics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6375e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. hybrid score + uncertainty evaluation + percentile rejection + reliability bins + histograms\n",
    "def minmax_norm(x: np.ndarray) -> np.ndarray:\n",
    "    lo, hi = float(np.min(x)), float(np.max(x))\n",
    "    if hi - lo < 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return ((x - lo) / (hi - lo)).astype(np.float32)\n",
    "\n",
    "def compute_uncertainty_metrics(method_scores: Dict[str, np.ndarray], eval_pack: Dict, cfg: TrainConfig):\n",
    "    y_true = eval_pack[\"y_true\"]\n",
    "    y_pred = eval_pack[\"y_pred\"]\n",
    "    probs  = eval_pack[\"probs\"]\n",
    "    correct = eval_pack[\"correct\"]\n",
    "\n",
    "    errors = 1 - correct  # 1 if incorrect, 0 if correct\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # ECE uses probabilities (common baseline confidence calibration)\n",
    "    ece = expected_calibration_error(probs, y_true, n_bins=15)\n",
    "\n",
    "    for name, score in method_scores.items():\n",
    "        # AUROC for detecting errors: higher uncertainty should indicate error\n",
    "        # If all errors are same class, roc_auc_score can fail; handle safely\n",
    "        try:\n",
    "            auroc = roc_auc_score(errors, score)\n",
    "        except ValueError:\n",
    "            auroc = float(\"nan\")\n",
    "\n",
    "        # ARC area from risk-coverage curve (keep least uncertain first)\n",
    "        arc = arc_area(score, correct)\n",
    "        avuc = avuc_loss(score, correct)\n",
    "\n",
    "        results[name] = {\n",
    "            \"AUROC_error\": float(auroc),\n",
    "            \"ECE\": float(ece),\n",
    "            \"ARC_area\": float(arc),\n",
    "            \"AvUC\": float(avuc),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def percentile_rejection_table(method_scores: Dict[str, np.ndarray], eval_pack: Dict, cfg: TrainConfig):\n",
    "    y_true = eval_pack[\"y_true\"]\n",
    "    y_pred = eval_pack[\"y_pred\"]\n",
    "    correct = (y_pred == y_true).astype(np.int32)\n",
    "    base_acc = float(correct.mean())\n",
    "\n",
    "    table = []\n",
    "    N = len(correct)\n",
    "\n",
    "    for name, score in method_scores.items():\n",
    "        for p in cfg.reject_percentiles:\n",
    "            reject_n = int(round(N * (p / 100.0)))\n",
    "            # Reject most uncertain => sort descending\n",
    "            order = np.argsort(score)[::-1]\n",
    "            reject_idx = order[:reject_n]\n",
    "            keep_mask = np.ones(N, dtype=bool)\n",
    "            keep_mask[reject_idx] = False\n",
    "\n",
    "            kept = keep_mask.sum()\n",
    "            acc_after = float(correct[keep_mask].mean()) if kept > 0 else float(\"nan\")\n",
    "\n",
    "            table.append({\n",
    "                \"method\": name,\n",
    "                \"reject_percent\": p,\n",
    "                \"rejection_rate\": float(reject_n / N),\n",
    "                \"accuracy_before_rejection\": base_acc,\n",
    "                \"accuracy_after_rejection\": acc_after,\n",
    "                \"kept_count\": int(kept),\n",
    "            })\n",
    "\n",
    "    return table\n",
    "\n",
    "def reliability_bins_from_probs(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15):\n",
    "    conf = probs.max(axis=1)\n",
    "    pred = probs.argmax(axis=1)\n",
    "    correct = (pred == y_true).astype(np.int32)\n",
    "\n",
    "    edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(conf, edges, right=True) - 1\n",
    "    bin_ids = np.clip(bin_ids, 0, n_bins - 1)\n",
    "\n",
    "    bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_count = np.zeros(n_bins, dtype=np.int32)\n",
    "\n",
    "    for b in range(n_bins):\n",
    "        m = (bin_ids == b)\n",
    "        bin_count[b] = int(m.sum())\n",
    "        if bin_count[b] > 0:\n",
    "            bin_conf[b] = float(conf[m].mean())\n",
    "            bin_acc[b] = float(correct[m].mean())\n",
    "        else:\n",
    "            bin_conf[b] = np.nan\n",
    "            bin_acc[b] = np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_bins\": int(n_bins),\n",
    "        \"edges\": edges.tolist(),\n",
    "        \"bin_conf\": bin_conf.tolist(),\n",
    "        \"bin_acc\": bin_acc.tolist(),\n",
    "        \"bin_count\": bin_count.tolist(),\n",
    "    }\n",
    "\n",
    "def score_histogram_by_correct(score: np.ndarray, correct: np.ndarray, n_bins: int = 30, lo: float = 0.0, hi: float = 1.0):\n",
    "    score = np.clip(score, lo, hi)\n",
    "    edges = np.linspace(lo, hi, n_bins + 1)\n",
    "\n",
    "    correct_mask = correct.astype(bool)\n",
    "    incorrect_mask = ~correct_mask\n",
    "\n",
    "    c_counts, _ = np.histogram(score[correct_mask], bins=edges)\n",
    "    i_counts, _ = np.histogram(score[incorrect_mask], bins=edges)\n",
    "\n",
    "    return {\n",
    "        \"n_bins\": int(n_bins),\n",
    "        \"edges\": edges.tolist(),\n",
    "        \"correct_counts\": c_counts.astype(int).tolist(),\n",
    "        \"incorrect_counts\": i_counts.astype(int).tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c1831ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. run one full experiment (train → evaluate → save results)\n",
    "def run_experiment(cfg: TrainConfig):\n",
    "    # Clear GPU cache at start\n",
    "    torch.cuda.empty_cache()\n",
    "    set_seed(cfg.seed)\n",
    "    \n",
    "    num_classes = get_num_classes(cfg.dataset)\n",
    "    train_loader, test_loader = build_loaders(cfg)\n",
    "\n",
    "    model = build_model(cfg.model_name, num_classes)\n",
    "\n",
    "    print(\"Training configuration:\")\n",
    "    print(json.dumps(asdict(cfg), indent=2))\n",
    "\n",
    "    # For corrupted datasets, try to load pre-trained clean model\n",
    "    tag = f\"{cfg.dataset}_{cfg.model_name}_e{cfg.epochs}_bs{cfg.batch_size}\"\n",
    "    ckpt_path = os.path.join(cfg.out_dir, f\"{tag}.pth\")\n",
    "    \n",
    "    # Map corrupted dataset names to their clean counterparts\n",
    "    clean_dataset_map = {\n",
    "        \"cifar10-c\": \"cifar10\",\n",
    "        \"cifar100-c\": \"cifar100\",\n",
    "        \"tinyimagenet-c\": \"tinyimagenet\"\n",
    "    }\n",
    "    \n",
    "    model_loaded = False\n",
    "    if cfg.dataset in clean_dataset_map:\n",
    "        # This is a corrupted dataset - try to load clean model\n",
    "        clean_dataset = clean_dataset_map[cfg.dataset]\n",
    "        clean_tag = f\"{clean_dataset}_{cfg.model_name}_e{cfg.epochs}_bs{cfg.batch_size}\"\n",
    "        clean_ckpt_path = os.path.join(cfg.out_dir, f\"{clean_tag}.pth\")\n",
    "        \n",
    "        if os.path.exists(clean_ckpt_path):\n",
    "            print(f\"\\n✓ Loading pre-trained model from {clean_ckpt_path}\")\n",
    "            model.load_state_dict(torch.load(clean_ckpt_path, map_location=DEVICE))\n",
    "            model_loaded = True\n",
    "            train_hist = []  # No training history since we skipped training\n",
    "        else:\n",
    "            print(f\"\\n⚠ Pre-trained model not found at {clean_ckpt_path}\")\n",
    "            print(f\"   Training from scratch on clean {clean_dataset} data...\")\n",
    "    \n",
    "    # Train if model wasn't loaded\n",
    "    if not model_loaded:\n",
    "        if os.path.exists(ckpt_path):\n",
    "            print(f\"\\n✓ Loading existing model from {ckpt_path}\")\n",
    "            model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "            train_hist = []  # No training history since we loaded existing model\n",
    "        else:\n",
    "            print(\"\\nTraining model from scratch...\")\n",
    "            model, train_hist = train_model(cfg, model, train_loader)\n",
    "            # Save final model\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "    else:\n",
    "        # For corrupted datasets, still save under the corrupted name for reference\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    # Unified evaluation with uncertainty\n",
    "    eval_pack = evaluate_with_uncertainty(cfg, model, test_loader)\n",
    "\n",
    "    # Build uncertainty scores\n",
    "    entropy = eval_pack[\"entropy\"]\n",
    "    grad = eval_pack[\"gradnorm\"]\n",
    "\n",
    "    # Entropy: normalize by ln(num_classes) to get [0,1] scale\n",
    "    entropy_n = np.clip(entropy / np.log(num_classes), 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "    # Gradient norm: min-max normalize to [0,1]\n",
    "    grad_n = minmax_norm(grad).astype(np.float32)\n",
    "\n",
    "    hybrid = (cfg.hybrid_weight_entropy * entropy_n) + (cfg.hybrid_weight_grad * grad_n)\n",
    "\n",
    "    method_scores = {\n",
    "        \"entropy\": entropy_n,\n",
    "        \"gradient\": grad_n,\n",
    "        \"hybrid\": hybrid.astype(np.float32)\n",
    "    }\n",
    "\n",
    "    # Metrics\n",
    "    metrics = compute_uncertainty_metrics(method_scores, eval_pack, cfg)\n",
    "    rej_table = percentile_rejection_table(method_scores, eval_pack, cfg)\n",
    "\n",
    "    # reliability + score distributions\n",
    "    reliability_summary = reliability_bins_from_probs(\n",
    "        probs=eval_pack[\"probs\"],\n",
    "        y_true=eval_pack[\"y_true\"],\n",
    "        n_bins=15\n",
    "    )\n",
    "\n",
    "    score_distributions = {\n",
    "        name: score_histogram_by_correct(score, eval_pack[\"correct\"], n_bins=30, lo=0.0, hi=1.0)\n",
    "        for name, score in method_scores.items()\n",
    "    }\n",
    "\n",
    "    # Consistency check\n",
    "    base_acc = eval_pack[\"test_acc\"]\n",
    "    acc_before = float((eval_pack[\"y_pred\"] == eval_pack[\"y_true\"]).mean())\n",
    "    print(f\"\\nConsistency check: test_acc={base_acc:.6f} vs accuracy_before_rejection={acc_before:.6f}\")\n",
    "\n",
    "    # Save artifacts\n",
    "    out = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"train_history\": train_hist,\n",
    "        \"baseline\": {\"test_acc\": eval_pack[\"test_acc\"], \"test_loss\": eval_pack[\"test_loss\"]},\n",
    "        \"uncertainty_metrics\": metrics,\n",
    "        \"percentile_rejection\": rej_table,\n",
    "        \"reliability\": reliability_summary,              \n",
    "        \"score_distributions\": score_distributions,     \n",
    "        \"checkpoint_path\": ckpt_path\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(cfg.out_dir, f\"{tag}_results.json\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"Saved results JSON: {out_path}\")\n",
    "\n",
    "    # Clear GPU cache at end\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c5108da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10 experiments:\n",
      "  1. tissuemnist     + efficientnetv2_s    \n",
      "  2. cifar10-c       + shufflenetv2_0_5    \n",
      "  3. cifar10-c       + mobilenetv3_small   \n",
      "  4. cifar10-c       + efficientnetv2_s    \n",
      "  5. cifar100-c      + shufflenetv2_0_5    \n",
      "  6. cifar100-c      + mobilenetv3_small   \n",
      "  7. cifar100-c      + efficientnetv2_s    \n",
      "  8. tinyimagenet-c  + shufflenetv2_0_5    \n",
      "  9. tinyimagenet-c  + mobilenetv3_small   \n",
      "  10. tinyimagenet-c  + efficientnetv2_s    \n"
     ]
    }
   ],
   "source": [
    "# 12. Define dataset-model pairs to run\n",
    "dataset_model_pairs = [\n",
    "    # Clean datasets\n",
    "    # (\"cifar10\", \"shufflenetv2_0_5\"),\n",
    "    # (\"cifar10\", \"mobilenetv3_small\"),\n",
    "    # (\"cifar10\", \"efficientnetv2_s\"),\n",
    "    # (\"cifar100\", \"shufflenetv2_0_5\"),\n",
    "    # (\"cifar100\", \"mobilenetv3_small\"),\n",
    "    # (\"cifar100\", \"efficientnetv2_s\"),\n",
    "    # (\"tinyimagenet\", \"shufflenetv2_0_5\"),\n",
    "    # (\"tinyimagenet\", \"mobilenetv3_small\"),\n",
    "    # (\"tinyimagenet\", \"efficientnetv2_s\"),\n",
    "    # (\"tissuemnist\", \"shufflenetv2_0_5\"),\n",
    "    # (\"tissuemnist\", \"mobilenetv3_small\"),\n",
    "    (\"tissuemnist\", \"efficientnetv2_s\"),\n",
    "    \n",
    "    # Corrupted datasets\n",
    "    (\"cifar10-c\", \"shufflenetv2_0_5\"),\n",
    "    (\"cifar10-c\", \"mobilenetv3_small\"),\n",
    "    (\"cifar10-c\", \"efficientnetv2_s\"),\n",
    "    (\"cifar100-c\", \"shufflenetv2_0_5\"),\n",
    "    (\"cifar100-c\", \"mobilenetv3_small\"),\n",
    "    (\"cifar100-c\", \"efficientnetv2_s\"),\n",
    "    (\"tinyimagenet-c\", \"shufflenetv2_0_5\"),\n",
    "    (\"tinyimagenet-c\", \"mobilenetv3_small\"),\n",
    "    (\"tinyimagenet-c\", \"efficientnetv2_s\"),\n",
    "]\n",
    "\n",
    "print(f\"Running {len(dataset_model_pairs)} experiments:\")\n",
    "for i, (ds, model) in enumerate(dataset_model_pairs, 1):\n",
    "    print(f\"  {i}. {ds:15} + {model:20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e318018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Experiment 1/10: tissuemnist + efficientnetv2_s\n",
      "================================================================================\n",
      "\n",
      "Training configuration:\n",
      "{\n",
      "  \"dataset\": \"tissuemnist\",\n",
      "  \"model_name\": \"efficientnetv2_s\",\n",
      "  \"data_root\": \"./data\",\n",
      "  \"tinyimagenet_root\": \"./tiny-imagenet-200\",\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 0,\n",
      "  \"lr\": 0.05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"seed\": 42,\n",
      "  \"reject_percentiles\": [\n",
      "    10,\n",
      "    20,\n",
      "    30,\n",
      "    40,\n",
      "    50\n",
      "  ],\n",
      "  \"hybrid_weight_entropy\": 0.5,\n",
      "  \"hybrid_weight_grad\": 0.5,\n",
      "  \"out_dir\": \"./outputs\"\n",
      "}\n",
      "\n",
      "Training model from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879192d5a88c4ce7831e8c09f2f97c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/50 | train_loss=1.6545 | train_acc=0.4273 | lr=0.049951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689e070879e348a49e002bc2d4edb588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002/50 | train_loss=1.3968 | train_acc=0.4862 | lr=0.049803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e1be6bc375437ea7df2b7efbaf9e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003/50 | train_loss=1.3749 | train_acc=0.4877 | lr=0.049557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34737fd69e464c459320697ffc9e65e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004/50 | train_loss=1.3406 | train_acc=0.4976 | lr=0.049215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3945433abfd7466a8ceb9ffed29b5fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005/50 | train_loss=1.3458 | train_acc=0.5014 | lr=0.048776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f21be9a0574e74ae9c21f90df67db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006/50 | train_loss=1.2876 | train_acc=0.5182 | lr=0.048244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370e2d8d5ade43faa260a1b75a34d22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007/50 | train_loss=1.3809 | train_acc=0.4835 | lr=0.047621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6054e9d58a447139b50a0dbe9f12bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008/50 | train_loss=1.3747 | train_acc=0.4899 | lr=0.046908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84b074ad2b14c1e99873466dd6bd70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009/50 | train_loss=1.3063 | train_acc=0.5124 | lr=0.046108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae7720bd9484a63b74c670c853bf01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010/50 | train_loss=1.2539 | train_acc=0.5300 | lr=0.045225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ab9ccc92e1455a825a21d59dbc8159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011/50 | train_loss=1.2301 | train_acc=0.5381 | lr=0.044263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922132f8d8d044cc820ee810ddc254db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012/50 | train_loss=1.1751 | train_acc=0.5584 | lr=0.043224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240721c892df485f93bff3302d7520bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013/50 | train_loss=1.1367 | train_acc=0.5760 | lr=0.042114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae485b545b340b783d2a8df34a91afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014/50 | train_loss=1.1054 | train_acc=0.5897 | lr=0.040936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0714f197f9a1418c8e2c7e0e61c08759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015/50 | train_loss=1.0744 | train_acc=0.6030 | lr=0.039695\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87680bcbf244a0caca9b8cf5e12c485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016/50 | train_loss=1.0521 | train_acc=0.6129 | lr=0.038396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6159c89b614a2381f048c84cc14f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017/50 | train_loss=1.0325 | train_acc=0.6192 | lr=0.037044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35a340a5ba14d9cb9f99e8fb258b884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018/50 | train_loss=1.0140 | train_acc=0.6274 | lr=0.035644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154f14bb20be483f865ec8ad542d91f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019/50 | train_loss=0.9965 | train_acc=0.6352 | lr=0.034203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cef6e29be2841cfb3f0673bc922dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020/50 | train_loss=0.9832 | train_acc=0.6392 | lr=0.032725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807bdddd406b4353a67c23bfafe4fbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021/50 | train_loss=0.9696 | train_acc=0.6447 | lr=0.031217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438ef00c09f24f46b99eec7085b07d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022/50 | train_loss=0.9557 | train_acc=0.6492 | lr=0.029685\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4d19a99f2e4e05b94aa2768d826e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023/50 | train_loss=0.9415 | train_acc=0.6546 | lr=0.028133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93120d8cc1e64d538cfa6f3ee0ba5a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024/50 | train_loss=0.9300 | train_acc=0.6593 | lr=0.026570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf88793633b40f5b60c1109ac15de32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025/50 | train_loss=0.9186 | train_acc=0.6646 | lr=0.025000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3986e116f5d4bbdaaf93e030b8fd8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026/50 | train_loss=0.9058 | train_acc=0.6690 | lr=0.023430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a22ed8e698c4470b15b7ea0661a8993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027/50 | train_loss=0.8939 | train_acc=0.6727 | lr=0.021867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fc6dbd3cb44801b660de96439583f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028/50 | train_loss=0.8840 | train_acc=0.6768 | lr=0.020315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1a02af694d437c886e8d59c732e8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029/50 | train_loss=0.8701 | train_acc=0.6813 | lr=0.018783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e567111ec8348659b8dbea26c2cde64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030/50 | train_loss=0.8597 | train_acc=0.6861 | lr=0.017275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c9e02ce58d4c4ebc34be7766452c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031/50 | train_loss=0.8460 | train_acc=0.6905 | lr=0.015797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afa100a13b642bc8e12b76884ec18e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032/50 | train_loss=0.8340 | train_acc=0.6965 | lr=0.014356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eda1399e6bf42e08302523a667df2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033/50 | train_loss=0.8207 | train_acc=0.7002 | lr=0.012956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0afb5ef2ff84e3ebb50ef48677daaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034/50 | train_loss=0.8058 | train_acc=0.7081 | lr=0.011604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be149e582154fb2ab0b3bdeeea686ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035/50 | train_loss=0.7918 | train_acc=0.7115 | lr=0.010305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a078900108ea4ef5a30edc6b2198ab9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036/50 | train_loss=0.7779 | train_acc=0.7163 | lr=0.009064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dde15b70f9466283360064d5793084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037/50 | train_loss=0.7607 | train_acc=0.7231 | lr=0.007886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67667ff3e3764ec8913354b896d0f86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038/50 | train_loss=0.7437 | train_acc=0.7295 | lr=0.006776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fcb08ecb8e4d95bd18db2bc2e68181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039/50 | train_loss=0.7254 | train_acc=0.7371 | lr=0.005737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464755a64e094930929b5ad4f117ad30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040/50 | train_loss=0.7056 | train_acc=0.7439 | lr=0.004775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb82986f26c4974a970d0738a3053aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041/50 | train_loss=0.6865 | train_acc=0.7508 | lr=0.003892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a476b64b744c958b4413bc3b05e0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042/50 | train_loss=0.6631 | train_acc=0.7602 | lr=0.003092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3065619a8fc4c32b4deed5773ca34cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043/50 | train_loss=0.6404 | train_acc=0.7690 | lr=0.002379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9cd250265b46118c24c60aee93c834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044/50 | train_loss=0.6175 | train_acc=0.7777 | lr=0.001756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8500b38c124077a9cc5daeaf75ba06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045/50 | train_loss=0.5931 | train_acc=0.7878 | lr=0.001224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e6c20552574331b1eb7f8d937773d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046/50 | train_loss=0.5686 | train_acc=0.7963 | lr=0.000785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae6c13279e84e67b6610f4f2f546b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047/50 | train_loss=0.5490 | train_acc=0.8038 | lr=0.000443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87463d3ea15411ea3b821d9ae25adcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048/50 | train_loss=0.5299 | train_acc=0.8120 | lr=0.000197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f3563d5e06411e9356d9f7946a1d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049/50 | train_loss=0.5160 | train_acc=0.8165 | lr=0.000049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e39cde3ca448dcb413bb81702076e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050/50 | train_loss=0.5086 | train_acc=0.8207 | lr=0.000000\n",
      "Saved checkpoint: ./outputs\\tissuemnist_efficientnetv2_s_e50_bs64.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf29bdd8300404fa6b380a6313066ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test+Uncertainty:   0%|          | 0/739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 50/739] loss_sum=6.8287e+01 batch_avg_loss=1.0670e+00 logits_max_abs=1.0697e+01 logits_mean=-3.1021e-04 nan=False inf=False\n",
      "[BATCH 100/739] loss_sum=5.9278e+01 batch_avg_loss=9.2622e-01 logits_max_abs=1.1182e+01 logits_mean=-1.8586e-04 nan=False inf=False\n",
      "[BATCH 150/739] loss_sum=6.1283e+01 batch_avg_loss=9.5755e-01 logits_max_abs=1.1644e+01 logits_mean=-1.6010e-05 nan=False inf=False\n",
      "[BATCH 200/739] loss_sum=5.2886e+01 batch_avg_loss=8.2634e-01 logits_max_abs=1.2195e+01 logits_mean=-2.0765e-04 nan=False inf=False\n",
      "[BATCH 250/739] loss_sum=9.8172e+01 batch_avg_loss=1.5339e+00 logits_max_abs=1.2886e+01 logits_mean=-2.3213e-04 nan=False inf=False\n",
      "[BATCH 300/739] loss_sum=6.4334e+01 batch_avg_loss=1.0052e+00 logits_max_abs=9.6771e+00 logits_mean=-1.8001e-04 nan=False inf=False\n",
      "[BATCH 350/739] loss_sum=7.1567e+01 batch_avg_loss=1.1182e+00 logits_max_abs=1.2160e+01 logits_mean=-1.2834e-04 nan=False inf=False\n",
      "[BATCH 400/739] loss_sum=6.2153e+01 batch_avg_loss=9.7114e-01 logits_max_abs=1.4442e+01 logits_mean=-2.6914e-04 nan=False inf=False\n",
      "[BATCH 450/739] loss_sum=6.7564e+01 batch_avg_loss=1.0557e+00 logits_max_abs=1.0688e+01 logits_mean=-2.3756e-04 nan=False inf=False\n",
      "[BATCH 500/739] loss_sum=7.3329e+01 batch_avg_loss=1.1458e+00 logits_max_abs=1.2212e+01 logits_mean=-3.9372e-04 nan=False inf=False\n",
      "[BATCH 550/739] loss_sum=6.8904e+01 batch_avg_loss=1.0766e+00 logits_max_abs=1.0973e+01 logits_mean=-9.1442e-05 nan=False inf=False\n",
      "[BATCH 600/739] loss_sum=5.4952e+01 batch_avg_loss=8.5862e-01 logits_max_abs=9.9011e+00 logits_mean=-2.2196e-04 nan=False inf=False\n",
      "[BATCH 650/739] loss_sum=7.8895e+01 batch_avg_loss=1.2327e+00 logits_max_abs=1.3599e+01 logits_mean=-2.0697e-04 nan=False inf=False\n",
      "[BATCH 700/739] loss_sum=7.2627e+01 batch_avg_loss=1.1348e+00 logits_max_abs=1.0865e+01 logits_mean=-1.6911e-04 nan=False inf=False\n",
      "=== EVAL DIAGNOSTICS ===\n",
      "total_samples=47280 total_loss_sum=5.2528e+04 test_loss_avg=1.1110e+00 test_acc=0.6466\n",
      "max_abs_logit=1.8099e+01 saw_nan=False saw_inf=False\n",
      "\n",
      "Consistency check: test_acc=0.646616 vs accuracy_before_rejection=0.646616\n",
      "Saved results JSON: ./outputs\\tissuemnist_efficientnetv2_s_e50_bs64_results.json\n",
      "\n",
      "================================================================================\n",
      "Experiment 2/10: cifar10-c + shufflenetv2_0_5\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded CIFAR-10-C: 19 corruptions, severity 5, 190000 total images\n",
      "Training configuration:\n",
      "{\n",
      "  \"dataset\": \"cifar10-c\",\n",
      "  \"model_name\": \"shufflenetv2_0_5\",\n",
      "  \"data_root\": \"./data\",\n",
      "  \"tinyimagenet_root\": \"./tiny-imagenet-200\",\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 0,\n",
      "  \"lr\": 0.05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"seed\": 42,\n",
      "  \"reject_percentiles\": [\n",
      "    10,\n",
      "    20,\n",
      "    30,\n",
      "    40,\n",
      "    50\n",
      "  ],\n",
      "  \"hybrid_weight_entropy\": 0.5,\n",
      "  \"hybrid_weight_grad\": 0.5,\n",
      "  \"out_dir\": \"./outputs\"\n",
      "}\n",
      "\n",
      "✓ Loading pre-trained model from ./outputs\\cifar10_shufflenetv2_0_5_e50_bs64.pth\n",
      "Saved checkpoint: ./outputs\\cifar10-c_shufflenetv2_0_5_e50_bs64.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da15080beea4bfd90aa0415a066501b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test+Uncertainty:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 50/2969] loss_sum=9.8565e+02 batch_avg_loss=1.5401e+01 logits_max_abs=1.9852e+01 logits_mean=2.4665e-02 nan=False inf=False\n",
      "[BATCH 100/2969] loss_sum=9.4573e+02 batch_avg_loss=1.4777e+01 logits_max_abs=1.8122e+01 logits_mean=2.2849e-02 nan=False inf=False\n",
      "[BATCH 150/2969] loss_sum=1.0170e+03 batch_avg_loss=1.5891e+01 logits_max_abs=1.7245e+01 logits_mean=2.3681e-02 nan=False inf=False\n",
      "[BATCH 200/2969] loss_sum=1.0158e+03 batch_avg_loss=1.5872e+01 logits_max_abs=2.4342e+01 logits_mean=2.1999e-02 nan=False inf=False\n",
      "[BATCH 250/2969] loss_sum=1.1173e+03 batch_avg_loss=1.7458e+01 logits_max_abs=1.7731e+01 logits_mean=2.3233e-02 nan=False inf=False\n",
      "[BATCH 300/2969] loss_sum=9.6484e+02 batch_avg_loss=1.5076e+01 logits_max_abs=2.1500e+01 logits_mean=2.2441e-02 nan=False inf=False\n",
      "[BATCH 350/2969] loss_sum=1.0327e+03 batch_avg_loss=1.6136e+01 logits_max_abs=2.0634e+01 logits_mean=2.8022e-02 nan=False inf=False\n",
      "[BATCH 400/2969] loss_sum=1.0332e+03 batch_avg_loss=1.6143e+01 logits_max_abs=2.0202e+01 logits_mean=2.6126e-02 nan=False inf=False\n",
      "[BATCH 450/2969] loss_sum=1.0556e+03 batch_avg_loss=1.6494e+01 logits_max_abs=2.2184e+01 logits_mean=2.6749e-02 nan=False inf=False\n",
      "[BATCH 500/2969] loss_sum=1.0606e+03 batch_avg_loss=1.6572e+01 logits_max_abs=2.1926e+01 logits_mean=2.6427e-02 nan=False inf=False\n",
      "[BATCH 550/2969] loss_sum=1.0067e+03 batch_avg_loss=1.5730e+01 logits_max_abs=2.1107e+01 logits_mean=2.7802e-02 nan=False inf=False\n",
      "[BATCH 600/2969] loss_sum=1.0564e+03 batch_avg_loss=1.6507e+01 logits_max_abs=2.2860e+01 logits_mean=2.7128e-02 nan=False inf=False\n",
      "[BATCH 650/2969] loss_sum=9.6505e+02 batch_avg_loss=1.5079e+01 logits_max_abs=1.7192e+01 logits_mean=2.6314e-02 nan=False inf=False\n",
      "[BATCH 700/2969] loss_sum=1.0368e+03 batch_avg_loss=1.6199e+01 logits_max_abs=1.7227e+01 logits_mean=2.7109e-02 nan=False inf=False\n",
      "[BATCH 750/2969] loss_sum=1.0121e+03 batch_avg_loss=1.5814e+01 logits_max_abs=1.7278e+01 logits_mean=2.7273e-02 nan=False inf=False\n",
      "[BATCH 800/2969] loss_sum=9.3512e+02 batch_avg_loss=1.4611e+01 logits_max_abs=1.7292e+01 logits_mean=2.7277e-02 nan=False inf=False\n",
      "[BATCH 850/2969] loss_sum=9.4541e+02 batch_avg_loss=1.4772e+01 logits_max_abs=1.7583e+01 logits_mean=2.6138e-02 nan=False inf=False\n",
      "[BATCH 900/2969] loss_sum=1.0081e+03 batch_avg_loss=1.5751e+01 logits_max_abs=1.8302e+01 logits_mean=2.7349e-02 nan=False inf=False\n",
      "[BATCH 950/2969] loss_sum=9.5755e+02 batch_avg_loss=1.4962e+01 logits_max_abs=1.9091e+01 logits_mean=2.5427e-02 nan=False inf=False\n",
      "[BATCH 1000/2969] loss_sum=1.0664e+03 batch_avg_loss=1.6663e+01 logits_max_abs=2.0775e+01 logits_mean=2.6022e-02 nan=False inf=False\n",
      "[BATCH 1050/2969] loss_sum=1.0334e+03 batch_avg_loss=1.6147e+01 logits_max_abs=2.0544e+01 logits_mean=2.6454e-02 nan=False inf=False\n",
      "[BATCH 1100/2969] loss_sum=8.8815e+02 batch_avg_loss=1.3877e+01 logits_max_abs=1.9706e+01 logits_mean=2.3432e-02 nan=False inf=False\n",
      "[BATCH 1150/2969] loss_sum=9.6480e+02 batch_avg_loss=1.5075e+01 logits_max_abs=1.8007e+01 logits_mean=2.4478e-02 nan=False inf=False\n",
      "[BATCH 1200/2969] loss_sum=9.7311e+02 batch_avg_loss=1.5205e+01 logits_max_abs=1.8233e+01 logits_mean=2.3505e-02 nan=False inf=False\n",
      "[BATCH 1250/2969] loss_sum=8.9563e+02 batch_avg_loss=1.3994e+01 logits_max_abs=1.7756e+01 logits_mean=2.3403e-02 nan=False inf=False\n",
      "[BATCH 1300/2969] loss_sum=1.0219e+03 batch_avg_loss=1.5967e+01 logits_max_abs=2.0965e+01 logits_mean=2.6904e-02 nan=False inf=False\n",
      "[BATCH 1350/2969] loss_sum=9.7449e+02 batch_avg_loss=1.5226e+01 logits_max_abs=1.9230e+01 logits_mean=2.6396e-02 nan=False inf=False\n",
      "[BATCH 1400/2969] loss_sum=1.0494e+03 batch_avg_loss=1.6397e+01 logits_max_abs=1.9842e+01 logits_mean=2.6458e-02 nan=False inf=False\n",
      "[BATCH 1450/2969] loss_sum=1.0865e+03 batch_avg_loss=1.6976e+01 logits_max_abs=2.2750e+01 logits_mean=2.8453e-02 nan=False inf=False\n",
      "[BATCH 1500/2969] loss_sum=1.1743e+03 batch_avg_loss=1.8349e+01 logits_max_abs=1.8256e+01 logits_mean=2.7377e-02 nan=False inf=False\n",
      "[BATCH 1550/2969] loss_sum=1.0286e+03 batch_avg_loss=1.6071e+01 logits_max_abs=1.9646e+01 logits_mean=2.7468e-02 nan=False inf=False\n",
      "[BATCH 1600/2969] loss_sum=1.0369e+03 batch_avg_loss=1.6202e+01 logits_max_abs=2.0348e+01 logits_mean=2.9022e-02 nan=False inf=False\n",
      "[BATCH 1650/2969] loss_sum=1.0492e+03 batch_avg_loss=1.6394e+01 logits_max_abs=2.0118e+01 logits_mean=2.7911e-02 nan=False inf=False\n",
      "[BATCH 1700/2969] loss_sum=1.0702e+03 batch_avg_loss=1.6721e+01 logits_max_abs=2.2768e+01 logits_mean=2.8162e-02 nan=False inf=False\n",
      "[BATCH 1750/2969] loss_sum=1.0485e+03 batch_avg_loss=1.6384e+01 logits_max_abs=2.2045e+01 logits_mean=2.5480e-02 nan=False inf=False\n",
      "[BATCH 1800/2969] loss_sum=9.8639e+02 batch_avg_loss=1.5412e+01 logits_max_abs=2.0996e+01 logits_mean=2.6178e-02 nan=False inf=False\n",
      "[BATCH 1850/2969] loss_sum=1.0421e+03 batch_avg_loss=1.6282e+01 logits_max_abs=2.2085e+01 logits_mean=2.5665e-02 nan=False inf=False\n",
      "[BATCH 1900/2969] loss_sum=1.0293e+03 batch_avg_loss=1.6082e+01 logits_max_abs=2.4008e+01 logits_mean=2.6163e-02 nan=False inf=False\n",
      "[BATCH 1950/2969] loss_sum=1.0654e+03 batch_avg_loss=1.6648e+01 logits_max_abs=2.1814e+01 logits_mean=2.7622e-02 nan=False inf=False\n",
      "[BATCH 2000/2969] loss_sum=1.0373e+03 batch_avg_loss=1.6207e+01 logits_max_abs=1.8819e+01 logits_mean=2.6532e-02 nan=False inf=False\n",
      "[BATCH 2050/2969] loss_sum=1.1475e+03 batch_avg_loss=1.7930e+01 logits_max_abs=2.5036e+01 logits_mean=2.8361e-02 nan=False inf=False\n",
      "[BATCH 2100/2969] loss_sum=1.1623e+03 batch_avg_loss=1.8161e+01 logits_max_abs=2.3535e+01 logits_mean=2.4696e-02 nan=False inf=False\n",
      "[BATCH 2150/2969] loss_sum=1.2043e+03 batch_avg_loss=1.8817e+01 logits_max_abs=2.5659e+01 logits_mean=2.9163e-02 nan=False inf=False\n",
      "[BATCH 2200/2969] loss_sum=9.1459e+02 batch_avg_loss=1.4291e+01 logits_max_abs=1.9315e+01 logits_mean=2.3203e-02 nan=False inf=False\n",
      "[BATCH 2250/2969] loss_sum=1.0352e+03 batch_avg_loss=1.6175e+01 logits_max_abs=1.9216e+01 logits_mean=2.3841e-02 nan=False inf=False\n",
      "[BATCH 2300/2969] loss_sum=9.6833e+02 batch_avg_loss=1.5130e+01 logits_max_abs=1.9244e+01 logits_mean=2.4219e-02 nan=False inf=False\n",
      "[BATCH 2350/2969] loss_sum=8.6556e+02 batch_avg_loss=1.3524e+01 logits_max_abs=1.8260e+01 logits_mean=2.6709e-02 nan=False inf=False\n",
      "[BATCH 2400/2969] loss_sum=9.1827e+02 batch_avg_loss=1.4348e+01 logits_max_abs=1.8085e+01 logits_mean=2.7449e-02 nan=False inf=False\n",
      "[BATCH 2450/2969] loss_sum=9.4793e+02 batch_avg_loss=1.4811e+01 logits_max_abs=1.6988e+01 logits_mean=2.8147e-02 nan=False inf=False\n",
      "[BATCH 2500/2969] loss_sum=8.5517e+02 batch_avg_loss=1.3362e+01 logits_max_abs=1.7259e+01 logits_mean=2.7127e-02 nan=False inf=False\n",
      "[BATCH 2550/2969] loss_sum=1.0454e+03 batch_avg_loss=1.6334e+01 logits_max_abs=2.0494e+01 logits_mean=2.9760e-02 nan=False inf=False\n",
      "[BATCH 2600/2969] loss_sum=1.0231e+03 batch_avg_loss=1.5986e+01 logits_max_abs=1.9957e+01 logits_mean=2.9423e-02 nan=False inf=False\n",
      "[BATCH 2650/2969] loss_sum=1.0967e+03 batch_avg_loss=1.7136e+01 logits_max_abs=2.0543e+01 logits_mean=2.8896e-02 nan=False inf=False\n",
      "[BATCH 2700/2969] loss_sum=1.0338e+03 batch_avg_loss=1.6153e+01 logits_max_abs=2.3920e+01 logits_mean=2.4886e-02 nan=False inf=False\n",
      "[BATCH 2750/2969] loss_sum=1.1129e+03 batch_avg_loss=1.7389e+01 logits_max_abs=1.7759e+01 logits_mean=2.4615e-02 nan=False inf=False\n",
      "[BATCH 2800/2969] loss_sum=9.8134e+02 batch_avg_loss=1.5333e+01 logits_max_abs=1.9076e+01 logits_mean=2.4723e-02 nan=False inf=False\n",
      "[BATCH 2850/2969] loss_sum=1.0273e+03 batch_avg_loss=1.6052e+01 logits_max_abs=2.1288e+01 logits_mean=2.8094e-02 nan=False inf=False\n",
      "[BATCH 2900/2969] loss_sum=1.0355e+03 batch_avg_loss=1.6180e+01 logits_max_abs=2.0234e+01 logits_mean=2.6793e-02 nan=False inf=False\n",
      "[BATCH 2950/2969] loss_sum=1.0508e+03 batch_avg_loss=1.6418e+01 logits_max_abs=2.2069e+01 logits_mean=2.6469e-02 nan=False inf=False\n",
      "=== EVAL DIAGNOSTICS ===\n",
      "total_samples=190000 total_loss_sum=3.0450e+06 test_loss_avg=1.6027e+01 test_acc=0.1243\n",
      "max_abs_logit=2.7420e+01 saw_nan=False saw_inf=False\n",
      "\n",
      "Consistency check: test_acc=0.124326 vs accuracy_before_rejection=0.124326\n",
      "Saved results JSON: ./outputs\\cifar10-c_shufflenetv2_0_5_e50_bs64_results.json\n",
      "\n",
      "================================================================================\n",
      "Experiment 3/10: cifar10-c + mobilenetv3_small\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded CIFAR-10-C: 19 corruptions, severity 5, 190000 total images\n",
      "Training configuration:\n",
      "{\n",
      "  \"dataset\": \"cifar10-c\",\n",
      "  \"model_name\": \"mobilenetv3_small\",\n",
      "  \"data_root\": \"./data\",\n",
      "  \"tinyimagenet_root\": \"./tiny-imagenet-200\",\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 0,\n",
      "  \"lr\": 0.05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"seed\": 42,\n",
      "  \"reject_percentiles\": [\n",
      "    10,\n",
      "    20,\n",
      "    30,\n",
      "    40,\n",
      "    50\n",
      "  ],\n",
      "  \"hybrid_weight_entropy\": 0.5,\n",
      "  \"hybrid_weight_grad\": 0.5,\n",
      "  \"out_dir\": \"./outputs\"\n",
      "}\n",
      "\n",
      "✓ Loading pre-trained model from ./outputs\\cifar10_mobilenetv3_small_e50_bs64.pth\n",
      "Saved checkpoint: ./outputs\\cifar10-c_mobilenetv3_small_e50_bs64.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31646893f2c747e8b58938015836c76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test+Uncertainty:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 50/2969] loss_sum=2.3081e+02 batch_avg_loss=3.6063e+00 logits_max_abs=7.2321e+00 logits_mean=9.2990e-05 nan=False inf=False\n",
      "[BATCH 100/2969] loss_sum=2.1384e+02 batch_avg_loss=3.3413e+00 logits_max_abs=6.1923e+00 logits_mean=-8.3705e-05 nan=False inf=False\n",
      "[BATCH 150/2969] loss_sum=2.1575e+02 batch_avg_loss=3.3710e+00 logits_max_abs=5.0071e+00 logits_mean=-1.9174e-04 nan=False inf=False\n",
      "[BATCH 200/2969] loss_sum=2.5237e+02 batch_avg_loss=3.9432e+00 logits_max_abs=8.9917e+00 logits_mean=-5.9755e-03 nan=False inf=False\n",
      "[BATCH 250/2969] loss_sum=2.7487e+02 batch_avg_loss=4.2949e+00 logits_max_abs=6.1451e+00 logits_mean=-7.8064e-03 nan=False inf=False\n",
      "[BATCH 300/2969] loss_sum=2.5639e+02 batch_avg_loss=4.0060e+00 logits_max_abs=9.1263e+00 logits_mean=-7.4939e-03 nan=False inf=False\n",
      "[BATCH 350/2969] loss_sum=2.3705e+02 batch_avg_loss=3.7040e+00 logits_max_abs=5.3618e+00 logits_mean=-5.1426e-03 nan=False inf=False\n",
      "[BATCH 400/2969] loss_sum=2.2940e+02 batch_avg_loss=3.5843e+00 logits_max_abs=5.1311e+00 logits_mean=-3.6478e-03 nan=False inf=False\n",
      "[BATCH 450/2969] loss_sum=2.3156e+02 batch_avg_loss=3.6182e+00 logits_max_abs=7.0210e+00 logits_mean=-3.9872e-03 nan=False inf=False\n",
      "[BATCH 500/2969] loss_sum=2.1389e+02 batch_avg_loss=3.3420e+00 logits_max_abs=5.7664e+00 logits_mean=-3.4401e-03 nan=False inf=False\n",
      "[BATCH 550/2969] loss_sum=2.2096e+02 batch_avg_loss=3.4525e+00 logits_max_abs=5.3833e+00 logits_mean=-3.1821e-03 nan=False inf=False\n",
      "[BATCH 600/2969] loss_sum=2.4753e+02 batch_avg_loss=3.8676e+00 logits_max_abs=4.9701e+00 logits_mean=-4.0800e-03 nan=False inf=False\n",
      "[BATCH 650/2969] loss_sum=2.2261e+02 batch_avg_loss=3.4783e+00 logits_max_abs=5.4261e+00 logits_mean=-6.7699e-03 nan=False inf=False\n",
      "[BATCH 700/2969] loss_sum=2.5251e+02 batch_avg_loss=3.9454e+00 logits_max_abs=5.1940e+00 logits_mean=-6.8407e-03 nan=False inf=False\n",
      "[BATCH 750/2969] loss_sum=2.7504e+02 batch_avg_loss=4.2976e+00 logits_max_abs=4.8669e+00 logits_mean=-8.0157e-03 nan=False inf=False\n",
      "[BATCH 800/2969] loss_sum=2.4279e+02 batch_avg_loss=3.7936e+00 logits_max_abs=4.4828e+00 logits_mean=-4.8789e-03 nan=False inf=False\n",
      "[BATCH 850/2969] loss_sum=2.1655e+02 batch_avg_loss=3.3836e+00 logits_max_abs=5.9532e+00 logits_mean=-4.7366e-03 nan=False inf=False\n",
      "[BATCH 900/2969] loss_sum=2.4554e+02 batch_avg_loss=3.8366e+00 logits_max_abs=4.9054e+00 logits_mean=-4.4019e-03 nan=False inf=False\n",
      "[BATCH 950/2969] loss_sum=2.1177e+02 batch_avg_loss=3.3089e+00 logits_max_abs=6.0105e+00 logits_mean=-3.7900e-03 nan=False inf=False\n",
      "[BATCH 1000/2969] loss_sum=2.6651e+02 batch_avg_loss=4.1642e+00 logits_max_abs=6.3542e+00 logits_mean=-4.4131e-03 nan=False inf=False\n",
      "[BATCH 1050/2969] loss_sum=2.2901e+02 batch_avg_loss=3.5782e+00 logits_max_abs=5.3891e+00 logits_mean=-4.0061e-03 nan=False inf=False\n",
      "[BATCH 1100/2969] loss_sum=2.5962e+02 batch_avg_loss=4.0565e+00 logits_max_abs=7.8652e+00 logits_mean=2.7421e-03 nan=False inf=False\n",
      "[BATCH 1150/2969] loss_sum=2.7558e+02 batch_avg_loss=4.3060e+00 logits_max_abs=7.7874e+00 logits_mean=3.4330e-03 nan=False inf=False\n",
      "[BATCH 1200/2969] loss_sum=2.8492e+02 batch_avg_loss=4.4518e+00 logits_max_abs=6.9017e+00 logits_mean=2.9926e-03 nan=False inf=False\n",
      "[BATCH 1250/2969] loss_sum=2.5507e+02 batch_avg_loss=3.9855e+00 logits_max_abs=6.5124e+00 logits_mean=3.2815e-03 nan=False inf=False\n",
      "[BATCH 1300/2969] loss_sum=2.5632e+02 batch_avg_loss=4.0050e+00 logits_max_abs=5.7414e+00 logits_mean=-2.7885e-03 nan=False inf=False\n",
      "[BATCH 1350/2969] loss_sum=2.3433e+02 batch_avg_loss=3.6614e+00 logits_max_abs=4.8669e+00 logits_mean=-3.1163e-03 nan=False inf=False\n",
      "[BATCH 1400/2969] loss_sum=2.3389e+02 batch_avg_loss=3.6545e+00 logits_max_abs=5.2025e+00 logits_mean=-3.0429e-03 nan=False inf=False\n",
      "[BATCH 1450/2969] loss_sum=2.6790e+02 batch_avg_loss=4.1859e+00 logits_max_abs=7.1371e+00 logits_mean=5.3827e-03 nan=False inf=False\n",
      "[BATCH 1500/2969] loss_sum=2.9519e+02 batch_avg_loss=4.6123e+00 logits_max_abs=6.4706e+00 logits_mean=4.4172e-03 nan=False inf=False\n",
      "[BATCH 1550/2969] loss_sum=2.5532e+02 batch_avg_loss=3.9894e+00 logits_max_abs=7.3986e+00 logits_mean=5.0960e-03 nan=False inf=False\n",
      "[BATCH 1600/2969] loss_sum=2.3972e+02 batch_avg_loss=3.7455e+00 logits_max_abs=6.5545e+00 logits_mean=-4.0385e-03 nan=False inf=False\n",
      "[BATCH 1650/2969] loss_sum=2.2794e+02 batch_avg_loss=3.5616e+00 logits_max_abs=5.2740e+00 logits_mean=-2.8787e-03 nan=False inf=False\n",
      "[BATCH 1700/2969] loss_sum=2.2773e+02 batch_avg_loss=3.5583e+00 logits_max_abs=8.8620e+00 logits_mean=-2.8803e-03 nan=False inf=False\n",
      "[BATCH 1750/2969] loss_sum=2.1984e+02 batch_avg_loss=3.4349e+00 logits_max_abs=6.0313e+00 logits_mean=-4.7647e-03 nan=False inf=False\n",
      "[BATCH 1800/2969] loss_sum=2.2618e+02 batch_avg_loss=3.5340e+00 logits_max_abs=5.3613e+00 logits_mean=-4.2183e-03 nan=False inf=False\n",
      "[BATCH 1850/2969] loss_sum=2.5035e+02 batch_avg_loss=3.9117e+00 logits_max_abs=5.2622e+00 logits_mean=-5.2261e-03 nan=False inf=False\n",
      "[BATCH 1900/2969] loss_sum=2.0007e+02 batch_avg_loss=3.1261e+00 logits_max_abs=6.6603e+00 logits_mean=-2.4422e-03 nan=False inf=False\n",
      "[BATCH 1950/2969] loss_sum=2.3161e+02 batch_avg_loss=3.6189e+00 logits_max_abs=5.6707e+00 logits_mean=-2.6686e-03 nan=False inf=False\n",
      "[BATCH 2000/2969] loss_sum=2.4594e+02 batch_avg_loss=3.8427e+00 logits_max_abs=5.3866e+00 logits_mean=-3.7713e-03 nan=False inf=False\n",
      "[BATCH 2050/2969] loss_sum=2.4738e+02 batch_avg_loss=3.8653e+00 logits_max_abs=9.8745e+00 logits_mean=-4.7480e-04 nan=False inf=False\n",
      "[BATCH 2100/2969] loss_sum=2.5249e+02 batch_avg_loss=3.9451e+00 logits_max_abs=8.7227e+00 logits_mean=-1.6267e-04 nan=False inf=False\n",
      "[BATCH 2150/2969] loss_sum=2.5467e+02 batch_avg_loss=3.9792e+00 logits_max_abs=9.0052e+00 logits_mean=3.7843e-04 nan=False inf=False\n",
      "[BATCH 2200/2969] loss_sum=2.4344e+02 batch_avg_loss=3.8038e+00 logits_max_abs=7.5064e+00 logits_mean=3.0325e-03 nan=False inf=False\n",
      "[BATCH 2250/2969] loss_sum=3.1266e+02 batch_avg_loss=4.8853e+00 logits_max_abs=8.2127e+00 logits_mean=2.4354e-03 nan=False inf=False\n",
      "[BATCH 2300/2969] loss_sum=2.7096e+02 batch_avg_loss=4.2337e+00 logits_max_abs=7.0866e+00 logits_mean=2.1939e-03 nan=False inf=False\n",
      "[BATCH 2350/2969] loss_sum=2.0510e+02 batch_avg_loss=3.2047e+00 logits_max_abs=5.1703e+00 logits_mean=-1.1435e-03 nan=False inf=False\n",
      "[BATCH 2400/2969] loss_sum=2.1318e+02 batch_avg_loss=3.3309e+00 logits_max_abs=4.9545e+00 logits_mean=-1.5916e-03 nan=False inf=False\n",
      "[BATCH 2450/2969] loss_sum=2.2850e+02 batch_avg_loss=3.5703e+00 logits_max_abs=5.0302e+00 logits_mean=-1.6680e-03 nan=False inf=False\n",
      "[BATCH 2500/2969] loss_sum=2.0252e+02 batch_avg_loss=3.1644e+00 logits_max_abs=5.0282e+00 logits_mean=-8.9136e-04 nan=False inf=False\n",
      "[BATCH 2550/2969] loss_sum=2.5596e+02 batch_avg_loss=3.9993e+00 logits_max_abs=5.8074e+00 logits_mean=-7.0521e-04 nan=False inf=False\n",
      "[BATCH 2600/2969] loss_sum=2.3410e+02 batch_avg_loss=3.6578e+00 logits_max_abs=5.3702e+00 logits_mean=-7.6660e-04 nan=False inf=False\n",
      "[BATCH 2650/2969] loss_sum=2.4343e+02 batch_avg_loss=3.8036e+00 logits_max_abs=5.6778e+00 logits_mean=-8.5414e-04 nan=False inf=False\n",
      "[BATCH 2700/2969] loss_sum=2.6418e+02 batch_avg_loss=4.1279e+00 logits_max_abs=7.6622e+00 logits_mean=2.4646e-03 nan=False inf=False\n",
      "[BATCH 2750/2969] loss_sum=2.9260e+02 batch_avg_loss=4.5719e+00 logits_max_abs=7.8885e+00 logits_mean=2.0920e-03 nan=False inf=False\n",
      "[BATCH 2800/2969] loss_sum=2.4732e+02 batch_avg_loss=3.8644e+00 logits_max_abs=7.9508e+00 logits_mean=1.8222e-03 nan=False inf=False\n",
      "[BATCH 2850/2969] loss_sum=2.3556e+02 batch_avg_loss=3.6806e+00 logits_max_abs=5.3261e+00 logits_mean=-4.8365e-03 nan=False inf=False\n",
      "[BATCH 2900/2969] loss_sum=2.2692e+02 batch_avg_loss=3.5457e+00 logits_max_abs=5.4155e+00 logits_mean=-3.3093e-03 nan=False inf=False\n",
      "[BATCH 2950/2969] loss_sum=2.3038e+02 batch_avg_loss=3.5997e+00 logits_max_abs=7.0445e+00 logits_mean=-3.7966e-03 nan=False inf=False\n",
      "=== EVAL DIAGNOSTICS ===\n",
      "total_samples=190000 total_loss_sum=7.1853e+05 test_loss_avg=3.7817e+00 test_acc=0.1072\n",
      "max_abs_logit=1.2100e+01 saw_nan=False saw_inf=False\n",
      "\n",
      "Consistency check: test_acc=0.107242 vs accuracy_before_rejection=0.107242\n",
      "Saved results JSON: ./outputs\\cifar10-c_mobilenetv3_small_e50_bs64_results.json\n",
      "\n",
      "================================================================================\n",
      "Experiment 4/10: cifar10-c + efficientnetv2_s\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded CIFAR-10-C: 19 corruptions, severity 5, 190000 total images\n",
      "Training configuration:\n",
      "{\n",
      "  \"dataset\": \"cifar10-c\",\n",
      "  \"model_name\": \"efficientnetv2_s\",\n",
      "  \"data_root\": \"./data\",\n",
      "  \"tinyimagenet_root\": \"./tiny-imagenet-200\",\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 0,\n",
      "  \"lr\": 0.05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"seed\": 42,\n",
      "  \"reject_percentiles\": [\n",
      "    10,\n",
      "    20,\n",
      "    30,\n",
      "    40,\n",
      "    50\n",
      "  ],\n",
      "  \"hybrid_weight_entropy\": 0.5,\n",
      "  \"hybrid_weight_grad\": 0.5,\n",
      "  \"out_dir\": \"./outputs\"\n",
      "}\n",
      "\n",
      "✓ Loading pre-trained model from ./outputs\\cifar10_efficientnetv2_s_e50_bs64.pth\n",
      "Saved checkpoint: ./outputs\\cifar10-c_efficientnetv2_s_e50_bs64.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77e45df29d64901a26cb9a211c846d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test+Uncertainty:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 50/2969] loss_sum=4.6076e+05 batch_avg_loss=7.1994e+03 logits_max_abs=1.4193e+05 logits_mean=-6.7399e+01 nan=False inf=False\n",
      "[BATCH 100/2969] loss_sum=3.8004e+05 batch_avg_loss=5.9381e+03 logits_max_abs=1.2251e+05 logits_mean=-6.4230e+01 nan=False inf=False\n",
      "[BATCH 150/2969] loss_sum=6.8041e+05 batch_avg_loss=1.0631e+04 logits_max_abs=1.2499e+05 logits_mean=-1.0014e+02 nan=False inf=False\n",
      "[BATCH 200/2969] loss_sum=1.4023e+06 batch_avg_loss=2.1911e+04 logits_max_abs=2.2807e+05 logits_mean=-2.0846e+02 nan=False inf=False\n",
      "[BATCH 250/2969] loss_sum=7.4174e+05 batch_avg_loss=1.1590e+04 logits_max_abs=1.5467e+05 logits_mean=-1.1269e+02 nan=False inf=False\n",
      "[BATCH 300/2969] loss_sum=6.9903e+05 batch_avg_loss=1.0922e+04 logits_max_abs=2.5956e+05 logits_mean=-1.1296e+02 nan=False inf=False\n",
      "[BATCH 350/2969] loss_sum=6.6254e+05 batch_avg_loss=1.0352e+04 logits_max_abs=1.4663e+05 logits_mean=-9.6506e+01 nan=False inf=False\n",
      "[BATCH 400/2969] loss_sum=5.4487e+05 batch_avg_loss=8.5136e+03 logits_max_abs=9.0487e+04 logits_mean=-9.2420e+01 nan=False inf=False\n",
      "[BATCH 450/2969] loss_sum=1.0758e+06 batch_avg_loss=1.6810e+04 logits_max_abs=1.0759e+05 logits_mean=-1.4218e+02 nan=False inf=False\n",
      "[BATCH 500/2969] loss_sum=7.1633e+05 batch_avg_loss=1.1193e+04 logits_max_abs=1.5900e+05 logits_mean=-1.0539e+02 nan=False inf=False\n",
      "[BATCH 550/2969] loss_sum=5.7161e+05 batch_avg_loss=8.9314e+03 logits_max_abs=7.6766e+04 logits_mean=-8.4939e+01 nan=False inf=False\n",
      "[BATCH 600/2969] loss_sum=8.3412e+05 batch_avg_loss=1.3033e+04 logits_max_abs=1.4268e+05 logits_mean=-1.2267e+02 nan=False inf=False\n",
      "[BATCH 650/2969] loss_sum=1.3208e+05 batch_avg_loss=2.0637e+03 logits_max_abs=2.4203e+04 logits_mean=-2.0641e+01 nan=False inf=False\n",
      "[BATCH 700/2969] loss_sum=7.8094e+04 batch_avg_loss=1.2202e+03 logits_max_abs=3.2821e+04 logits_mean=-1.0084e+01 nan=False inf=False\n",
      "[BATCH 750/2969] loss_sum=3.6779e+04 batch_avg_loss=5.7467e+02 logits_max_abs=9.0965e+03 logits_mean=-5.2546e+00 nan=False inf=False\n",
      "[BATCH 800/2969] loss_sum=8.8554e+04 batch_avg_loss=1.3837e+03 logits_max_abs=1.5595e+04 logits_mean=-1.2284e+01 nan=False inf=False\n",
      "[BATCH 850/2969] loss_sum=6.9838e+04 batch_avg_loss=1.0912e+03 logits_max_abs=3.8074e+04 logits_mean=-1.1220e+01 nan=False inf=False\n",
      "[BATCH 900/2969] loss_sum=1.1041e+05 batch_avg_loss=1.7251e+03 logits_max_abs=3.3071e+04 logits_mean=-1.4029e+01 nan=False inf=False\n",
      "[BATCH 950/2969] loss_sum=8.8367e+05 batch_avg_loss=1.3807e+04 logits_max_abs=1.7641e+05 logits_mean=-1.3501e+02 nan=False inf=False\n",
      "[BATCH 1000/2969] loss_sum=1.1421e+06 batch_avg_loss=1.7845e+04 logits_max_abs=2.4260e+05 logits_mean=-1.7007e+02 nan=False inf=False\n",
      "[BATCH 1050/2969] loss_sum=6.0066e+05 batch_avg_loss=9.3853e+03 logits_max_abs=1.2195e+05 logits_mean=-8.9577e+01 nan=False inf=False\n",
      "[BATCH 1100/2969] loss_sum=5.3528e+05 batch_avg_loss=8.3638e+03 logits_max_abs=1.0584e+05 logits_mean=-6.5929e+01 nan=False inf=False\n",
      "[BATCH 1150/2969] loss_sum=4.8218e+05 batch_avg_loss=7.5340e+03 logits_max_abs=9.1955e+04 logits_mean=-6.5097e+01 nan=False inf=False\n",
      "[BATCH 1200/2969] loss_sum=2.5391e+05 batch_avg_loss=3.9673e+03 logits_max_abs=6.9953e+04 logits_mean=-3.4979e+01 nan=False inf=False\n",
      "[BATCH 1250/2969] loss_sum=3.0937e+05 batch_avg_loss=4.8339e+03 logits_max_abs=7.4038e+04 logits_mean=-4.4837e+01 nan=False inf=False\n",
      "[BATCH 1300/2969] loss_sum=2.9835e+05 batch_avg_loss=4.6616e+03 logits_max_abs=7.6596e+04 logits_mean=-4.6501e+01 nan=False inf=False\n",
      "[BATCH 1350/2969] loss_sum=3.0852e+05 batch_avg_loss=4.8206e+03 logits_max_abs=6.1466e+04 logits_mean=-5.5826e+01 nan=False inf=False\n",
      "[BATCH 1400/2969] loss_sum=6.4949e+05 batch_avg_loss=1.0148e+04 logits_max_abs=1.6238e+05 logits_mean=-9.2649e+01 nan=False inf=False\n",
      "[BATCH 1450/2969] loss_sum=4.8843e+05 batch_avg_loss=7.6318e+03 logits_max_abs=9.1889e+04 logits_mean=-7.2497e+01 nan=False inf=False\n",
      "[BATCH 1500/2969] loss_sum=2.8089e+05 batch_avg_loss=4.3889e+03 logits_max_abs=5.6806e+04 logits_mean=-4.1219e+01 nan=False inf=False\n",
      "[BATCH 1550/2969] loss_sum=2.3680e+05 batch_avg_loss=3.7000e+03 logits_max_abs=9.4126e+04 logits_mean=-3.9751e+01 nan=False inf=False\n",
      "[BATCH 1600/2969] loss_sum=6.5406e+05 batch_avg_loss=1.0220e+04 logits_max_abs=1.5031e+05 logits_mean=-9.5293e+01 nan=False inf=False\n",
      "[BATCH 1650/2969] loss_sum=5.3025e+05 batch_avg_loss=8.2852e+03 logits_max_abs=9.1216e+04 logits_mean=-9.0772e+01 nan=False inf=False\n",
      "[BATCH 1700/2969] loss_sum=1.0301e+06 batch_avg_loss=1.6096e+04 logits_max_abs=1.0627e+05 logits_mean=-1.3633e+02 nan=False inf=False\n",
      "[BATCH 1750/2969] loss_sum=6.8437e+05 batch_avg_loss=1.0693e+04 logits_max_abs=1.6792e+05 logits_mean=-9.8977e+01 nan=False inf=False\n",
      "[BATCH 1800/2969] loss_sum=5.7427e+05 batch_avg_loss=8.9730e+03 logits_max_abs=7.3258e+04 logits_mean=-8.5249e+01 nan=False inf=False\n",
      "[BATCH 1850/2969] loss_sum=8.2996e+05 batch_avg_loss=1.2968e+04 logits_max_abs=1.3469e+05 logits_mean=-1.2056e+02 nan=False inf=False\n",
      "[BATCH 1900/2969] loss_sum=9.0141e+05 batch_avg_loss=1.4085e+04 logits_max_abs=1.3710e+05 logits_mean=-1.5445e+02 nan=False inf=False\n",
      "[BATCH 1950/2969] loss_sum=5.3782e+05 batch_avg_loss=8.4035e+03 logits_max_abs=7.6226e+04 logits_mean=-7.0752e+01 nan=False inf=False\n",
      "[BATCH 2000/2969] loss_sum=4.8274e+05 batch_avg_loss=7.5429e+03 logits_max_abs=7.7832e+04 logits_mean=-6.8273e+01 nan=False inf=False\n",
      "[BATCH 2050/2969] loss_sum=2.0785e+06 batch_avg_loss=3.2476e+04 logits_max_abs=3.0857e+05 logits_mean=-3.0611e+02 nan=False inf=False\n",
      "[BATCH 2100/2969] loss_sum=1.9408e+06 batch_avg_loss=3.0325e+04 logits_max_abs=2.7422e+05 logits_mean=-3.4432e+02 nan=False inf=False\n",
      "[BATCH 2150/2969] loss_sum=1.9564e+06 batch_avg_loss=3.0569e+04 logits_max_abs=2.3905e+05 logits_mean=-2.9338e+02 nan=False inf=False\n",
      "[BATCH 2200/2969] loss_sum=5.1068e+05 batch_avg_loss=7.9793e+03 logits_max_abs=1.4471e+05 logits_mean=-7.5926e+01 nan=False inf=False\n",
      "[BATCH 2250/2969] loss_sum=6.3656e+05 batch_avg_loss=9.9463e+03 logits_max_abs=1.7465e+05 logits_mean=-9.7930e+01 nan=False inf=False\n",
      "[BATCH 2300/2969] loss_sum=2.8050e+05 batch_avg_loss=4.3829e+03 logits_max_abs=9.0579e+04 logits_mean=-4.1266e+01 nan=False inf=False\n",
      "[BATCH 2350/2969] loss_sum=3.1278e+05 batch_avg_loss=4.8872e+03 logits_max_abs=8.1754e+04 logits_mean=-3.8785e+01 nan=False inf=False\n",
      "[BATCH 2400/2969] loss_sum=2.1058e+05 batch_avg_loss=3.2903e+03 logits_max_abs=7.1015e+04 logits_mean=-2.9290e+01 nan=False inf=False\n",
      "[BATCH 2450/2969] loss_sum=1.4896e+05 batch_avg_loss=2.3275e+03 logits_max_abs=5.9177e+04 logits_mean=-1.8530e+01 nan=False inf=False\n",
      "[BATCH 2500/2969] loss_sum=1.3594e+05 batch_avg_loss=2.1241e+03 logits_max_abs=5.3406e+04 logits_mean=-2.1396e+01 nan=False inf=False\n",
      "[BATCH 2550/2969] loss_sum=2.7591e+05 batch_avg_loss=4.3111e+03 logits_max_abs=6.5366e+04 logits_mean=-4.3658e+01 nan=False inf=False\n",
      "[BATCH 2600/2969] loss_sum=2.9797e+05 batch_avg_loss=4.6559e+03 logits_max_abs=5.7232e+04 logits_mean=-5.4506e+01 nan=False inf=False\n",
      "[BATCH 2650/2969] loss_sum=6.1187e+05 batch_avg_loss=9.5604e+03 logits_max_abs=1.4401e+05 logits_mean=-8.9269e+01 nan=False inf=False\n",
      "[BATCH 2700/2969] loss_sum=7.6646e+05 batch_avg_loss=1.1976e+04 logits_max_abs=1.2214e+05 logits_mean=-1.1274e+02 nan=False inf=False\n",
      "[BATCH 2750/2969] loss_sum=4.7121e+05 batch_avg_loss=7.3627e+03 logits_max_abs=9.9746e+04 logits_mean=-6.8985e+01 nan=False inf=False\n",
      "[BATCH 2800/2969] loss_sum=3.7437e+05 batch_avg_loss=5.8496e+03 logits_max_abs=1.4497e+05 logits_mean=-6.3700e+01 nan=False inf=False\n",
      "[BATCH 2850/2969] loss_sum=6.6282e+05 batch_avg_loss=1.0357e+04 logits_max_abs=1.3777e+05 logits_mean=-9.5169e+01 nan=False inf=False\n",
      "[BATCH 2900/2969] loss_sum=5.2386e+05 batch_avg_loss=8.1852e+03 logits_max_abs=7.5573e+04 logits_mean=-8.6901e+01 nan=False inf=False\n",
      "[BATCH 2950/2969] loss_sum=1.0772e+06 batch_avg_loss=1.6831e+04 logits_max_abs=1.1016e+05 logits_mean=-1.4194e+02 nan=False inf=False\n",
      "=== EVAL DIAGNOSTICS ===\n",
      "total_samples=190000 total_loss_sum=1.9659e+09 test_loss_avg=1.0347e+04 test_acc=0.1130\n",
      "max_abs_logit=3.7728e+05 saw_nan=False saw_inf=False\n",
      "\n",
      "Consistency check: test_acc=0.112958 vs accuracy_before_rejection=0.112958\n",
      "Saved results JSON: ./outputs\\cifar10-c_efficientnetv2_s_e50_bs64_results.json\n",
      "\n",
      "================================================================================\n",
      "Experiment 5/10: cifar100-c + shufflenetv2_0_5\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded CIFAR-100-C: 19 corruptions, severity 5, 190000 total images\n",
      "Training configuration:\n",
      "{\n",
      "  \"dataset\": \"cifar100-c\",\n",
      "  \"model_name\": \"shufflenetv2_0_5\",\n",
      "  \"data_root\": \"./data\",\n",
      "  \"tinyimagenet_root\": \"./tiny-imagenet-200\",\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 0,\n",
      "  \"lr\": 0.05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"seed\": 42,\n",
      "  \"reject_percentiles\": [\n",
      "    10,\n",
      "    20,\n",
      "    30,\n",
      "    40,\n",
      "    50\n",
      "  ],\n",
      "  \"hybrid_weight_entropy\": 0.5,\n",
      "  \"hybrid_weight_grad\": 0.5,\n",
      "  \"out_dir\": \"./outputs\"\n",
      "}\n",
      "\n",
      "✓ Loading pre-trained model from ./outputs\\cifar100_shufflenetv2_0_5_e50_bs64.pth\n",
      "Saved checkpoint: ./outputs\\cifar100-c_shufflenetv2_0_5_e50_bs64.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735feef431024695b388470c611f433e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test+Uncertainty:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 50/2969] loss_sum=9.3468e+02 batch_avg_loss=1.4604e+01 logits_max_abs=4.1375e+01 logits_mean=6.9806e-03 nan=False inf=False\n",
      "[BATCH 100/2969] loss_sum=9.8477e+02 batch_avg_loss=1.5387e+01 logits_max_abs=5.9310e+01 logits_mean=8.4915e-03 nan=False inf=False\n",
      "[BATCH 150/2969] loss_sum=8.8699e+02 batch_avg_loss=1.3859e+01 logits_max_abs=6.7738e+01 logits_mean=7.1796e-03 nan=False inf=False\n",
      "[BATCH 200/2969] loss_sum=7.8643e+02 batch_avg_loss=1.2288e+01 logits_max_abs=2.8275e+01 logits_mean=-2.2349e-03 nan=False inf=False\n",
      "[BATCH 250/2969] loss_sum=7.7311e+02 batch_avg_loss=1.2080e+01 logits_max_abs=2.6719e+01 logits_mean=-2.5569e-03 nan=False inf=False\n",
      "[BATCH 300/2969] loss_sum=8.7779e+02 batch_avg_loss=1.3715e+01 logits_max_abs=4.2320e+01 logits_mean=-9.7475e-04 nan=False inf=False\n",
      "[BATCH 350/2969] loss_sum=8.7321e+02 batch_avg_loss=1.3644e+01 logits_max_abs=2.5628e+01 logits_mean=1.1592e-03 nan=False inf=False\n",
      "[BATCH 400/2969] loss_sum=9.2151e+02 batch_avg_loss=1.4399e+01 logits_max_abs=6.6665e+01 logits_mean=2.2343e-03 nan=False inf=False\n",
      "[BATCH 450/2969] loss_sum=8.8057e+02 batch_avg_loss=1.3759e+01 logits_max_abs=5.0154e+01 logits_mean=1.5015e-03 nan=False inf=False\n",
      "[BATCH 500/2969] loss_sum=8.6293e+02 batch_avg_loss=1.3483e+01 logits_max_abs=3.6084e+01 logits_mean=2.2402e-03 nan=False inf=False\n",
      "[BATCH 550/2969] loss_sum=8.6261e+02 batch_avg_loss=1.3478e+01 logits_max_abs=2.8578e+01 logits_mean=8.4815e-04 nan=False inf=False\n",
      "[BATCH 600/2969] loss_sum=8.7726e+02 batch_avg_loss=1.3707e+01 logits_max_abs=6.2307e+01 logits_mean=1.9763e-03 nan=False inf=False\n",
      "[BATCH 650/2969] loss_sum=7.3623e+02 batch_avg_loss=1.1504e+01 logits_max_abs=1.9677e+01 logits_mean=-2.7315e-03 nan=False inf=False\n",
      "[BATCH 700/2969] loss_sum=7.7674e+02 batch_avg_loss=1.2137e+01 logits_max_abs=1.9930e+01 logits_mean=-1.8152e-03 nan=False inf=False\n",
      "[BATCH 750/2969] loss_sum=7.8931e+02 batch_avg_loss=1.2333e+01 logits_max_abs=2.2194e+01 logits_mean=-1.8566e-03 nan=False inf=False\n",
      "[BATCH 800/2969] loss_sum=7.6820e+02 batch_avg_loss=1.2003e+01 logits_max_abs=3.3519e+01 logits_mean=3.9551e-03 nan=False inf=False\n",
      "[BATCH 850/2969] loss_sum=7.6466e+02 batch_avg_loss=1.1948e+01 logits_max_abs=2.2674e+01 logits_mean=3.8259e-03 nan=False inf=False\n",
      "[BATCH 900/2969] loss_sum=8.2960e+02 batch_avg_loss=1.2962e+01 logits_max_abs=2.7377e+01 logits_mean=3.8214e-03 nan=False inf=False\n",
      "[BATCH 950/2969] loss_sum=8.4174e+02 batch_avg_loss=1.3152e+01 logits_max_abs=5.0830e+01 logits_mean=1.2387e-03 nan=False inf=False\n",
      "[BATCH 1000/2969] loss_sum=8.3525e+02 batch_avg_loss=1.3051e+01 logits_max_abs=4.1579e+01 logits_mean=1.1617e-03 nan=False inf=False\n",
      "[BATCH 1050/2969] loss_sum=7.6855e+02 batch_avg_loss=1.2009e+01 logits_max_abs=2.9962e+01 logits_mean=9.1153e-04 nan=False inf=False\n",
      "[BATCH 1100/2969] loss_sum=9.2746e+02 batch_avg_loss=1.4492e+01 logits_max_abs=3.1149e+01 logits_mean=1.0590e-02 nan=False inf=False\n",
      "[BATCH 1150/2969] loss_sum=9.3159e+02 batch_avg_loss=1.4556e+01 logits_max_abs=5.6633e+01 logits_mean=1.0831e-02 nan=False inf=False\n",
      "[BATCH 1200/2969] loss_sum=9.5946e+02 batch_avg_loss=1.4991e+01 logits_max_abs=4.1401e+01 logits_mean=9.7660e-03 nan=False inf=False\n",
      "[BATCH 1250/2969] loss_sum=1.1073e+03 batch_avg_loss=1.7301e+01 logits_max_abs=4.6857e+01 logits_mean=9.8168e-03 nan=False inf=False\n",
      "[BATCH 1300/2969] loss_sum=8.7788e+02 batch_avg_loss=1.3717e+01 logits_max_abs=3.6207e+01 logits_mean=1.8857e-03 nan=False inf=False\n",
      "[BATCH 1350/2969] loss_sum=9.1499e+02 batch_avg_loss=1.4297e+01 logits_max_abs=5.4677e+01 logits_mean=2.7851e-03 nan=False inf=False\n",
      "[BATCH 1400/2969] loss_sum=8.1001e+02 batch_avg_loss=1.2656e+01 logits_max_abs=4.2784e+01 logits_mean=1.9622e-03 nan=False inf=False\n",
      "[BATCH 1450/2969] loss_sum=1.0032e+03 batch_avg_loss=1.5675e+01 logits_max_abs=3.9146e+01 logits_mean=1.3736e-02 nan=False inf=False\n",
      "[BATCH 1500/2969] loss_sum=9.6680e+02 batch_avg_loss=1.5106e+01 logits_max_abs=3.6486e+01 logits_mean=1.4006e-02 nan=False inf=False\n",
      "[BATCH 1550/2969] loss_sum=1.0893e+03 batch_avg_loss=1.7020e+01 logits_max_abs=5.5086e+01 logits_mean=1.3598e-02 nan=False inf=False\n",
      "[BATCH 1600/2969] loss_sum=8.6563e+02 batch_avg_loss=1.3526e+01 logits_max_abs=2.5252e+01 logits_mean=2.5842e-03 nan=False inf=False\n",
      "[BATCH 1650/2969] loss_sum=9.1854e+02 batch_avg_loss=1.4352e+01 logits_max_abs=6.5840e+01 logits_mean=3.2509e-03 nan=False inf=False\n",
      "[BATCH 1700/2969] loss_sum=8.7798e+02 batch_avg_loss=1.3718e+01 logits_max_abs=5.0156e+01 logits_mean=2.4396e-03 nan=False inf=False\n",
      "[BATCH 1750/2969] loss_sum=8.4918e+02 batch_avg_loss=1.3268e+01 logits_max_abs=3.3449e+01 logits_mean=9.7764e-04 nan=False inf=False\n",
      "[BATCH 1800/2969] loss_sum=8.5502e+02 batch_avg_loss=1.3360e+01 logits_max_abs=2.8049e+01 logits_mean=2.8962e-04 nan=False inf=False\n",
      "[BATCH 1850/2969] loss_sum=8.7570e+02 batch_avg_loss=1.3683e+01 logits_max_abs=6.2649e+01 logits_mean=7.4864e-04 nan=False inf=False\n",
      "[BATCH 1900/2969] loss_sum=8.6799e+02 batch_avg_loss=1.3562e+01 logits_max_abs=4.6945e+01 logits_mean=1.9305e-03 nan=False inf=False\n",
      "[BATCH 1950/2969] loss_sum=8.6134e+02 batch_avg_loss=1.3458e+01 logits_max_abs=3.4854e+01 logits_mean=2.3185e-03 nan=False inf=False\n",
      "[BATCH 2000/2969] loss_sum=9.2335e+02 batch_avg_loss=1.4427e+01 logits_max_abs=5.2018e+01 logits_mean=3.0809e-03 nan=False inf=False\n",
      "[BATCH 2050/2969] loss_sum=1.2199e+03 batch_avg_loss=1.9061e+01 logits_max_abs=8.5738e+01 logits_mean=6.6559e-03 nan=False inf=False\n",
      "[BATCH 2100/2969] loss_sum=1.0547e+03 batch_avg_loss=1.6479e+01 logits_max_abs=5.2608e+01 logits_mean=6.3490e-03 nan=False inf=False\n",
      "[BATCH 2150/2969] loss_sum=1.3295e+03 batch_avg_loss=2.0773e+01 logits_max_abs=7.8055e+01 logits_mean=6.4423e-03 nan=False inf=False\n",
      "[BATCH 2200/2969] loss_sum=9.0863e+02 batch_avg_loss=1.4197e+01 logits_max_abs=5.8464e+01 logits_mean=9.5523e-03 nan=False inf=False\n",
      "[BATCH 2250/2969] loss_sum=8.6562e+02 batch_avg_loss=1.3525e+01 logits_max_abs=4.9160e+01 logits_mean=9.1938e-03 nan=False inf=False\n",
      "[BATCH 2300/2969] loss_sum=8.4384e+02 batch_avg_loss=1.3185e+01 logits_max_abs=3.7095e+01 logits_mean=9.8179e-03 nan=False inf=False\n",
      "[BATCH 2350/2969] loss_sum=8.0826e+02 batch_avg_loss=1.2629e+01 logits_max_abs=2.3362e+01 logits_mean=5.9653e-03 nan=False inf=False\n",
      "[BATCH 2400/2969] loss_sum=7.9699e+02 batch_avg_loss=1.2453e+01 logits_max_abs=3.3899e+01 logits_mean=6.2376e-03 nan=False inf=False\n",
      "[BATCH 2450/2969] loss_sum=8.1559e+02 batch_avg_loss=1.2744e+01 logits_max_abs=2.4065e+01 logits_mean=5.2068e-03 nan=False inf=False\n",
      "[BATCH 2500/2969] loss_sum=8.9692e+02 batch_avg_loss=1.4014e+01 logits_max_abs=2.6880e+01 logits_mean=5.5057e-03 nan=False inf=False\n",
      "[BATCH 2550/2969] loss_sum=8.9868e+02 batch_avg_loss=1.4042e+01 logits_max_abs=3.7290e+01 logits_mean=4.1571e-03 nan=False inf=False\n",
      "[BATCH 2600/2969] loss_sum=9.4781e+02 batch_avg_loss=1.4810e+01 logits_max_abs=5.4098e+01 logits_mean=5.2780e-03 nan=False inf=False\n",
      "[BATCH 2650/2969] loss_sum=8.3278e+02 batch_avg_loss=1.3012e+01 logits_max_abs=4.0922e+01 logits_mean=3.7695e-03 nan=False inf=False\n",
      "[BATCH 2700/2969] loss_sum=9.0564e+02 batch_avg_loss=1.4151e+01 logits_max_abs=4.0248e+01 logits_mean=8.8039e-03 nan=False inf=False\n",
      "[BATCH 2750/2969] loss_sum=8.9604e+02 batch_avg_loss=1.4001e+01 logits_max_abs=3.2888e+01 logits_mean=8.9130e-03 nan=False inf=False\n",
      "[BATCH 2800/2969] loss_sum=9.9768e+02 batch_avg_loss=1.5589e+01 logits_max_abs=5.1587e+01 logits_mean=9.4548e-03 nan=False inf=False\n",
      "[BATCH 2850/2969] loss_sum=8.7681e+02 batch_avg_loss=1.3700e+01 logits_max_abs=2.6076e+01 logits_mean=7.4168e-04 nan=False inf=False\n",
      "[BATCH 2900/2969] loss_sum=9.1738e+02 batch_avg_loss=1.4334e+01 logits_max_abs=6.2102e+01 logits_mean=1.8569e-03 nan=False inf=False\n",
      "[BATCH 2950/2969] loss_sum=8.8640e+02 batch_avg_loss=1.3850e+01 logits_max_abs=5.2811e+01 logits_mean=1.3693e-03 nan=False inf=False\n",
      "=== EVAL DIAGNOSTICS ===\n",
      "total_samples=190000 total_loss_sum=2.6576e+06 test_loss_avg=1.3987e+01 test_acc=0.0103\n",
      "max_abs_logit=1.2869e+02 saw_nan=False saw_inf=False\n",
      "\n",
      "Consistency check: test_acc=0.010332 vs accuracy_before_rejection=0.010332\n",
      "Saved results JSON: ./outputs\\cifar100-c_shufflenetv2_0_5_e50_bs64_results.json\n",
      "\n",
      "================================================================================\n",
      "Experiment 6/10: cifar100-c + mobilenetv3_small\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded CIFAR-100-C: 19 corruptions, severity 5, 190000 total images\n",
      "Training configuration:\n",
      "{\n",
      "  \"dataset\": \"cifar100-c\",\n",
      "  \"model_name\": \"mobilenetv3_small\",\n",
      "  \"data_root\": \"./data\",\n",
      "  \"tinyimagenet_root\": \"./tiny-imagenet-200\",\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 0,\n",
      "  \"lr\": 0.05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"seed\": 42,\n",
      "  \"reject_percentiles\": [\n",
      "    10,\n",
      "    20,\n",
      "    30,\n",
      "    40,\n",
      "    50\n",
      "  ],\n",
      "  \"hybrid_weight_entropy\": 0.5,\n",
      "  \"hybrid_weight_grad\": 0.5,\n",
      "  \"out_dir\": \"./outputs\"\n",
      "}\n",
      "\n",
      "✓ Loading pre-trained model from ./outputs\\cifar100_mobilenetv3_small_e50_bs64.pth\n",
      "Saved checkpoint: ./outputs\\cifar100-c_mobilenetv3_small_e50_bs64.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44d64822a0d4ca4b336c4eceb0a33e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test+Uncertainty:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 50/2969] loss_sum=5.6830e+02 batch_avg_loss=8.8797e+00 logits_max_abs=1.0569e+01 logits_mean=8.7137e-03 nan=False inf=False\n",
      "[BATCH 100/2969] loss_sum=5.9107e+02 batch_avg_loss=9.2355e+00 logits_max_abs=1.0596e+01 logits_mean=8.4416e-03 nan=False inf=False\n",
      "[BATCH 150/2969] loss_sum=5.5030e+02 batch_avg_loss=8.5984e+00 logits_max_abs=1.0550e+01 logits_mean=9.0808e-03 nan=False inf=False\n",
      "[BATCH 200/2969] loss_sum=7.0517e+02 batch_avg_loss=1.1018e+01 logits_max_abs=1.5127e+01 logits_mean=4.0685e-03 nan=False inf=False\n",
      "[BATCH 250/2969] loss_sum=6.5417e+02 batch_avg_loss=1.0221e+01 logits_max_abs=1.4442e+01 logits_mean=2.3311e-03 nan=False inf=False\n",
      "[BATCH 300/2969] loss_sum=6.8757e+02 batch_avg_loss=1.0743e+01 logits_max_abs=1.4123e+01 logits_mean=3.4816e-03 nan=False inf=False\n",
      "[BATCH 350/2969] loss_sum=6.4316e+02 batch_avg_loss=1.0049e+01 logits_max_abs=1.1954e+01 logits_mean=5.5866e-03 nan=False inf=False\n",
      "[BATCH 400/2969] loss_sum=6.2022e+02 batch_avg_loss=9.6909e+00 logits_max_abs=1.2485e+01 logits_mean=5.1473e-03 nan=False inf=False\n",
      "[BATCH 450/2969] loss_sum=5.9393e+02 batch_avg_loss=9.2802e+00 logits_max_abs=1.1582e+01 logits_mean=4.7881e-03 nan=False inf=False\n",
      "[BATCH 500/2969] loss_sum=5.9710e+02 batch_avg_loss=9.3298e+00 logits_max_abs=1.2109e+01 logits_mean=5.8237e-03 nan=False inf=False\n",
      "[BATCH 550/2969] loss_sum=6.4561e+02 batch_avg_loss=1.0088e+01 logits_max_abs=1.1489e+01 logits_mean=5.7088e-03 nan=False inf=False\n",
      "[BATCH 600/2969] loss_sum=5.9184e+02 batch_avg_loss=9.2474e+00 logits_max_abs=1.1865e+01 logits_mean=5.9630e-03 nan=False inf=False\n",
      "[BATCH 650/2969] loss_sum=6.0896e+02 batch_avg_loss=9.5149e+00 logits_max_abs=1.2189e+01 logits_mean=2.7401e-03 nan=False inf=False\n",
      "[BATCH 700/2969] loss_sum=5.9834e+02 batch_avg_loss=9.3491e+00 logits_max_abs=1.1614e+01 logits_mean=2.6101e-03 nan=False inf=False\n",
      "[BATCH 750/2969] loss_sum=6.6636e+02 batch_avg_loss=1.0412e+01 logits_max_abs=1.1607e+01 logits_mean=2.7055e-03 nan=False inf=False\n",
      "[BATCH 800/2969] loss_sum=6.2667e+02 batch_avg_loss=9.7917e+00 logits_max_abs=1.0318e+01 logits_mean=4.8824e-03 nan=False inf=False\n",
      "[BATCH 850/2969] loss_sum=5.4303e+02 batch_avg_loss=8.4848e+00 logits_max_abs=9.9195e+00 logits_mean=5.0773e-03 nan=False inf=False\n",
      "[BATCH 900/2969] loss_sum=6.3014e+02 batch_avg_loss=9.8460e+00 logits_max_abs=1.0643e+01 logits_mean=6.1039e-03 nan=False inf=False\n",
      "[BATCH 950/2969] loss_sum=6.1193e+02 batch_avg_loss=9.5615e+00 logits_max_abs=1.1900e+01 logits_mean=5.8334e-03 nan=False inf=False\n",
      "[BATCH 1000/2969] loss_sum=6.0659e+02 batch_avg_loss=9.4779e+00 logits_max_abs=1.3878e+01 logits_mean=5.8480e-03 nan=False inf=False\n",
      "[BATCH 1050/2969] loss_sum=6.0835e+02 batch_avg_loss=9.5054e+00 logits_max_abs=1.2183e+01 logits_mean=5.5913e-03 nan=False inf=False\n",
      "[BATCH 1100/2969] loss_sum=5.4229e+02 batch_avg_loss=8.4732e+00 logits_max_abs=9.8858e+00 logits_mean=9.2434e-03 nan=False inf=False\n",
      "[BATCH 1150/2969] loss_sum=5.6661e+02 batch_avg_loss=8.8533e+00 logits_max_abs=1.0162e+01 logits_mean=9.3133e-03 nan=False inf=False\n",
      "[BATCH 1200/2969] loss_sum=5.4402e+02 batch_avg_loss=8.5003e+00 logits_max_abs=9.2053e+00 logits_mean=8.7518e-03 nan=False inf=False\n",
      "[BATCH 1250/2969] loss_sum=5.3419e+02 batch_avg_loss=8.3467e+00 logits_max_abs=9.8358e+00 logits_mean=7.9475e-03 nan=False inf=False\n",
      "[BATCH 1300/2969] loss_sum=5.9478e+02 batch_avg_loss=9.2934e+00 logits_max_abs=1.2948e+01 logits_mean=6.1458e-03 nan=False inf=False\n",
      "[BATCH 1350/2969] loss_sum=6.0175e+02 batch_avg_loss=9.4023e+00 logits_max_abs=1.2667e+01 logits_mean=6.2709e-03 nan=False inf=False\n",
      "[BATCH 1400/2969] loss_sum=5.5138e+02 batch_avg_loss=8.6153e+00 logits_max_abs=1.1833e+01 logits_mean=6.0364e-03 nan=False inf=False\n",
      "[BATCH 1450/2969] loss_sum=5.5377e+02 batch_avg_loss=8.6526e+00 logits_max_abs=1.0123e+01 logits_mean=6.8199e-03 nan=False inf=False\n",
      "[BATCH 1500/2969] loss_sum=5.6374e+02 batch_avg_loss=8.8085e+00 logits_max_abs=1.0774e+01 logits_mean=6.9057e-03 nan=False inf=False\n",
      "[BATCH 1550/2969] loss_sum=5.4862e+02 batch_avg_loss=8.5723e+00 logits_max_abs=1.0577e+01 logits_mean=6.7507e-03 nan=False inf=False\n",
      "[BATCH 1600/2969] loss_sum=6.2028e+02 batch_avg_loss=9.6919e+00 logits_max_abs=1.0279e+01 logits_mean=6.1910e-03 nan=False inf=False\n",
      "[BATCH 1650/2969] loss_sum=5.9971e+02 batch_avg_loss=9.3705e+00 logits_max_abs=1.1858e+01 logits_mean=5.7946e-03 nan=False inf=False\n",
      "[BATCH 1700/2969] loss_sum=5.8033e+02 batch_avg_loss=9.0676e+00 logits_max_abs=1.0649e+01 logits_mean=5.8797e-03 nan=False inf=False\n",
      "[BATCH 1750/2969] loss_sum=6.0028e+02 batch_avg_loss=9.3794e+00 logits_max_abs=1.2468e+01 logits_mean=5.3907e-03 nan=False inf=False\n",
      "[BATCH 1800/2969] loss_sum=6.6504e+02 batch_avg_loss=1.0391e+01 logits_max_abs=1.1936e+01 logits_mean=5.4952e-03 nan=False inf=False\n",
      "[BATCH 1850/2969] loss_sum=5.9537e+02 batch_avg_loss=9.3026e+00 logits_max_abs=1.2298e+01 logits_mean=5.2927e-03 nan=False inf=False\n",
      "[BATCH 1900/2969] loss_sum=5.4432e+02 batch_avg_loss=8.5051e+00 logits_max_abs=1.0250e+01 logits_mean=6.3945e-03 nan=False inf=False\n",
      "[BATCH 1950/2969] loss_sum=5.4815e+02 batch_avg_loss=8.5648e+00 logits_max_abs=1.1142e+01 logits_mean=6.0541e-03 nan=False inf=False\n",
      "[BATCH 2000/2969] loss_sum=6.2168e+02 batch_avg_loss=9.7138e+00 logits_max_abs=1.0790e+01 logits_mean=6.1620e-03 nan=False inf=False\n",
      "[BATCH 2050/2969] loss_sum=5.8102e+02 batch_avg_loss=9.0784e+00 logits_max_abs=9.9118e+00 logits_mean=7.6633e-03 nan=False inf=False\n",
      "[BATCH 2100/2969] loss_sum=5.0222e+02 batch_avg_loss=7.8472e+00 logits_max_abs=1.0002e+01 logits_mean=8.2938e-03 nan=False inf=False\n",
      "[BATCH 2150/2969] loss_sum=5.9528e+02 batch_avg_loss=9.3012e+00 logits_max_abs=1.1499e+01 logits_mean=7.9627e-03 nan=False inf=False\n",
      "[BATCH 2200/2969] loss_sum=5.5098e+02 batch_avg_loss=8.6091e+00 logits_max_abs=1.0815e+01 logits_mean=7.9001e-03 nan=False inf=False\n",
      "[BATCH 2250/2969] loss_sum=5.4864e+02 batch_avg_loss=8.5725e+00 logits_max_abs=9.8633e+00 logits_mean=8.4035e-03 nan=False inf=False\n",
      "[BATCH 2300/2969] loss_sum=5.4352e+02 batch_avg_loss=8.4925e+00 logits_max_abs=9.9053e+00 logits_mean=9.0084e-03 nan=False inf=False\n",
      "[BATCH 2350/2969] loss_sum=5.9496e+02 batch_avg_loss=9.2963e+00 logits_max_abs=1.0147e+01 logits_mean=7.6003e-03 nan=False inf=False\n",
      "[BATCH 2400/2969] loss_sum=5.9326e+02 batch_avg_loss=9.2697e+00 logits_max_abs=1.0344e+01 logits_mean=8.2133e-03 nan=False inf=False\n",
      "[BATCH 2450/2969] loss_sum=5.5683e+02 batch_avg_loss=8.7005e+00 logits_max_abs=9.9384e+00 logits_mean=7.9475e-03 nan=False inf=False\n",
      "[BATCH 2500/2969] loss_sum=6.0810e+02 batch_avg_loss=9.5015e+00 logits_max_abs=1.0626e+01 logits_mean=7.1219e-03 nan=False inf=False\n",
      "[BATCH 2550/2969] loss_sum=6.0574e+02 batch_avg_loss=9.4647e+00 logits_max_abs=1.2329e+01 logits_mean=7.7004e-03 nan=False inf=False\n",
      "[BATCH 2600/2969] loss_sum=6.0708e+02 batch_avg_loss=9.4856e+00 logits_max_abs=1.1539e+01 logits_mean=7.7333e-03 nan=False inf=False\n",
      "[BATCH 2650/2969] loss_sum=5.4552e+02 batch_avg_loss=8.5237e+00 logits_max_abs=1.1667e+01 logits_mean=8.2319e-03 nan=False inf=False\n",
      "[BATCH 2700/2969] loss_sum=5.6312e+02 batch_avg_loss=8.7988e+00 logits_max_abs=1.0654e+01 logits_mean=8.6261e-03 nan=False inf=False\n",
      "[BATCH 2750/2969] loss_sum=5.6198e+02 batch_avg_loss=8.7809e+00 logits_max_abs=9.8301e+00 logits_mean=8.6402e-03 nan=False inf=False\n",
      "[BATCH 2800/2969] loss_sum=5.5973e+02 batch_avg_loss=8.7458e+00 logits_max_abs=1.0080e+01 logits_mean=8.5121e-03 nan=False inf=False\n",
      "[BATCH 2850/2969] loss_sum=6.4367e+02 batch_avg_loss=1.0057e+01 logits_max_abs=1.1157e+01 logits_mean=5.6538e-03 nan=False inf=False\n",
      "[BATCH 2900/2969] loss_sum=6.1750e+02 batch_avg_loss=9.6485e+00 logits_max_abs=1.1845e+01 logits_mean=4.8829e-03 nan=False inf=False\n",
      "[BATCH 2950/2969] loss_sum=5.9297e+02 batch_avg_loss=9.2652e+00 logits_max_abs=1.0869e+01 logits_mean=4.8338e-03 nan=False inf=False\n",
      "=== EVAL DIAGNOSTICS ===\n",
      "total_samples=190000 total_loss_sum=1.7613e+06 test_loss_avg=9.2701e+00 test_acc=0.0123\n",
      "max_abs_logit=1.6333e+01 saw_nan=False saw_inf=False\n",
      "\n",
      "Consistency check: test_acc=0.012274 vs accuracy_before_rejection=0.012274\n",
      "Saved results JSON: ./outputs\\cifar100-c_mobilenetv3_small_e50_bs64_results.json\n",
      "\n",
      "================================================================================\n",
      "Experiment 7/10: cifar100-c + efficientnetv2_s\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded CIFAR-100-C: 19 corruptions, severity 5, 190000 total images\n",
      "Training configuration:\n",
      "{\n",
      "  \"dataset\": \"cifar100-c\",\n",
      "  \"model_name\": \"efficientnetv2_s\",\n",
      "  \"data_root\": \"./data\",\n",
      "  \"tinyimagenet_root\": \"./tiny-imagenet-200\",\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 0,\n",
      "  \"lr\": 0.05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"seed\": 42,\n",
      "  \"reject_percentiles\": [\n",
      "    10,\n",
      "    20,\n",
      "    30,\n",
      "    40,\n",
      "    50\n",
      "  ],\n",
      "  \"hybrid_weight_entropy\": 0.5,\n",
      "  \"hybrid_weight_grad\": 0.5,\n",
      "  \"out_dir\": \"./outputs\"\n",
      "}\n",
      "\n",
      "✓ Loading pre-trained model from ./outputs\\cifar100_efficientnetv2_s_e50_bs64.pth\n",
      "Saved checkpoint: ./outputs\\cifar100-c_efficientnetv2_s_e50_bs64.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0eed751a0641e5af13f85b2bdf095c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test+Uncertainty:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 50/2969] loss_sum=3.1085e+06 batch_avg_loss=4.8570e+04 logits_max_abs=7.9318e+05 logits_mean=-5.9052e+01 nan=False inf=False\n",
      "[BATCH 100/2969] loss_sum=3.0227e+06 batch_avg_loss=4.7229e+04 logits_max_abs=1.0868e+06 logits_mean=-5.6801e+01 nan=False inf=False\n",
      "[BATCH 150/2969] loss_sum=1.9194e+06 batch_avg_loss=2.9990e+04 logits_max_abs=7.6977e+05 logits_mean=-3.5234e+01 nan=False inf=False\n",
      "[BATCH 200/2969] loss_sum=1.9906e+06 batch_avg_loss=3.1103e+04 logits_max_abs=1.0546e+06 logits_mean=-3.5388e+01 nan=False inf=False\n",
      "[BATCH 250/2969] loss_sum=7.5457e+05 batch_avg_loss=1.1790e+04 logits_max_abs=4.2756e+05 logits_mean=-1.0982e+01 nan=False inf=False\n",
      "[BATCH 300/2969] loss_sum=5.6538e+05 batch_avg_loss=8.8341e+03 logits_max_abs=4.7545e+05 logits_mean=-1.0307e+01 nan=False inf=False\n",
      "[BATCH 350/2969] loss_sum=1.5501e+06 batch_avg_loss=2.4221e+04 logits_max_abs=5.1885e+05 logits_mean=-4.0184e+01 nan=False inf=False\n",
      "[BATCH 400/2969] loss_sum=2.6313e+06 batch_avg_loss=4.1114e+04 logits_max_abs=1.3247e+06 logits_mean=-4.8722e+01 nan=False inf=False\n",
      "[BATCH 450/2969] loss_sum=1.3015e+06 batch_avg_loss=2.0336e+04 logits_max_abs=3.8492e+05 logits_mean=-2.6556e+01 nan=False inf=False\n",
      "[BATCH 500/2969] loss_sum=2.6458e+06 batch_avg_loss=4.1341e+04 logits_max_abs=8.8776e+05 logits_mean=-5.4211e+01 nan=False inf=False\n",
      "[BATCH 550/2969] loss_sum=1.7560e+06 batch_avg_loss=2.7437e+04 logits_max_abs=1.1375e+06 logits_mean=-3.4621e+01 nan=False inf=False\n",
      "[BATCH 600/2969] loss_sum=3.4454e+06 batch_avg_loss=5.3835e+04 logits_max_abs=1.0445e+06 logits_mean=-6.0360e+01 nan=False inf=False\n",
      "[BATCH 650/2969] loss_sum=1.4763e+05 batch_avg_loss=2.3067e+03 logits_max_abs=1.7555e+05 logits_mean=-2.9706e+00 nan=False inf=False\n",
      "[BATCH 700/2969] loss_sum=1.7650e+05 batch_avg_loss=2.7578e+03 logits_max_abs=1.9581e+05 logits_mean=-3.8093e+00 nan=False inf=False\n",
      "[BATCH 750/2969] loss_sum=4.5131e+05 batch_avg_loss=7.0517e+03 logits_max_abs=2.5322e+05 logits_mean=-7.9456e+00 nan=False inf=False\n",
      "[BATCH 800/2969] loss_sum=2.1041e+06 batch_avg_loss=3.2877e+04 logits_max_abs=6.1899e+05 logits_mean=-4.3575e+01 nan=False inf=False\n",
      "[BATCH 850/2969] loss_sum=2.6766e+06 batch_avg_loss=4.1822e+04 logits_max_abs=5.4055e+05 logits_mean=-4.7425e+01 nan=False inf=False\n",
      "[BATCH 900/2969] loss_sum=1.4234e+06 batch_avg_loss=2.2240e+04 logits_max_abs=5.8550e+05 logits_mean=-2.8389e+01 nan=False inf=False\n",
      "[BATCH 950/2969] loss_sum=9.8008e+05 batch_avg_loss=1.5314e+04 logits_max_abs=2.6534e+05 logits_mean=-1.7019e+01 nan=False inf=False\n",
      "[BATCH 1000/2969] loss_sum=1.4867e+06 batch_avg_loss=2.3230e+04 logits_max_abs=3.1119e+05 logits_mean=-2.4882e+01 nan=False inf=False\n",
      "[BATCH 1050/2969] loss_sum=1.5210e+06 batch_avg_loss=2.3766e+04 logits_max_abs=4.5071e+05 logits_mean=-3.0265e+01 nan=False inf=False\n",
      "[BATCH 1100/2969] loss_sum=2.5792e+06 batch_avg_loss=4.0300e+04 logits_max_abs=7.1698e+05 logits_mean=-4.7183e+01 nan=False inf=False\n",
      "[BATCH 1150/2969] loss_sum=1.5259e+06 batch_avg_loss=2.3843e+04 logits_max_abs=1.0274e+06 logits_mean=-3.4742e+01 nan=False inf=False\n",
      "[BATCH 1200/2969] loss_sum=1.4942e+06 batch_avg_loss=2.3347e+04 logits_max_abs=8.4970e+05 logits_mean=-2.9473e+01 nan=False inf=False\n",
      "[BATCH 1250/2969] loss_sum=3.3141e+06 batch_avg_loss=5.1782e+04 logits_max_abs=1.0576e+06 logits_mean=-6.2709e+01 nan=False inf=False\n",
      "[BATCH 1300/2969] loss_sum=3.5422e+06 batch_avg_loss=5.5346e+04 logits_max_abs=1.1304e+06 logits_mean=-6.3885e+01 nan=False inf=False\n",
      "[BATCH 1350/2969] loss_sum=4.0451e+06 batch_avg_loss=6.3204e+04 logits_max_abs=1.4423e+06 logits_mean=-7.5301e+01 nan=False inf=False\n",
      "[BATCH 1400/2969] loss_sum=2.0476e+06 batch_avg_loss=3.1993e+04 logits_max_abs=8.9779e+05 logits_mean=-3.8772e+01 nan=False inf=False\n",
      "[BATCH 1450/2969] loss_sum=1.4395e+06 batch_avg_loss=2.2493e+04 logits_max_abs=7.5695e+05 logits_mean=-2.6068e+01 nan=False inf=False\n",
      "[BATCH 1500/2969] loss_sum=8.3758e+05 batch_avg_loss=1.3087e+04 logits_max_abs=2.6239e+05 logits_mean=-1.3314e+01 nan=False inf=False\n",
      "[BATCH 1550/2969] loss_sum=8.3583e+05 batch_avg_loss=1.3060e+04 logits_max_abs=3.4026e+05 logits_mean=-1.3633e+01 nan=False inf=False\n",
      "[BATCH 1600/2969] loss_sum=1.6757e+06 batch_avg_loss=2.6183e+04 logits_max_abs=5.4753e+05 logits_mean=-4.1990e+01 nan=False inf=False\n",
      "[BATCH 1650/2969] loss_sum=2.7079e+06 batch_avg_loss=4.2311e+04 logits_max_abs=1.2831e+06 logits_mean=-5.0734e+01 nan=False inf=False\n",
      "[BATCH 1700/2969] loss_sum=1.3368e+06 batch_avg_loss=2.0887e+04 logits_max_abs=3.7383e+05 logits_mean=-2.6130e+01 nan=False inf=False\n",
      "[BATCH 1750/2969] loss_sum=2.6845e+06 batch_avg_loss=4.1946e+04 logits_max_abs=8.9967e+05 logits_mean=-5.4881e+01 nan=False inf=False\n",
      "[BATCH 1800/2969] loss_sum=1.7081e+06 batch_avg_loss=2.6690e+04 logits_max_abs=1.0663e+06 logits_mean=-3.4224e+01 nan=False inf=False\n",
      "[BATCH 1850/2969] loss_sum=3.3210e+06 batch_avg_loss=5.1891e+04 logits_max_abs=1.0394e+06 logits_mean=-5.8412e+01 nan=False inf=False\n",
      "[BATCH 1900/2969] loss_sum=1.6894e+06 batch_avg_loss=2.6397e+04 logits_max_abs=1.0200e+06 logits_mean=-3.3354e+01 nan=False inf=False\n",
      "[BATCH 1950/2969] loss_sum=2.4821e+06 batch_avg_loss=3.8782e+04 logits_max_abs=7.5327e+05 logits_mean=-5.1851e+01 nan=False inf=False\n",
      "[BATCH 2000/2969] loss_sum=2.8538e+06 batch_avg_loss=4.4591e+04 logits_max_abs=1.2611e+06 logits_mean=-5.4753e+01 nan=False inf=False\n",
      "[BATCH 2050/2969] loss_sum=7.3560e+06 batch_avg_loss=1.1494e+05 logits_max_abs=1.3476e+06 logits_mean=-1.6083e+02 nan=False inf=False\n",
      "[BATCH 2100/2969] loss_sum=1.0277e+07 batch_avg_loss=1.6058e+05 logits_max_abs=1.2961e+06 logits_mean=-1.5698e+02 nan=False inf=False\n",
      "[BATCH 2150/2969] loss_sum=6.6266e+06 batch_avg_loss=1.0354e+05 logits_max_abs=1.7173e+06 logits_mean=-1.2559e+02 nan=False inf=False\n",
      "[BATCH 2200/2969] loss_sum=8.9789e+05 batch_avg_loss=1.4030e+04 logits_max_abs=2.5847e+05 logits_mean=-1.5984e+01 nan=False inf=False\n",
      "[BATCH 2250/2969] loss_sum=1.4456e+06 batch_avg_loss=2.2588e+04 logits_max_abs=2.7172e+05 logits_mean=-2.3549e+01 nan=False inf=False\n",
      "[BATCH 2300/2969] loss_sum=1.4410e+06 batch_avg_loss=2.2516e+04 logits_max_abs=4.0031e+05 logits_mean=-2.8263e+01 nan=False inf=False\n",
      "[BATCH 2350/2969] loss_sum=1.0395e+06 batch_avg_loss=1.6243e+04 logits_max_abs=4.3136e+05 logits_mean=-1.7212e+01 nan=False inf=False\n",
      "[BATCH 2400/2969] loss_sum=5.8165e+05 batch_avg_loss=9.0882e+03 logits_max_abs=5.1633e+05 logits_mean=-1.3803e+01 nan=False inf=False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     11\u001b[39m experiment_cfg = TrainConfig(\n\u001b[32m     12\u001b[39m     dataset=dataset,\n\u001b[32m     13\u001b[39m     model_name=model_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     out_dir=cfg.out_dir,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     result = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     all_results.append({\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: dataset,\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model_name,\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m: result\n\u001b[32m     36\u001b[39m     })\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Unified evaluation with uncertainty\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m eval_pack = \u001b[43mevaluate_with_uncertainty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Build uncertainty scores\u001b[39;00m\n\u001b[32m     63\u001b[39m entropy = eval_pack[\u001b[33m\"\u001b[39m\u001b[33mentropy\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mevaluate_with_uncertainty\u001b[39m\u001b[34m(cfg, model, test_loader)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# --- diagnostics checks (new) ---\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     logits_abs_max_batch = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.item())\n\u001b[32m     84\u001b[39m     logits_mean_batch = \u001b[38;5;28mfloat\u001b[39m(logits.mean().cpu().item())\n\u001b[32m     85\u001b[39m     loss_val = \u001b[38;5;28mfloat\u001b[39m(loss.item())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 13. Run all experiments in sequence\n",
    "all_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, (dataset, model_name) in enumerate(dataset_model_pairs, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Experiment {idx}/{len(dataset_model_pairs)}: {dataset} + {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create a fresh config for this pair\n",
    "    experiment_cfg = TrainConfig(\n",
    "        dataset=dataset,\n",
    "        model_name=model_name,\n",
    "        data_root=cfg.data_root,\n",
    "        tinyimagenet_root=cfg.tinyimagenet_root,\n",
    "        epochs=cfg.epochs,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.num_workers,\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        momentum=cfg.momentum,\n",
    "        seed=cfg.seed,\n",
    "        reject_percentiles=cfg.reject_percentiles,\n",
    "        hybrid_weight_entropy=cfg.hybrid_weight_entropy,\n",
    "        hybrid_weight_grad=cfg.hybrid_weight_grad,\n",
    "        out_dir=cfg.out_dir,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = run_experiment(experiment_cfg)\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model_name,\n",
    "            \"status\": \"success\",\n",
    "            \"result\": result\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error in experiment {idx}: {e}\")\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model_name,\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"All experiments completed in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in all_results if r[\"status\"] == \"success\")\n",
    "failed = sum(1 for r in all_results if r[\"status\"] == \"failed\")\n",
    "print(f\"Summary: {successful} successful, {failed} failed\")\n",
    "for r in all_results:\n",
    "    status_icon = \"✓\" if r[\"status\"] == \"success\" else \"✗\"\n",
    "    print(f\"  {status_icon} {r['dataset']:15} + {r['model']:20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9521161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13a. DIAGNOSTIC: Quick test - where is it hanging?\n",
    "\n",
    "print(\"DIAGNOSTIC TEST - Finding the bottleneck\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Test CIFAR-10-C dataset loading (this is likely the culprit)\n",
    "print(\"\\n[STEP 1] Testing CIFAR-10-C dataset loading...\")\n",
    "t0 = time.time()\n",
    "\n",
    "cifar10c_root = \"./data/Tiny-ImageNet-C\"\n",
    "print(f\"Looking for files in: {cifar10c_root}\")\n",
    "\n",
    "if os.path.exists(cifar10c_root):\n",
    "    files = os.listdir(cifar10c_root)\n",
    "    print(f\"Found files: {files}\")\n",
    "    \n",
    "    # Check one file\n",
    "    data_file = os.path.join(cifar10c_root, \"gaussian_noise.npy\")\n",
    "    if os.path.exists(data_file):\n",
    "        print(f\"\\nLoading {data_file}...\")\n",
    "        t1 = time.time()\n",
    "        images = np.load(data_file)\n",
    "        elapsed = time.time() - t1\n",
    "        print(f\"✓ Loaded in {elapsed:.1f}s, shape: {images.shape}\")\n",
    "        \n",
    "        # Try to slice it\n",
    "        print(f\"\\nSlicing to severity 5 (rows 40000:50000)...\")\n",
    "        t1 = time.time()\n",
    "        subset = images[40000:50000]\n",
    "        elapsed = time.time() - t1\n",
    "        print(f\"✓ Sliced in {elapsed:.3f}s, shape: {subset.shape}\")\n",
    "    else:\n",
    "        print(f\"✗ File not found: {data_file}\")\n",
    "else:\n",
    "    print(f\"✗ Directory not found: {cifar10c_root}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOTAL DIAGNOSTIC TIME:\", time.time() - t0, \"seconds\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
